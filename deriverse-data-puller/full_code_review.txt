

===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\.vscode\settings.json =====

{
    "python.defaultInterpreterPath": "${workspaceFolder}/.venv/Scripts/python.exe",
    "python.analysis.extraPaths": ["./"],
    "python.autoComplete.extraPaths": [
        "${workspaceFolder}/.venv/Lib/site-packages"
    ]
}


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\ingestion.yaml =====

# configs/ingestion.yaml

# Path to raw mock protocol events (JSON array)
raw_data_path: configs/mock_data.json

# Append-only normalized output (JSONL format)
normalized_output_path: data/normalized/events.jsonl

# Watermark / checkpoint store for incremental ingestion
checkpoint_path: data/checkpoints/watermark.json

# Allowed lateness for event-time processing (seconds)
allowed_lateness_seconds: 0

# Optional controls (future-safe)
markets: []
traders: []
max_events_per_run: null


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\loader.py =====

# configs/loader.py

import yaml
from pathlib import Path
import logging  # âœ… ADD THIS

logger = logging.getLogger(__name__) 

def load_config(path: str) -> dict:
    with open(Path(path), "r") as f:
        return yaml.safe_load(f)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\mock_data.json =====

[
  {
    "event_type": "open",
    "timestamp": "2026-02-11T15:11:37.986930Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 100,
    "size": 10,
    "fee": 0.5,
    "event_id": "cd14996880299a2a463755a7a0f8ae331876f0379cd2623017b28818650012ca",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T17:11:37.986930Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 110,
    "size": 10,
    "fee": 0.5,
    "event_id": "50db090a38149bf034f7f9be9303432e2894bf637e35b2053fb16f077be83955",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T15:21:37.986930Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 2000,
    "size": 5,
    "fee": 1.0,
    "event_id": "c9deed41e6cc353163c8697d8fc42b65cd396c44096b860fdf21c9fc308b5ee0",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T18:11:37.986930Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 1950,
    "size": 5,
    "fee": 1.0,
    "event_id": "87f160081c51272e0c1ec4f41241ef4a08a376f8df7e2c2ec675c64ca8caa93d",
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T15:31:37.986930Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 100,
    "size": 10,
    "fee": 0.5,
    "event_id": "ce66056d5b5b783be72115cac9d01970f8f77eb52678c71f6f45254e67467a5a",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T19:11:37.986930Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 120,
    "size": 10,
    "fee": 0.5,
    "event_id": "af7a2763d8b2da55fdb12db19af648997c7bb4b2391ad96948f2cc286442fff7",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T15:41:37.986930Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 50000,
    "size": 1,
    "fee": 5.0,
    "event_id": "d41b46319a38fcb0dc317fe5e8e16c62b97217213bc825b53fe4afa02546c556",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T20:11:37.986930Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 48000,
    "size": 1,
    "fee": 5.0,
    "event_id": "292eb7267b28279afc6d54e7cac13b9b105d2b5f4534ff885a1d7e906c43005a",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T15:51:37.986930Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2100,
    "size": 5,
    "fee": 2.0,
    "event_id": "1898a69235f54d7d5c4bd86ccf8338da43b22494d6964953a7b85fed4efa2df5",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T21:11:37.986930Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2050,
    "size": 5,
    "fee": 2.0,
    "event_id": "20b362c8de5b3a25323846a05a1e1c0a8d19458fd7fbdf8aeb02d185f11daff8",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:11:37.986930Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "buy",
    "price": 5.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.65,
    "implied_vol": 0.45,
    "event_id": "a8542adc1a58c95c3044996dec975929619851e78b2e06fa7778fde599f2005b",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-12T15:11:37.986930Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "sell",
    "price": 8.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.85,
    "implied_vol": 0.5,
    "event_id": "cb15aae4f1b519f6aaa03b9c1912f01eccc155d323792da09b8ed5e173d11256",
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:41:37.986930Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "sell",
    "price": 4.0,
    "size": 15,
    "fee": 0.7,
    "delta": -0.25,
    "implied_vol": 0.4,
    "event_id": "7fb11a5bbf07f5740bf9bc5f5c4753aba007317e811d34250a309bbb97aa7ffc",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-13T03:11:37.986930Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "buy",
    "price": 1.5,
    "size": 15,
    "fee": 0.7,
    "delta": -0.1,
    "implied_vol": 0.3,
    "event_id": "d5af78f8dbbb2b8a312fb56bc3ce90b26f708bbf99d9b2f8f64f508cd337f348",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T17:11:37.986930Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "buy",
    "price": 45.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.35,
    "implied_vol": 0.55,
    "event_id": "114e89b21d07895dde45be01e73cf4066d9f94da9ca1807284505e0f79b30855",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-13T15:11:37.986930Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "sell",
    "price": 20.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.15,
    "implied_vol": 0.4,
    "event_id": "dade0a214e3521ffb4c5c1e43933f24284b48616481a69e17512fed79aa6aa51",
    "order_type": "limit"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T18:11:37.986930Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "buy",
    "price": 2000.0,
    "size": 1,
    "fee": 10.0,
    "event_id": "c14e52e1ec3ed2acf379e07485b894b0870d1e5e70fa1400ab177d14f618f80b",
    "order_type": "market"
  },
  {
    "event_type": "exercise",
    "timestamp": "2026-02-28T15:11:37.986930Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "exercise",
    "size": 1,
    "fee": 10.0,
    "underlying_price": 55000,
    "event_id": "b1a0b4cdfa946f2b9e29359ad885e5c64e0dd5319c888c04f26634dd17ea0c5b"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T19:11:37.986930Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "buy",
    "price": 3.0,
    "size": 20,
    "fee": 0.2,
    "event_id": "9256996d2a5e29b4920387b80857e670cc731cb8828de2a374839fcf295438b3",
    "order_type": "market"
  },
  {
    "event_type": "expire",
    "timestamp": "2026-03-01T15:11:37.986930Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "expire",
    "price": 0.0,
    "size": 20,
    "fee": 0.0,
    "underlying_price": 95,
    "event_id": "86f94ec22630d2b6c3c6e0e9c7029e7b4844f18a0b3c97979eabb89edbcd97e4"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T20:11:37.986930Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "buy",
    "price": 8.0,
    "size": 20,
    "fee": 1.0,
    "event_id": "dc75ea52b119a51f0ba1ceb0583fb3df898185d0ea597f4f628dfa74d021c208",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-12T17:11:37.986930Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "sell",
    "price": 12.0,
    "size": 10,
    "fee": 0.5,
    "event_id": "90fdd5d8b9b68d4712fcd967b6be1f1fe9944419c150978e3f51a6e724d59be8",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-13T17:11:37.986930Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-03-01T15:11:37.986930Z",
    "side": "sell",
    "price": 15.0,
    "size": 10,
    "fee": 0.5,
    "event_id": "8864789882b925e730127b47c658593b017a370245ff849a5031d366834b68d9",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:11:37.986930Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 105,
    "size": 5,
    "fee": 0.3,
    "event_id": "19f871f55ef0ddbee3165bc3c5d500086c131256722b3425d30f4f502d50a99a",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-12T01:11:37.986930Z",
    "trader_id": "GhostWallet1111111111111111111111111111",
    "market_id": "GHOST-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 999,
    "size": 1,
    "fee": 0.1,
    "event_id": "fa69011a7a365d2a0725dd468dda85c1c892caa3896554f48e4c130f59254443",
    "order_type": "stop"
  },
  {
    "event_type": "trade",
    "timestamp": "2026-02-11T15:26:37.986930Z",
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 101,
    "size": 100,
    "fee": 1.0,
    "event_id": "363c89842a9e12c32b0366d7f611dca1b3f840e3b6dc968b0ef7690de72fe65e"
  },
  {
    "event_type": "trade",
    "timestamp": "2026-02-11T15:56:37.986930Z",
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "sell",
    "price": 2105,
    "size": 50,
    "fee": 5.0,
    "event_id": "5808bcd884a41d1107d03d44018a5d473a0b3268d376aa87353117036675dcd6"
  }
]


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\dashboards\app.py =====

# dashboards/app.py
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
from datetime import datetime

DATA_DIR = Path("data/analytics_output")

st.set_page_config(page_title="Deriverse Trading Analytics", layout="wide")

# Custom CSS for better styling
st.markdown("""
    <style>
    .metric-card { padding: 20px; border-radius: 10px; background: #f0f2f6; }
    .stMetric { background: white; padding: 15px; border-radius: 8px; }
    </style>
""", unsafe_allow_html=True)

st.title("ðŸ“Š Deriverse Trading Analytics Dashboard")

# Load data with error handling
@st.cache_data
def load_data():
    try:
        equity = pd.read_csv(DATA_DIR / "equity_curve.csv", parse_dates=["timestamp"])
        positions = pd.read_csv(DATA_DIR / "positions.csv", parse_dates=["open_time", "close_time"])
        summary = pd.read_csv(DATA_DIR / "summary_metrics.csv")
        fees = pd.read_csv(DATA_DIR / "fees_breakdown.csv")
        volume = pd.read_csv(DATA_DIR / "volume_by_market.csv")
        pnl_day = pd.read_csv(DATA_DIR / "pnl_by_day.csv", parse_dates=["date"])
        pnl_hour = pd.read_csv(DATA_DIR / "pnl_by_hour.csv")
        directional = pd.read_csv(DATA_DIR / "directional_bias.csv")
        order_perf = pd.read_csv(DATA_DIR / "order_type_performance.csv")
        greeks = pd.read_csv(DATA_DIR / "greeks_exposure.csv")
        
        return {
            'equity': equity,
            'positions': positions,
            'summary': summary,
            'fees': fees,
            'volume': volume,
            'pnl_day': pnl_day,
            'pnl_hour': pnl_hour,
            'directional': directional,
            'order_perf': order_perf,
            'greeks': greeks
        }
    except FileNotFoundError as e:
        st.error(f"Analytics files not found. Please run: python -m scripts.run_analytics")
        return None

data = load_data()

if data is None or data['positions'].empty:
    st.warning("No trading data available. Please generate mock data and run analytics.")
    st.stop()

# Sidebar filters
st.sidebar.header("ðŸŽ›ï¸ Filters")

# Trader selector
traders = sorted(data['positions']['trader_id'].unique())
selected_trader = st.sidebar.selectbox("Select Trader", ["All Traders"] + traders)

# Date range filter
min_date = data['positions']['close_time'].min().date()
max_date = data['positions']['close_time'].max().date()
date_range = st.sidebar.date_input(
    "Date Range",
    value=(min_date, max_date),
    min_value=min_date,
    max_value=max_date
)

# Market selector
markets = sorted(data['positions']['market_id'].unique())
selected_market = st.sidebar.selectbox("Select Market", ["All Markets"] + markets)

# Filter data based on selections
filtered_positions = data['positions'].copy()
filtered_equity = data['equity'].copy()

if selected_trader != "All Traders":
    filtered_positions = filtered_positions[filtered_positions['trader_id'] == selected_trader]
    filtered_equity = filtered_equity[filtered_equity['trader_id'] == selected_trader]

if len(date_range) == 2:
    start_date, end_date = date_range
    filtered_positions = filtered_positions[
        (filtered_positions['close_time'].dt.date >= start_date) &
        (filtered_positions['close_time'].dt.date <= end_date)
    ]

if selected_market != "All Markets":
    filtered_positions = filtered_positions[filtered_positions['market_id'] == selected_market]

# KPI Tiles
st.header("ðŸ“ˆ Key Performance Indicators")

col1, col2, col3, col4 = st.columns(4)

with col1:
    total_pnl = filtered_positions['realized_pnl'].sum()
    st.metric(
        "Total PnL",
        f"${total_pnl:,.2f}",
        delta=f"{total_pnl:,.2f}" if total_pnl > 0 else None
    )

with col2:
    win_rate = (filtered_positions['realized_pnl'] > 0).mean() * 100
    st.metric("Win Rate", f"{win_rate:.1f}%")

with col3:
    if selected_trader != "All Traders":
        trader_data = data['summary'][data['summary']['trader_id'] == selected_trader]
        if not trader_data.empty:
            max_dd = trader_data['max_drawdown'].iloc[0]
        else:
            max_dd = 0
    else:
        max_dd = data['summary']['max_drawdown'].min()
    st.metric("Max Drawdown", f"${max_dd:,.2f}")

with col4:
    total_fees = filtered_positions['fees'].sum()
    st.metric("Total Fees", f"${total_fees:,.2f}")

# âœ… NEW: Average Win/Loss Comparison
st.header("ðŸ’¹ Win/Loss Analysis")
col1, col2, col3 = st.columns(3)

with col1:
    if not filtered_positions.empty:
        winning = filtered_positions[filtered_positions['realized_pnl'] > 0]
        avg_win = winning['realized_pnl'].mean() if len(winning) > 0 else 0
        st.metric("Average Win", f"${avg_win:,.2f}")

with col2:
    if not filtered_positions.empty:
        losing = filtered_positions[filtered_positions['realized_pnl'] < 0]
        avg_loss = losing['realized_pnl'].mean() if len(losing) > 0 else 0
        st.metric("Average Loss", f"${avg_loss:,.2f}")

with col3:
    if not filtered_positions.empty and 'avg_win' in locals() and 'avg_loss' in locals():
        profit_factor = abs(avg_win / avg_loss) if avg_loss != 0 else 0
        st.metric("Profit Factor", f"{profit_factor:.2f}x")

# Risk-Adjusted Returns (if summary has sharpe/sortino)
if 'sharpe_ratio' in data['summary'].columns:
    st.header("ðŸ“Š Risk-Adjusted Returns")
    col1, col2 = st.columns(2)
    with col1:
        if selected_trader != "All Traders":
            trader_sharpe = data['summary'][data['summary']['trader_id'] == selected_trader]['sharpe_ratio'].iloc[0] if not data['summary'][data['summary']['trader_id'] == selected_trader].empty else 0
        else:
            trader_sharpe = data['summary']['sharpe_ratio'].mean()
        st.metric("Sharpe Ratio", f"{trader_sharpe:.2f}")
    with col2:
        if selected_trader != "All Traders":
            trader_sortino = data['summary'][data['summary']['trader_id'] == selected_trader]['sortino_ratio'].iloc[0] if not data['summary'][data['summary']['trader_id'] == selected_trader].empty else 0
        else:
            trader_sortino = data['summary']['sortino_ratio'].mean()
        st.metric("Sortino Ratio", f"{trader_sortino:.2f}")

# Equity Curve with Drawdown
st.header("ðŸ’° Equity Curve")

if not filtered_equity.empty:
    fig_equity = go.Figure()
    
    for trader in filtered_equity['trader_id'].unique():
        trader_equity = filtered_equity[filtered_equity['trader_id'] == trader].sort_values('timestamp')
        
        # Cumulative PnL line
        fig_equity.add_trace(go.Scatter(
            x=trader_equity['timestamp'],
            y=trader_equity['cumulative_pnl'],
            name=f"{trader} - PnL",
            mode='lines'
        ))
        
        # Drawdown shading
        fig_equity.add_trace(go.Scatter(
            x=trader_equity['timestamp'],
            y=trader_equity['drawdown'],
            name=f"{trader} - Drawdown",
            fill='tozeroy',
            fillcolor='rgba(255,0,0,0.2)',
            line=dict(color='red', width=0.5)
        ))
    
    fig_equity.update_layout(
        xaxis_title="Time",
        yaxis_title="PnL ($)",
        hovermode='x unified',
        height=400
    )
    st.plotly_chart(fig_equity, use_container_width=True)
else:
    st.info("No equity data for selected filters")

# Two column layout
col_left, col_right = st.columns(2)

# Daily PnL Chart
with col_left:
    st.subheader("ðŸ“… Daily PnL")
    if not filtered_positions.empty:
        daily_pnl = filtered_positions.groupby(
            filtered_positions['close_time'].dt.date
        )['realized_pnl'].sum().reset_index()
        daily_pnl.columns = ['date', 'pnl']
        
        fig_daily = px.bar(
            daily_pnl,
            x='date',
            y='pnl',
            color='pnl',
            color_continuous_scale=['red', 'gray', 'green'],
            color_continuous_midpoint=0
        )
        fig_daily.update_layout(height=300, showlegend=False)
        st.plotly_chart(fig_daily, use_container_width=True)
    else:
        st.info("No data")

# Fees Breakdown
with col_right:
    st.subheader("ðŸ’¸ Fees by Product Type")
    if not filtered_positions.empty:
        fees_by_product = filtered_positions.groupby('product_type')['fees'].sum().reset_index()
        
        fig_fees = px.bar(
            fees_by_product,
            x='product_type',
            y='fees',
            color='product_type'
        )
        fig_fees.update_layout(height=300, showlegend=False)
        st.plotly_chart(fig_fees, use_container_width=True)
    else:
        st.info("No data")

# Long vs Short Ratio
col_left2, col_right2 = st.columns(2)

with col_left2:
    st.subheader("âš–ï¸ Long vs Short Ratio")
    if not filtered_positions.empty:
        long_count = len(filtered_positions[filtered_positions['side'].isin(['long', 'buy'])])
        short_count = len(filtered_positions[filtered_positions['side'].isin(['short', 'sell'])])
        
        fig_ratio = go.Figure(data=[go.Pie(
            labels=['Long', 'Short'],
            values=[long_count, short_count],
            hole=0.4,
            marker_colors=['#00cc96', '#ef553b']
        )])
        fig_ratio.update_layout(height=300)
        st.plotly_chart(fig_ratio, use_container_width=True)
    else:
        st.info("No data")

# Time of Day Performance
with col_right2:
    st.subheader("ðŸ• Performance by Hour")
    if not filtered_positions.empty:
        hourly = filtered_positions.copy()
        hourly['hour'] = hourly['close_time'].dt.hour
        hour_pnl = hourly.groupby('hour')['realized_pnl'].mean().reset_index()
        
        fig_hour = px.line(
            hour_pnl,
            x='hour',
            y='realized_pnl',
            markers=True
        )
        fig_hour.update_layout(
            height=300,
            xaxis_title="Hour of Day",
            yaxis_title="Avg PnL ($)"
        )
        st.plotly_chart(fig_hour, use_container_width=True)
    else:
        st.info("No data")

# âœ… NEW: Order Type Performance Chart
st.header("ðŸ“Š Order Type Performance")

if not data['order_perf'].empty:
    fig_order = go.Figure()
    
    fig_order.add_trace(go.Bar(
        name='Win Rate',
        x=data['order_perf']['order_type'],
        y=data['order_perf']['win_rate'] * 100,
        yaxis='y',
        marker_color='lightblue'
    ))
    
    fig_order.add_trace(go.Scatter(
        name='Avg PnL',
        x=data['order_perf']['order_type'],
        y=data['order_perf']['avg_pnl'],
        yaxis='y2',
        marker_color='green',
        line=dict(width=3)
    ))
    
    fig_order.update_layout(
        xaxis_title="Order Type",
        yaxis_title="Win Rate (%)",
        yaxis2=dict(
            title="Avg PnL ($)",
            overlaying='y',
            side='right'
        ),
        hovermode='x unified',
        height=400
    )
    
    st.plotly_chart(fig_order, use_container_width=True)
else:
    st.info("No order type data available")

# Greeks Exposure (if available)
if not data['greeks'].empty:
    st.header("ðŸ“ Options Greeks Exposure")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Net Delta by Trader")
        fig_delta = px.bar(
            data['greeks'],
            x='trader_id',
            y='net_delta',
            title="Delta Exposure"
        )
        st.plotly_chart(fig_delta, use_container_width=True)
    
    with col2:
        st.subheader("Options Position Count")
        fig_options = px.bar(
            data['greeks'],
            x='trader_id',
            y='total_option_positions',
            title="Number of Options Positions"
        )
        st.plotly_chart(fig_options, use_container_width=True)

# Trade History Table
st.header("ðŸ“‹ Trade History")

if not filtered_positions.empty:
    # Prepare display dataframe
    display_df = filtered_positions[[
        'position_id', 'trader_id', 'market_id', 'product_type', 'side',
        'entry_price', 'exit_price', 'size', 'realized_pnl', 'fees',
        'open_time', 'close_time'
    ]].copy()
    
    # Format columns
    display_df['realized_pnl'] = display_df['realized_pnl'].apply(lambda x: f"${x:,.2f}")
    display_df['fees'] = display_df['fees'].apply(lambda x: f"${x:,.2f}")
    display_df['entry_price'] = display_df['entry_price'].apply(lambda x: f"${x:,.2f}")
    display_df['exit_price'] = display_df['exit_price'].apply(lambda x: f"${x:,.2f}")
    
    # Sort by close time descending
    display_df = display_df.sort_values('close_time', ascending=False)
    
    st.dataframe(
        display_df,
        use_container_width=True,
        height=400,
        hide_index=True
    )
    
    # Download button
    csv = filtered_positions.to_csv(index=False)
    st.download_button(
        label="ðŸ“¥ Download Trade History CSV",
        data=csv,
        file_name=f"trade_history_{datetime.now().strftime('%Y%m%d')}.csv",
        mime="text/csv"
    )
else:
    st.info("No trades match the selected filters")

# Additional Analytics
st.header("ðŸ“Š Additional Analytics")

col1, col2, col3 = st.columns(3)

with col1:
    st.subheader("ðŸŽ¯ Best/Worst Trades")
    if not filtered_positions.empty:
        best = filtered_positions.loc[filtered_positions['realized_pnl'].idxmax()]
        worst = filtered_positions.loc[filtered_positions['realized_pnl'].idxmin()]
        
        st.write(f"**Best Trade:** ${best['realized_pnl']:,.2f}")
        st.write(f"Market: {best['market_id']}")
        st.write(f"Side: {best['side']}")
        st.write("")
        st.write(f"**Worst Trade:** ${worst['realized_pnl']:,.2f}")
        st.write(f"Market: {worst['market_id']}")
        st.write(f"Side: {worst['side']}")

with col2:
    st.subheader("â±ï¸ Average Duration")
    if not filtered_positions.empty:
        avg_duration = filtered_positions['duration_seconds'].mean()
        hours = int(avg_duration // 3600)
        minutes = int((avg_duration % 3600) // 60)
        st.metric("Avg Position Duration", f"{hours}h {minutes}m")
        
        st.write(f"**Shortest:** {filtered_positions['duration_seconds'].min() / 60:.0f} min")
        st.write(f"**Longest:** {filtered_positions['duration_seconds'].max() / 3600:.1f} hrs")

with col3:
    st.subheader("ðŸ“ˆ Volume by Market")
    if not filtered_positions.empty:
        market_vol = filtered_positions.groupby('market_id').agg({
            'size': 'sum',
            'realized_pnl': 'sum'
        }).sort_values('size', ascending=False)
        
        for market, row in market_vol.head(5).iterrows():
            st.write(f"**{market}**")
            st.write(f"Volume: {row['size']:,.2f} | PnL: ${row['realized_pnl']:,.2f}")
            st.write("")

# Footer
st.markdown("---")
st.caption(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Deriverse Analytics v1.0")


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\dashboards\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\diagnose_data.py =====

# scripts/diagnose_data.py
"""
Data quality diagnostic tool.
Run after analytics to verify all data is properly processed.
"""

import pandas as pd
from pathlib import Path
import json

print("=" * 60)
print("DATA QUALITY DIAGNOSTIC")
print("=" * 60)

# Check positions file
positions_path = Path("data/analytics_output/positions.csv")
if positions_path.exists():
    positions = pd.read_csv(positions_path)
    
    print(f"\nðŸ“Š POSITIONS SUMMARY ({len(positions)} total)")
    print("\nâœ… By Product Type:")
    product_counts = positions['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} positions")
    
    print("\nâœ… By Market:")
    market_counts = positions['market_id'].value_counts()
    for market, count in market_counts.items():
        print(f"  {market:25} {count:>3} positions")
    
    print("\nâœ… By Trader:")
    trader_counts = positions['trader_id'].value_counts()
    for trader, count in trader_counts.items():
        print(f"  {trader:10} {count:>3} positions")
    
    print("\nðŸ’° PnL BY PRODUCT TYPE:")
    pnl_by_product = positions.groupby('product_type')['realized_pnl'].sum()
    for product, pnl in pnl_by_product.items():
        print(f"  {product:10} ${pnl:>12,.2f}")
    
    print("\nðŸ” OPTION POSITIONS DETAIL:")
    option_positions = positions[positions['product_type'] == 'option']
    if not option_positions.empty:
        print(f"  Found {len(option_positions)} option positions")
        for _, row in option_positions.iterrows():
            print(f"    â€¢ {row['market_id']:30} {row['trader_id']:10} ${row['realized_pnl']:>10,.2f}")
    else:
        print("  âŒ NO OPTION POSITIONS FOUND")
    
    print("\nðŸ“‹ ALL POSITIONS SUMMARY:")
    # Check which columns exist
    available_cols = ['position_id', 'trader_id', 'market_id', 'product_type', 'side', 'realized_pnl']
    if 'close_reason' in positions.columns:
        available_cols.append('close_reason')
    
    print(positions[available_cols].to_string(index=False))
    
else:
    print("âŒ positions.csv not found")

# Check normalized events
events_path = Path("data/normalized/events.jsonl")
if events_path.exists():
    events = []
    with open(events_path) as f:
        for line in f:
            line = line.strip()
            if line:
                events.append(json.loads(line))
    
    df = pd.DataFrame(events)
    
    print(f"\nðŸ“¥ NORMALIZED EVENTS ({len(df)} total)")
    print("\nâœ… By Event Type:")
    event_counts = df['event_type'].value_counts()
    for event_type, count in event_counts.items():
        print(f"  {event_type:10} {count:>3} events")
    
    print("\nâœ… By Product Type:")
    product_counts = df['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} events")
    
    print("\nðŸŽ¯ OPTION EVENTS BREAKDOWN:")
    option_events = df[df['product_type'] == 'option']
    print(f"  Total option events: {len(option_events)}")
    if not option_events.empty:
        print("\n  By event type:")
        option_event_counts = option_events['event_type'].value_counts()
        for event_type, count in option_event_counts.items():
            print(f"    {event_type:10} {count:>3}")
        
        print("\n  By market:")
        option_market_counts = option_events['market_id'].value_counts()
        for market, count in option_market_counts.items():
            print(f"    {market:30} {count:>3}")
    else:
        print("  âŒ NO OPTION EVENTS")
else:
    print("âŒ events.jsonl not found")

# Check raw mock data
mock_path = Path("configs/mock_data.json")
if mock_path.exists():
    with open(mock_path) as f:
        mock_data = json.load(f)
    
    print(f"\nðŸ“¦ RAW MOCK DATA ({len(mock_data)} events)")
    mock_df = pd.DataFrame(mock_data)
    
    print("\nâœ… By Event Type:")
    event_counts = mock_df['event_type'].value_counts()
    for event_type, count in event_counts.items():
        print(f"  {event_type:10} {count:>3} events")
    
    print("\nâœ… By Product Type:")
    product_counts = mock_df['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} events")
else:
    print("\nâŒ configs/mock_data.json not found")

print("\n" + "=" * 60)

# Check for duplicates
if events_path.exists():
    print("\nðŸ” CHECKING FOR DUPLICATES...")
    event_ids = [e['event_id'] for e in events]
    unique_ids = set(event_ids)
    
    if len(event_ids) != len(unique_ids):
        print(f"  âš ï¸  WARNING: Found {len(event_ids) - len(unique_ids)} duplicate events!")
        print(f"  Total events: {len(event_ids)}, Unique: {len(unique_ids)}")
    else:
        print(f"  âœ… No duplicates found ({len(event_ids)} unique events)")

print("=" * 60)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\generate_mock_data.py =====

# scripts/generate_mock_data.py
import json
import random
import os
import hashlib
from datetime import datetime, timezone, timedelta
from pathlib import Path

# âœ… Output to configs/mock_data.json (JSON array format)
OUTPUT_PATH = Path("configs/mock_data.json")
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

SEED = int(os.getenv("MOCK_SEED", "42"))
random.seed(SEED)

now = datetime.now(timezone.utc)
events = []

# âœ… REALISTIC SOLANA WALLET ADDRESSES (base58-like)
WALLETS = {
    "alice": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "bob": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "charlie": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "diana": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "evan": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "fiona": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "george": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "hannah": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "ivan": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "julia": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
}

def generate_event_id(event_data, index):
    """Generate deterministic event ID."""
    seed_parts = [
        str(event_data.get('event_type', '')),
        str(event_data.get('timestamp', '') if isinstance(event_data.get('timestamp'), str) else ''),
        str(event_data.get('trader_id', '')),
        str(event_data.get('market_id', '')),
        str(index)
    ]
    seed = "|".join(seed_parts)
    return hashlib.sha256(seed.encode()).hexdigest()

def emit(event):
    """Helper to add event with proper formatting and event_id."""
    if 'timestamp' in event and isinstance(event['timestamp'], datetime):
        event['timestamp'] = event['timestamp'].isoformat().replace("+00:00", "Z")
    
    if 'event_id' not in event:
        event['event_id'] = generate_event_id(event, len(events) + 1)
    
    # âœ… ADD THIS: Assign order type
    if event['event_type'] in ['open', 'close']:
        # Randomly assign order types for diversity
        event['order_type'] = random.choice(['market', 'limit', 'stop'])
    
    events.append(event)
    
# --------------------------------------------------
# 1ï¸âƒ£ SPOT TRADES (buy â†’ sell) - 1 WIN, 1 LOSS
# --------------------------------------------------

# WINNING SPOT TRADE
emit({
    "event_type": "open",
    "timestamp": now,
    "trader_id": WALLETS["alice"],
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 100,
    "size": 10,
    "fee": 0.5,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=2),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 110,
    "size": 10,
    "fee": 0.5,
})

# LOSING SPOT TRADE
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=10),
    "trader_id": WALLETS["bob"],
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 2000,
    "size": 5,
    "fee": 1.0,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=3),
    "trader_id": WALLETS["bob"],
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 1950,
    "size": 5,
    "fee": 1.0,
})

# --------------------------------------------------
# 2ï¸âƒ£ PERPETUAL TRADES (long/short) - 2 WINS, 1 LOSS
# --------------------------------------------------

# WINNING LONG PERP
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=20),
    "trader_id": WALLETS["charlie"],
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Perps use long/short
    "price": 100,
    "size": 10,
    "fee": 0.5,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=4),
    "trader_id": WALLETS["charlie"],
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Same side for perp close
    "price": 120,
    "size": 10,
    "fee": 0.5,
})

# WINNING SHORT PERP
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=30),
    "trader_id": WALLETS["diana"],
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",  # âœ… CORRECT: Perps use long/short
    "price": 50000,
    "size": 1,
    "fee": 5.0,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=5),
    "trader_id": WALLETS["diana"],
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",  # âœ… CORRECT: Same side for perp close
    "price": 48000,
    "size": 1,
    "fee": 5.0,
})

# LOSING LONG PERP
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=40),
    "trader_id": WALLETS["evan"],
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Perps use long/short
    "price": 2100,
    "size": 5,
    "fee": 2.0,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=6),
    "trader_id": WALLETS["evan"],
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Same side for perp close
    "price": 2050,
    "size": 5,
    "fee": 2.0,
})

# --------------------------------------------------
# 3ï¸âƒ£ OPTION TRADES - COMPLETE LIFECYCLE
# --------------------------------------------------

# === LONG CALL OPTION: Buy call, sell it back (Winning) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=1),
    "trader_id": WALLETS["fiona"],
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 5.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.65,
    "implied_vol": 0.45,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=24),
    "trader_id": WALLETS["fiona"],
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 8.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.85,
    "implied_vol": 0.50,
})

# === SHORT PUT OPTION: Sell put, buy it back (Winning) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=1, minutes=30),
    "trader_id": WALLETS["george"],
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… CORRECT: Selling to open short position
    "price": 4.0,
    "size": 15,
    "fee": 0.7,
    "delta": -0.25,
    "implied_vol": 0.40,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=36),
    "trader_id": WALLETS["george"],
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… CORRECT: Buying to close short position
    "price": 1.5,
    "size": 15,
    "fee": 0.7,
    "delta": -0.10,
    "implied_vol": 0.30,
})

# === LONG PUT OPTION: Buy put, sell it back (Losing) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=2),
    "trader_id": WALLETS["hannah"],
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 45.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.35,
    "implied_vol": 0.55,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=48),
    "trader_id": WALLETS["hannah"],
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 20.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.15,
    "implied_vol": 0.40,
})

# === LONG CALL: Buy and EXERCISE (ITM) - Winning ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=3),
    "trader_id": WALLETS["ivan"],
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 2000.0,
    "size": 1,
    "fee": 10.0,
})

emit({
    "event_type": "exercise",
    "timestamp": now + timedelta(days=17),
    "trader_id": WALLETS["ivan"],
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "exercise",  # âœ… CORRECT: Exercise is its own side
    "size": 1,
    "fee": 10.0,
    "underlying_price": 55000,
})

# === LONG PUT: Buy and let EXPIRE worthless (Losing) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=4),
    "trader_id": WALLETS["julia"],
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 3.0,
    "size": 20,
    "fee": 0.2,
})

emit({
    "event_type": "expire",
    "timestamp": now + timedelta(days=18),
    "trader_id": WALLETS["julia"],
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "expire",  # âœ… CORRECT: Expire is its own side
    "price": 0.0,
    "size": 20,
    "fee": 0.0,
    "underlying_price": 95,
})

# === PARTIAL CLOSE: Long call, close in 2 tranches ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=5),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 8.0,
    "size": 20,
    "fee": 1.0,
})

# First partial close
emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=26),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 12.0,
    "size": 10,
    "fee": 0.5,
})

# Second partial close
emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=50),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 15.0,
    "size": 10,
    "fee": 0.5,
})

# --------------------------------------------------
# 4ï¸âƒ£ EDGE CASES (for robustness testing)
# --------------------------------------------------

# Duplicate open (should be ignored)
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=60),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 105,
    "size": 5,
    "fee": 0.3,
})

# Close without open (should be rejected)
emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=10),
    "trader_id": "GhostWallet1111111111111111111111111111",
    "market_id": "GHOST-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Perps use long/short
    "price": 999,
    "size": 1,
    "fee": 0.1,
})

# Trade events (informational only)
emit({
    "event_type": "trade",
    "timestamp": now + timedelta(minutes=15),
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… CORRECT: Trades can use buy/sell
    "price": 101,
    "size": 100,
    "fee": 1.0,
})

emit({
    "event_type": "trade",
    "timestamp": now + timedelta(minutes=45),
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "sell",  # âœ… CORRECT: Trades can use buy/sell
    "price": 2105,
    "size": 50,
    "fee": 5.0,
})

# --------------------------------------------------
# âœ… WRITE OUTPUT AS JSON ARRAY
# --------------------------------------------------
with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(events, f, indent=2)

print(f"âœ… Generated {len(events)} mock events â†’ {OUTPUT_PATH} (seed={SEED})")
print(f"   Wallets: {len(WALLETS)} realistic Solana addresses")
print(f"   Includes: Spot (buy/sell), Perps (long/short), Options (buy/sell/exercise/expire)")
print(f"   Format: JSON array - ready for ingestion")


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\launch_dashboard.py =====

# scripts/launch_dashboard.py

import streamlit as st
from dashboards.app import run_app
from src.common.logging import get_logger
from configs.loader import load_config

logger = get_logger(__name__)


def main():
    config = load_config("configs/dashboard.yaml")

    logger.info("Launching analytics dashboard")

    st.set_page_config(
        page_title="Deriverse Trading Analytics",
        layout="wide"
    )

    run_app(config)


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\run_analytics.py =====

# scripts/run_analytics.py
import pandas as pd
from pathlib import Path
import io
import logging
from datetime import datetime, UTC
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.analytics.pnl_engine import compute_realized_pnl
from src.analytics.summary import compute_executive_summary
from src.analytics.analytics_builder import AnalyticsBuilder

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

NORMALIZED_EVENTS_PATH = Path("data/normalized/events.jsonl")
ANALYTICS_OUTPUT_DIR = Path("data/analytics_output")


def load_events(path: Path) -> pd.DataFrame:
    """Load events from JSONL file with flexible timestamp parsing."""
    events = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                events.append(pd.read_json(io.StringIO(line), typ="series"))
    
    if not events:
        logger.warning(f"No events found in {path}")
        return pd.DataFrame()
    
    df = pd.DataFrame(events)
    
    try:
        df["timestamp"] = pd.to_datetime(df["timestamp"], format='ISO8601', utc=True)
    except Exception as e:
        logger.warning(f"ISO8601 parsing failed, trying mixed format: {e}")
        df["timestamp"] = pd.to_datetime(df["timestamp"], format='mixed', utc=True)
    
    return df


def run_analytics(events_df, auto_summary=True):
    if events_df.empty:
        logger.error("No events to analyze")
        return None, None
    
    logger.info(f"Loaded {len(events_df)} events")

    logger.info("Computing realized PnL (truth engine)")
    positions_df, pnl_df = compute_realized_pnl(events_df)

    if positions_df.empty:
        logger.warning("No closed positions found â€“ creating empty analytics")
    
    # Build all analytics outputs
    logger.info("Building comprehensive analytics tables...")
    builder = AnalyticsBuilder(positions_df, pnl_df, ANALYTICS_OUTPUT_DIR)
    builder.build_all()

    if not positions_df.empty and auto_summary:
        summary = compute_executive_summary(positions_df, pnl_df)
        
        print("\n" + "=" * 50)
        print("EXECUTIVE SUMMARY")
        print("=" * 50)
        print(f"Total Realized PnL:  ${summary['total_pnl']:,.2f}")
        print(f"Total Fees Paid:     ${summary['total_fees']:,.2f}")
        print(f"Total Trades:        {summary['trade_count']}")
        print(f"Win Rate:            {summary['win_rate']:.1%}")
        print(f"Avg Win:             ${summary['avg_win']:,.2f}")
        print(f"Avg Loss:            ${summary['avg_loss']:,.2f}")
        print(f"Best Trade:          ${summary['best_trade']:,.2f}")
        print(f"Worst Trade:         ${summary['worst_trade']:,.2f}")
        print(f"Avg Duration:        {summary['avg_duration']}")
        print(f"Long Ratio:          {summary['long_ratio']:.1%}")
        print(f"Short Ratio:         {summary['short_ratio']:.1%}")
        print(f"Max Drawdown:        ${summary['max_drawdown']:,.2f}")
        
        # âœ… FIXED: Added risk-adjusted metrics to match summary.py
        if 'sharpe_ratio' in summary:
            print(f"Sharpe Ratio:        {summary['sharpe_ratio']:.2f}")
        if 'sortino_ratio' in summary:
            print(f"Sortino Ratio:       {summary['sortino_ratio']:.2f}")
        
        print("=" * 50 + "\n")

    logger.info("Analytics run complete âœ…")
    return positions_df, pnl_df


def main():
    logger.info("=" * 60)
    logger.info("Starting Deriverse Analytics Pipeline")
    logger.info("=" * 60)

    if not NORMALIZED_EVENTS_PATH.exists():
        logger.error(f"Normalized events not found at {NORMALIZED_EVENTS_PATH}")
        logger.error("Run 'python -m scripts.generate_mock_data' first")
        return

    events_df = load_events(NORMALIZED_EVENTS_PATH)
    run_analytics(events_df)


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\run_ingestion.py =====


# scripts/run_ingestion.py
import logging
from src.ingestion.pipelines import IngestionPipeline
from configs.loader import load_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    logger.info("Starting incremental ingestion")

    config = load_config("configs/ingestion.yaml")
    
    pipeline = IngestionPipeline(
        raw_path=config["raw_data_path"],
        output_path=config["normalized_output_path"],
        checkpoint_path=config["checkpoint_path"],
    )

    count = pipeline.run()
    logger.info(f"Ingested {count} new events")


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\validate_analytics.py =====

# scripts/validate_analytics.py
"""
Validation script to verify analytics output quality and correctness.
Run after python -m scripts.run_analytics
"""

import pandas as pd
from pathlib import Path
import sys

OUTPUT_DIR = Path("data/analytics_output")

class bcolors:
    OK = '\033[92m'
    FAIL = '\033[91m'
    WARN = '\033[93m'
    END = '\033[0m'

def validate_file_exists(filename):
    """Check if required file exists."""
    path = OUTPUT_DIR / filename
    if path.exists():
        print(f"{bcolors.OK}âœ“{bcolors.END} {filename} exists")
        return True
    else:
        print(f"{bcolors.FAIL}âœ—{bcolors.END} {filename} missing")
        return False

def validate_positions():
    """Validate positions.csv structure and data quality."""
    df = pd.read_csv(OUTPUT_DIR / "positions.csv")
    
    required_cols = [
        'position_id', 'trader_id', 'market_id', 'product_type', 'side',
        'open_time', 'close_time', 'duration_seconds',
        'entry_price', 'exit_price', 'size', 'gross_pnl', 'fees', 'realized_pnl'
    ]
    
    issues = []
    
    # Check columns
    missing_cols = set(required_cols) - set(df.columns)
    if missing_cols:
        issues.append(f"Missing columns: {missing_cols}")
    
    # Check data quality
    if not df.empty:
        if (df['duration_seconds'] < 0).any():
            issues.append("Negative duration_seconds found")
        
        if (df['fees'] < 0).any():
            issues.append("Negative fees found")
        
        # PnL consistency check
        expected_pnl = df['gross_pnl'] - df['fees']
        if not expected_pnl.equals(df['realized_pnl']):
            max_diff = abs(expected_pnl - df['realized_pnl']).max()
            if max_diff > 0.01:  # Allow for rounding
                issues.append(f"PnL inconsistency detected (max diff: {max_diff:.4f})")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} positions.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} positions.csv is valid ({len(df)} rows)")
        return True

def validate_summary_metrics():
    """Validate summary_metrics.csv calculations."""
    positions = pd.read_csv(OUTPUT_DIR / "positions.csv")
    summary = pd.read_csv(OUTPUT_DIR / "summary_metrics.csv")
    
    issues = []
    
    for _, row in summary.iterrows():
        trader = row['trader_id']
        trader_pos = positions[positions['trader_id'] == trader]
        
        # Validate win rate
        actual_wins = (trader_pos['realized_pnl'] > 0).sum()
        actual_total = len(trader_pos)
        expected_win_rate = actual_wins / actual_total if actual_total > 0 else 0
        
        if abs(row['win_rate'] - expected_win_rate) > 0.01:
            issues.append(f"{trader}: win_rate mismatch ({row['win_rate']:.2f} vs {expected_win_rate:.2f})")
        
        # Validate long/short ratio
        if abs(row['long_ratio'] + row['short_ratio'] - 1.0) > 0.01:
            issues.append(f"{trader}: long_ratio + short_ratio != 1.0")
        
        # Validate total_pnl
        expected_total = trader_pos['realized_pnl'].sum()
        if abs(row['total_pnl'] - expected_total) > 0.01:
            issues.append(f"{trader}: total_pnl mismatch")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} summary_metrics.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} summary_metrics.csv is valid")
        return True

def validate_equity_curve():
    """Validate equity_curve.csv drawdown calculations."""
    df = pd.read_csv(OUTPUT_DIR / "equity_curve.csv")
    
    issues = []
    
    for trader in df['trader_id'].unique():
        trader_data = df[df['trader_id'] == trader].sort_values('timestamp')
        
        # Validate drawdown is always <= 0
        if (trader_data['drawdown'] > 0.01).any():
            issues.append(f"{trader}: Positive drawdown found")
        
        # Validate cumulative PnL is monotonic sum
        if not trader_data['cumulative_pnl'].is_monotonic_increasing and not trader_data['cumulative_pnl'].is_monotonic_decreasing:
            # This is actually OK - cumulative can go up and down
            pass
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} equity_curve.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} equity_curve.csv is valid")
        return True

def validate_directional_bias():
    """Validate directional_bias.csv calculations."""
    df = pd.read_csv(OUTPUT_DIR / "directional_bias.csv")
    
    issues = []
    
    for _, row in df.iterrows():
        total = row['long_trades'] + row['short_trades']
        
        if total == 0:
            issues.append(f"{row['trader_id']}: No trades")
            continue
        
        expected_long_ratio = row['long_trades'] / total
        expected_short_ratio = row['short_trades'] / total
        
        if abs(row['long_ratio'] - expected_long_ratio) > 0.01:
            issues.append(f"{row['trader_id']}: long_ratio calculation error")
        
        if abs(row['short_ratio'] - expected_short_ratio) > 0.01:
            issues.append(f"{row['trader_id']}: short_ratio calculation error")
        
        if abs(row['long_ratio'] + row['short_ratio'] - 1.0) > 0.01:
            issues.append(f"{row['trader_id']}: ratios don't sum to 1.0")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} directional_bias.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} directional_bias.csv is valid")
        return True

def main():
    print("\n" + "=" * 60)
    print("DERIVERSE ANALYTICS VALIDATION")
    print("=" * 60 + "\n")
    
    if not OUTPUT_DIR.exists():
        print(f"{bcolors.FAIL}âœ—{bcolors.END} Output directory not found: {OUTPUT_DIR}")
        print("Run: python -m scripts.run_analytics")
        sys.exit(1)
    
    # Check file existence
    print("Checking required files...")
    required_files = [
        'positions.csv',
        'realized_pnl.csv',
        'equity_curve.csv',
        'summary_metrics.csv',
        'volume_by_market.csv',
        'fees_breakdown.csv',
        'pnl_by_day.csv',
        'pnl_by_hour.csv',
        'directional_bias.csv',
        'order_type_performance.csv'
    ]
    
    all_exist = all(validate_file_exists(f) for f in required_files)
    
    if not all_exist:
        print(f"\n{bcolors.FAIL}âœ— Some files are missing{bcolors.END}")
        sys.exit(1)
    
    print("\nValidating data quality...")
    
    validations = [
        validate_positions(),
        validate_summary_metrics(),
        validate_equity_curve(),
        validate_directional_bias()
    ]
    
    print("\n" + "=" * 60)
    if all(validations):
        print(f"{bcolors.OK}âœ“ ALL VALIDATIONS PASSED{bcolors.END}")
        print("=" * 60 + "\n")
        sys.exit(0)
    else:
        print(f"{bcolors.WARN}âš  SOME VALIDATIONS FAILED{bcolors.END}")
        print("=" * 60 + "\n")
        sys.exit(1)

if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\trades\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\analytics_builder.py =====

# src/analytics/analytics_builder.py
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class AnalyticsBuilder:
    """Build all required analytics tables from canonical PnL engine outputs."""
    
    def __init__(self, positions_df: pd.DataFrame, pnl_df: pd.DataFrame, output_dir: Path):
        self.positions = positions_df.copy()
        self.pnl = pnl_df.copy()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Ensure timestamps are datetime
        if not self.positions.empty:
            self.positions['open_time'] = pd.to_datetime(self.positions['open_time'])
            self.positions['close_time'] = pd.to_datetime(self.positions['close_time'])
            self.positions['duration_seconds'] = (
                self.positions['close_time'] - self.positions['open_time']
            ).dt.total_seconds()
    
    def build_all(self):
        """Generate all required analytics outputs."""
        if self.positions.empty:
            logger.warning("No positions data - generating empty outputs")
            self._generate_empty_outputs()
            return
        
        logger.info("Building core truth tables...")
        self._build_positions()
        self._build_realized_pnl()
        
        logger.info("Building performance metrics...")
        self._build_equity_curve()
        self._build_summary_metrics()
        
        logger.info("Building volume & fees analytics...")
        self._build_volume_by_market()
        self._build_fees_breakdown()
        
        logger.info("Building time-based analytics...")
        self._build_pnl_by_day()
        self._build_pnl_by_hour()
        
        logger.info("Building behavioral analytics...")
        self._build_directional_bias()
        self._build_order_type_performance()
        
        # âœ… FIXED: Added Greeks exposure
        logger.info("Building options Greeks...")
        self._build_greeks_exposure()
        
        logger.info(f"âœ… All analytics saved to {self.output_dir}")
    
    def _build_positions(self):
        """1. Core Truth: positions.csv"""
        output = self.positions[[
            'position_id', 'trader_id', 'market_id', 'product_type', 'side',
            'open_time', 'close_time', 'duration_seconds',
            'entry_price', 'exit_price', 'size', 'gross_pnl', 'fees', 'realized_pnl'
        ]].copy()
        output.to_csv(self.output_dir / 'positions.csv', index=False)
    
    def _build_realized_pnl(self):
        """2. Core Truth: realized_pnl.csv"""
        output = self.positions[[
            'close_time', 'trader_id', 'market_id', 'realized_pnl', 'fees'
        ]].copy()
        output.rename(columns={'close_time': 'timestamp'}, inplace=True)
        output['net_pnl'] = output['realized_pnl'] - output['fees']
        output = output[['timestamp', 'trader_id', 'market_id', 'realized_pnl', 'fees', 'net_pnl']]
        output.to_csv(self.output_dir / 'realized_pnl.csv', index=False)
    
    def _build_equity_curve(self):
        """3. Performance: equity_curve.csv"""
        # Aggregate by trader and timestamp
        equity = self.positions.copy()
        equity = equity.sort_values('close_time')
        
        result = []
        for trader in equity['trader_id'].unique():
            trader_data = equity[equity['trader_id'] == trader].copy()
            trader_data['cumulative_pnl'] = trader_data['realized_pnl'].cumsum()
            trader_data['rolling_max'] = trader_data['cumulative_pnl'].cummax()
            trader_data['drawdown'] = trader_data['cumulative_pnl'] - trader_data['rolling_max']
            
            for _, row in trader_data.iterrows():
                result.append({
                    'timestamp': row['close_time'],
                    'trader_id': row['trader_id'],
                    'net_realized_pnl': row['realized_pnl'],
                    'cumulative_pnl': row['cumulative_pnl'],
                    'drawdown': row['drawdown']
                })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'equity_curve.csv', index=False)
    
    def _build_summary_metrics(self):
        """4. Performance: summary_metrics.csv"""
        result = []
        
        for trader in self.positions['trader_id'].unique():
            trader_pos = self.positions[self.positions['trader_id'] == trader]
            
            winning = trader_pos[trader_pos['realized_pnl'] > 0]
            losing = trader_pos[trader_pos['realized_pnl'] < 0]
            
            # Calculate metrics
            total_pnl = trader_pos['realized_pnl'].sum()
            total_fees = trader_pos['fees'].sum()
            trade_count = len(trader_pos)
            win_rate = len(winning) / trade_count if trade_count > 0 else 0
            avg_win = winning['realized_pnl'].mean() if len(winning) > 0 else 0
            avg_loss = losing['realized_pnl'].mean() if len(losing) > 0 else 0
            best_trade = trader_pos['realized_pnl'].max()
            worst_trade = trader_pos['realized_pnl'].min()
            avg_duration = trader_pos['duration_seconds'].mean()
            
            # Directional bias
            long_trades = trader_pos[trader_pos['side'].isin(['long', 'buy'])]
            short_trades = trader_pos[trader_pos['side'].isin(['short', 'sell'])]
            long_ratio = len(long_trades) / trade_count if trade_count > 0 else 0
            short_ratio = len(short_trades) / trade_count if trade_count > 0 else 0
            
            # Max drawdown
            trader_sorted = trader_pos.sort_values('close_time')
            cum_pnl = trader_sorted['realized_pnl'].cumsum()
            rolling_max = cum_pnl.cummax()
            drawdown = cum_pnl - rolling_max
            max_drawdown = drawdown.min()
            
            # âœ… FIXED: Add risk-adjusted metrics to match summary.py
            # Get trader-specific PnL for risk metrics
            trader_pnl = self.pnl[self.pnl['trader_id'] == trader].copy()
            if not trader_pnl.empty and len(trader_pnl) > 1:
                trader_pnl['date'] = pd.to_datetime(trader_pnl['timestamp']).dt.date
                daily_returns = trader_pnl.groupby('date')['net_pnl'].sum()
                
                # Sharpe Ratio
                mean_return = daily_returns.mean()
                std_return = daily_returns.std()
                sharpe_ratio = mean_return / std_return if std_return > 0 else 0
                
                # Sortino Ratio
                downside_returns = daily_returns[daily_returns < 0]
                downside_std = downside_returns.std() if len(downside_returns) > 0 else std_return
                sortino_ratio = mean_return / downside_std if downside_std > 0 else 0
            else:
                sharpe_ratio = 0
                sortino_ratio = 0
            
            result.append({
                'trader_id': trader,
                'total_pnl': total_pnl,
                'total_fees': total_fees,
                'trade_count': trade_count,
                'win_rate': win_rate,
                'avg_win': avg_win,
                'avg_loss': avg_loss,
                'best_trade': best_trade,
                'worst_trade': worst_trade,
                'avg_duration_seconds': avg_duration,
                'long_ratio': long_ratio,
                'short_ratio': short_ratio,
                'max_drawdown': max_drawdown,
                'sharpe_ratio': sharpe_ratio,
                'sortino_ratio': sortino_ratio  # âœ… ADDED
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'summary_metrics.csv', index=False)
    
    def _build_volume_by_market(self):
        """5. Volume: volume_by_market.csv"""
        result = []
        
        for (market, product), group in self.positions.groupby(['market_id', 'product_type']):
            total_volume = (group['exit_price'] * group['size']).sum()
            trade_count = len(group)
            
            result.append({
                'market_id': market,
                'product_type': product,
                'total_volume': total_volume,
                'trade_count': trade_count
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'volume_by_market.csv', index=False)
    
    def _build_fees_breakdown(self):
        """6. Fees: fees_breakdown.csv"""
        result = []
        
        for (trader, product), group in self.positions.groupby(['trader_id', 'product_type']):
            total_fees = group['fees'].sum()
            
            result.append({
                'trader_id': trader,
                'product_type': product,
                'total_fees': total_fees
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'fees_breakdown.csv', index=False)
    
    def _build_pnl_by_day(self):
        """7. Time Analytics: pnl_by_day.csv"""
        df = self.positions.copy()
        df['date'] = df['close_time'].dt.date
        
        result = []
        for (date, trader), group in df.groupby(['date', 'trader_id']):
            daily_pnl = group['realized_pnl'].sum()
            
            # Calculate cumulative PnL up to this date
            trader_data = df[df['trader_id'] == trader]
            trader_data = trader_data[trader_data['date'] <= date]
            cumulative_pnl = trader_data['realized_pnl'].sum()
            
            result.append({
                'date': date,
                'trader_id': trader,
                'daily_pnl': daily_pnl,
                'cumulative_pnl': cumulative_pnl
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'pnl_by_day.csv', index=False)
    
    def _build_pnl_by_hour(self):
        """8. Time Analytics: pnl_by_hour.csv"""
        df = self.positions.copy()
        df['hour_of_day'] = df['close_time'].dt.hour
        
        result = []
        for (hour, trader), group in df.groupby(['hour_of_day', 'trader_id']):
            avg_pnl = group['realized_pnl'].mean()
            trade_count = len(group)
            
            result.append({
                'hour_of_day': hour,
                'trader_id': trader,
                'avg_pnl': avg_pnl,
                'trade_count': trade_count
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'pnl_by_hour.csv', index=False)
    
    def _build_directional_bias(self):
        """9. Behavioral: directional_bias.csv"""
        result = []
        
        for trader, group in self.positions.groupby('trader_id'):
            long_trades = len(group[group['side'].isin(['long', 'buy'])])
            short_trades = len(group[group['side'].isin(['short', 'sell'])])
            total = long_trades + short_trades
            
            result.append({
                'trader_id': trader,
                'long_trades': long_trades,
                'short_trades': short_trades,
                'long_ratio': long_trades / total if total > 0 else 0,
                'short_ratio': short_trades / total if total > 0 else 0
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'directional_bias.csv', index=False)
    
    def _build_order_type_performance(self):
        """10. Behavioral: order_type_performance.csv"""
        df = self.positions.copy()
        
        # Read order_type from events if available
        # For now, assign based on heuristics since we don't track it in positions
        # This would need to be joined from original events in production
        
        # Simple heuristic: classify by duration
        df['order_type'] = df['duration_seconds'].apply(lambda x:
            'market' if x < 300 else  # < 5 min
            'limit' if x < 3600 else   # < 1 hour
            'stop'                      # > 1 hour
        )
        
        result = []
        for order_type, group in df.groupby('order_type'):
            trade_count = len(group)
            avg_pnl = group['realized_pnl'].mean()
            win_rate = (group['realized_pnl'] > 0).mean()
            
            result.append({
                'order_type': order_type,
                'trade_count': trade_count,
                'avg_pnl': avg_pnl,  # âœ… FIXED: Use 'avg_pnl' not 'avg_pnl' (already correct)
                'win_rate': win_rate
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'order_type_performance.csv', index=False)
    
    # âœ… NEW: Greeks Exposure - Fixed naming consistency
    def _build_greeks_exposure(self):
        """11. Greeks: greeks_exposure.csv (options only)"""
        options = self.positions[self.positions['product_type'] == 'option'].copy()
        
        if options.empty:
            pd.DataFrame().to_csv(self.output_dir / 'greeks_exposure.csv', index=False)
            return
        
        result = []
        for trader in options['trader_id'].unique():
            trader_opts = options[options['trader_id'] == trader]
            
            # Calculate net delta (simplified: +1 for long call, -1 for short call, etc.)
            net_delta = 0
            for _, opt in trader_opts.iterrows():
                # Simplified delta calculation - in production this would come from events
                if opt['side'] in ['buy', 'long']:
                    delta_sign = 1
                else:
                    delta_sign = -1
                net_delta += delta_sign * opt['size'] * 0.5  # Assume 0.5 delta for demo
            
            result.append({
                'trader_id': trader,
                'total_option_positions': len(trader_opts),
                'net_delta': net_delta,
                'gamma_exposure': 0,  # Placeholder - would need gamma from events
                'theta_decay': 0      # Placeholder - would need theta from events
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'greeks_exposure.csv', index=False)
    
    def _generate_empty_outputs(self):
        """Generate empty CSV files when no data available."""
        empty_files = [
            'positions.csv', 'realized_pnl.csv', 'equity_curve.csv',
            'summary_metrics.csv', 'volume_by_market.csv', 'fees_breakdown.csv',
            'pnl_by_day.csv', 'pnl_by_hour.csv', 'directional_bias.csv',
            'order_type_performance.csv',
            'greeks_exposure.csv'  # âœ… FIXED: Added to empty outputs
        ]
        
        for filename in empty_files:
            pd.DataFrame().to_csv(self.output_dir / filename, index=False)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\debug.py =====

import json
from pathlib import Path
from collections import Counter

def inspect_normalized_data():
    normalized_path = Path("data/normalized/events.jsonl")

    if not normalized_path.exists():
        print("âŒ No normalized data file found.")
        return

    if normalized_path.stat().st_size == 0:
        print("âš ï¸ Normalized file exists but is empty.")
        return

    all_fields = set()
    event_types = Counter()
    sample_events = []

    with normalized_path.open() as f:
        for i, line in enumerate(f):
            event = json.loads(line)
            all_fields.update(event.keys())
            event_types[event.get("event_type")] += 1

            if i < 3:
                sample_events.append(event)

    print("\nðŸ“Š Normalized Data Overview")
    print(f"Total events: {sum(event_types.values())}")

    print("\nEvent types:")
    for et, c in event_types.items():
        print(f"  - {et}: {c}")

    print("\nColumns present:")
    for field in sorted(all_fields):
        print(f"  - {field}")

    print("\nSample events:")
    for i, ev in enumerate(sample_events, 1):
        print(f"\nEvent {i}")
        for k, v in ev.items():
            print(f"  {k}: {v}")


if __name__ == "__main__":
    inspect_normalized_data()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\pnl_engine.py =====

# src/analytics/pnl_engine.py
import pandas as pd
import hashlib
import logging

logger = logging.getLogger(__name__)


def compute_realized_pnl(events: pd.DataFrame):
    """
    Canonical PnL engine with full options lifecycle support.
    
    Supports:
    - Spot: buy/sell (different sides for open/close)
    - Perps: long/short (same side for open/close)
    - Options: buy/sell/exercise/expire (different sides)
    
    Options PnL logic:
    - buy: Pay premium (negative cash flow)
    - sell: Receive premium or exit price (positive cash flow)
    - exercise: Intrinsic value - fees
    - expire: Total loss of premium paid
    """

    required_cols = {
        "event_type", "timestamp", "trader_id",
        "market_id", "product_type", "side",
        "price", "size", "fee"
    }

    missing = required_cols - set(events.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    events = events.sort_values("timestamp")

    open_positions = {}
    closed_positions = []

    # Validation stats
    stats = {
        "duplicate_opens": 0,
        "close_without_open": 0,
        "oversized_closes": 0,
    }

    for _, event in events.iterrows():

        # POSITION KEY GENERATION
        # For options that open with SELL (short positions), we need to track them differently

        if event["product_type"] == "perp":
            # Perps: Include side (long stays long, short stays short)
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"],
                event["side"]
            )
        elif event["product_type"] == "option":
            # Options: Exclude side from key
            # BUT track the opening side in the position data
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"]
            )
        else:  # spot
            # Spot: Exclude side (buy opens, sell closes)
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"]
            )

        # ==========================================
        # OPEN POSITION
        # ==========================================
        if event["event_type"] == "open":
            if key in open_positions:
                stats["duplicate_opens"] += 1
                continue

            position_data = (
                f"{event['trader_id']}|{event['market_id']}|"
                f"{event['timestamp']}|{event.get('side', '')}"
            )
            position_id = hashlib.sha256(position_data.encode()).hexdigest()[:16]

            open_positions[key] = {
                "position_id": position_id,
                "open_time": event["timestamp"],
                "entry_price": event["price"],
                "size": event["size"],
                "fees": event["fee"],
                "trader_id": event["trader_id"],
                "market_id": event["market_id"],
                "product_type": event["product_type"],
                "side": event["side"],
            }
            
            # Store option-specific fields
            if event["product_type"] == "option":
                open_positions[key]["option_type"] = event.get("option_type")
                open_positions[key]["strike"] = event.get("strike")
                open_positions[key]["expiry"] = event.get("expiry")

        # ==========================================
        # CLOSE POSITION (close, liquidation, exercise, expire)
        # ==========================================
        elif event["event_type"] in {"close", "liquidation", "exercise", "expire"}:
            if key not in open_positions:
                stats["close_without_open"] += 1
                continue

            pos = open_positions[key]
            close_size = event["size"]

            if close_size > pos["size"]:
                stats["oversized_closes"] += 1
                continue

            # Calculate fees
            fee_ratio = close_size / pos["size"]
            allocated_open_fee = pos["fees"] * fee_ratio
            close_fee = event.get("fee", 0)
            total_fees = allocated_open_fee + close_fee

            # ==========================================
            # OPTIONS PNL CALCULATION
            # ==========================================
            if pos["product_type"] == "option":
                gross_pnl = calculate_option_pnl(
                    event_type=event["event_type"],
                    option_type=pos.get("option_type"),
                    side=pos["side"],
                    entry_price=pos["entry_price"],
                    exit_price=event.get("price", 0),
                    strike=pos.get("strike"),
                    underlying_price=event.get("underlying_price"),
                    size=close_size
                )
                net_pnl = gross_pnl - total_fees
                exit_price = event.get("price", 0)

            # ==========================================
            # SPOT/PERP PNL CALCULATION
            # ==========================================
            else:
                exit_price = event["price"]

                # If PnL provided in event, use it (already net)
                if pd.notna(event.get("pnl")):
                    net_pnl = event["pnl"]
                    gross_pnl = net_pnl + total_fees
                else:
                    # Calculate from price difference
                    if pos["side"] in {"long", "buy"}:
                        gross_pnl = (exit_price - pos["entry_price"]) * close_size
                    else:  # short/sell
                        gross_pnl = (pos["entry_price"] - exit_price) * close_size

                    net_pnl = gross_pnl - total_fees

            # Record closed position
            closed_positions.append({
                "position_id": pos["position_id"],
                "open_time": pos["open_time"],
                "close_time": event["timestamp"],
                "trader_id": pos["trader_id"],
                "market_id": pos["market_id"],
                "product_type": pos["product_type"],
                "side": pos["side"],
                "entry_price": pos["entry_price"],
                "exit_price": exit_price,
                "size": close_size,
                "gross_pnl": round(gross_pnl, 4),
                "net_pnl": round(net_pnl, 4),
                "realized_pnl": round(net_pnl, 4),
                "fees": round(total_fees, 4),
                "close_reason": event["event_type"],
            })

            # Update or remove position
            pos["size"] -= close_size
            pos["fees"] -= allocated_open_fee

            if pos["size"] <= 0:
                open_positions.pop(key)

    positions_df = pd.DataFrame(closed_positions)

    # Log validation summary
    logger.info(
        "PnL validation summary | "
        f"duplicate_opens={stats['duplicate_opens']} | "
        f"close_without_open={stats['close_without_open']} | "
        f"oversized_closes={stats['oversized_closes']}"
    )

    if positions_df.empty:
        return positions_df, pd.DataFrame()

    # Build daily PnL aggregates
    pnl_df = (
        positions_df
        .assign(date=lambda df: pd.to_datetime(df["close_time"]).dt.date)
        .groupby(
            ["date", "trader_id", "market_id", "product_type"],
            as_index=False
        )
        .agg(
            net_pnl=("net_pnl", "sum"),
            realized_pnl=("realized_pnl", "sum"),
            fees=("fees", "sum"),
            trade_count=("position_id", "count")
        )
    )

    logger.info(
        f"PnL engine results: {len(positions_df)} closed positions, "
        f"{len(open_positions)} still open"
    )

    return positions_df, pnl_df


def calculate_option_pnl(
    event_type: str,
    option_type: str,
    side: str,
    entry_price: float,
    exit_price: float,
    strike: float,
    underlying_price: float,
    size: float
) -> float:
    """
    Calculate options PnL based on event type.
    
    Args:
        event_type: close, exercise, or expire
        option_type: call or put
        side: buy or sell
        entry_price: Premium paid when opening
        exit_price: Premium received when closing (if close event)
        strike: Strike price
        underlying_price: Current underlying price (for exercise/expire)
        size: Number of contracts
    
    Returns:
        Gross PnL (before fees)
    """
    
    # ==========================================
    # CASE 1: NORMAL CLOSE (sell option back)
    # ==========================================
    if event_type == "close":
        if side == "buy":
            # Bought option, now selling it back
            gross_pnl = (exit_price - entry_price) * size
        else:  # side == "sell"
            # Sold option, now buying it back
            gross_pnl = (entry_price - exit_price) * size
        return gross_pnl
    
    # ==========================================
    # CASE 2: EXERCISE (convert to underlying)
    # ==========================================
    elif event_type == "exercise":
        if side == "buy":
            # Long option holder exercises
            if option_type == "call":
                # Call: Right to BUY at strike
                intrinsic_value = max(0, underlying_price - strike)
            else:  # put
                # Put: Right to SELL at strike
                intrinsic_value = max(0, strike - underlying_price)
            
            # PnL = intrinsic value - premium paid
            gross_pnl = (intrinsic_value - entry_price) * size
        else:
            # Short option holder (assigned) - opposite side
            if option_type == "call":
                intrinsic_value = max(0, underlying_price - strike)
            else:
                intrinsic_value = max(0, strike - underlying_price)
            
            # PnL = premium received - intrinsic value paid out
            gross_pnl = (entry_price - intrinsic_value) * size
        
        return gross_pnl
    
    # ==========================================
    # CASE 3: EXPIRE (worthless)
    # ==========================================
    elif event_type == "expire":
        if side == "buy":
            # Long option expires worthless - lose premium
            gross_pnl = -entry_price * size
        else:  # side == "sell"
            # Short option expires worthless - keep premium
            gross_pnl = entry_price * size
        
        return gross_pnl
    
    # Fallback
    return 0.0


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\summary.py =====

# src/analytics/summary.py
import pandas as pd

def compute_executive_summary(positions: pd.DataFrame, pnl: pd.DataFrame) -> dict:
    """
    Compute high-level KPIs from canonical PnL outputs.
    
    Args:
        positions: Output from compute_realized_pnl (positions_df)
        pnl: Output from compute_realized_pnl (pnl_df)
    
    Returns:
        Dictionary of KPI metrics
    """
    if positions.empty:
        return {"status": "no_data"}
    
    summary = {}
    
    # Core PnL
    summary["total_pnl"] = pnl["net_pnl"].sum()
    summary["total_fees"] = pnl["fees"].sum()
    summary["trade_count"] = len(positions)
    summary["win_rate"] = (positions["net_pnl"] > 0).mean()
    
    # Win/Loss Analysis
    winning_trades = positions[positions["net_pnl"] > 0]
    losing_trades = positions[positions["net_pnl"] < 0]
    
    summary["avg_win"] = winning_trades["net_pnl"].mean() if len(winning_trades) > 0 else 0
    summary["avg_loss"] = losing_trades["net_pnl"].mean() if len(losing_trades) > 0 else 0
    summary["best_trade"] = positions["net_pnl"].max()
    summary["worst_trade"] = positions["net_pnl"].min()
    
    # Duration Analysis
    positions = positions.copy()
    positions["duration"] = (
        pd.to_datetime(positions["close_time"]) - 
        pd.to_datetime(positions["open_time"])
    )
    summary["avg_duration"] = positions["duration"].mean()
    
    # Directional Bias
    summary["long_ratio"] = (positions["side"].isin(["long", "buy"])).mean()
    summary["short_ratio"] = (positions["side"].isin(["short", "sell"])).mean()
    
    # Drawdown
    pnl_sorted = pnl.sort_values("date")
    pnl_sorted["cum_pnl"] = pnl_sorted["net_pnl"].cumsum()
    pnl_sorted["drawdown"] = pnl_sorted["cum_pnl"] - pnl_sorted["cum_pnl"].cummax()
    summary["max_drawdown"] = pnl_sorted["drawdown"].min()
    
    # âœ… NEW: Risk-Adjusted Returns
    if not pnl.empty and len(pnl) > 1:
        daily_returns = pnl.groupby('date')['net_pnl'].sum()
        
        # Sharpe Ratio (assuming risk-free rate = 0)
        mean_return = daily_returns.mean()
        std_return = daily_returns.std()
        sharpe_ratio = mean_return / std_return if std_return > 0 else 0
        summary["sharpe_ratio"] = sharpe_ratio
        
        # Sortino Ratio (downside deviation only)
        downside_returns = daily_returns[daily_returns < 0]
        downside_std = downside_returns.std() if len(downside_returns) > 0 else std_return
        sortino_ratio = mean_return / downside_std if downside_std > 0 else 0
        summary["sortino_ratio"] = sortino_ratio
    else:
        summary["sharpe_ratio"] = 0
        summary["sortino_ratio"] = 0
    
    return summary


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\validate.py =====

# src/analytics/validate.py
from typing import Dict, Any, Set, List, Optional
from datetime import datetime
import pandas as pd

# Schema constants 
PRODUCT_TYPES = {"spot", "perp", "option"}

ALLOWED_EVENT_TYPES = {"trade", "open", "close", "exercise", "expire", "liquidation"}

# âœ… Consolidated side validation
ALLOWED_SIDES = {
    "spot": {"buy", "sell"},
    "perp": {"long", "short"},
    "option": {"buy", "sell", "long", "short", "exercise", "expire"}
}

# Market schema registry
MARKET_SCHEMAS = {
    "BTC-USD": {"product_types": {"spot", "perp", "option"}, "min_size": 0.0001, "max_size": 100.0},
    "ETH-USD": {"product_types": {"spot", "perp", "option"}, "min_size": 0.01, "max_size": 1000.0},
    "AAPL": {"product_types": {"spot", "option"}, "min_size": 1.0, "max_size": 100000.0},
    "MSFT": {"product_types": {"spot", "option"}, "min_size": 1.0, "max_size": 100000.0},
    "TSLA": {"product_types": {"spot", "option"}, "min_size": 1.0, "max_size": 100000.0}
}

class EventValidationError(Exception):
    """Raised when event fails validation."""
    pass

# --- Base required fields for ALL events ---
BASE_REQUIRED_FIELDS = {
    "event_id",
    "event_type",
    "timestamp",
    "trader_id",
    "market_id",
    "product_type"
}

# --- Base optional fields for ALL events ---
BASE_OPTIONAL_FIELDS = {
    "side",
    "price",
    "size",
    "fee",
    "pnl",
    "position_id"  # Added for position tracking
}

# --- Option-specific fields ---
OPTION_REQUIRED_FIELDS = {
    "option_type",
    "strike",
    "expiry"
}

OPTION_OPTIONAL_FIELDS = {
    "delta",
    "gamma",
    "theta",
    "vega",
    "implied_vol",
    "underlying_price"
}

# --- Event type schemas ---
EVENT_TYPE_SCHEMAS = {
    "trade": {
        "required": {"side", "price", "size", "fee"},
        "optional": {"pnl", "position_id"}
    },
    "open": {
        "required": {"side", "price", "size", "fee"},
        "optional": {"pnl", "position_id"}
    },
    "close": {
        "required": {"side", "price", "size", "fee"},
        "optional": {"pnl", "position_id"}
    },
    "exercise": {
        "required": {"side", "size", "fee"},
        "optional": {"price", "pnl", "underlying_price", "position_id"}
    },
    "expire": {
        "required": {"side", "size"},
        "optional": {"price", "fee", "pnl", "underlying_price", "position_id"}
    },
    "liquidation": {
        "required": {"side", "size", "price"},
        "optional": {"fee", "pnl", "position_id"}
    }
}


# âœ… NEW: Position Lifecycle Validator
class PositionLifecycleValidator:
    """
    Validate events in the context of existing positions.
    Tracks open positions and validates state transitions.
    """
    
    def __init__(self):
        self.open_positions = {}  # position_id -> position state
        self.position_histories = {}  # position_id -> list of events
    
    def validate_event(self, event: dict, position_state: Optional[dict] = None) -> None:
        """
        Validate that event is valid given current position state.
        
        Raises:
            EventValidationError: If transition is invalid
        """
        event_type = event.get("event_type")
        product_type = event.get("product_type")
        position_id = event.get("position_id")
        
        # Skip validation for informational trades
        if event_type == "trade":
            return
        
        # Case 1: No position exists or position not tracked
        if position_state is None and position_id and position_id not in self.open_positions:
            if event_type not in {"open"}:
                raise EventValidationError(
                    f"Cannot {event_type} - position {position_id} does not exist"
                )
        
        # Case 2: Position exists
        elif position_id and position_id in self.open_positions:
            position = self.open_positions[position_id]
            
            if event_type == "open":
                raise EventValidationError(
                    f"Cannot open - position {position_id} is already open"
                )
            
            # Validate closing/expiring/exercising size
            if "size" in event and event["size"] is not None:
                remaining_size = position.get("remaining_size", position.get("size", 0))
                requested_size = abs(event["size"])
                
                if requested_size > remaining_size + 1e-6:  # Allow small floating point diff
                    raise EventValidationError(
                        f"Cannot {event_type} {requested_size} units - "
                        f"only {remaining_size} remaining in position {position_id}"
                    )
            
            # Validate side matches position
            if event.get("side") and position.get("side"):
                # Convert buy/sell to long/short for consistency
                event_side = event["side"]
                position_side = position["side"]
                
                # Check if closing/exercise should be opposite side
                if event_type in {"close", "exercise"}:
                    expected_side = "sell" if position_side in {"buy", "long"} else "buy"
                    if event_side != expected_side:
                        raise EventValidationError(
                            f"Cannot {event_type} position with side '{event_side}' - "
                            f"expected '{expected_side}' for position side '{position_side}'"
                        )
    
    def update_position(self, event: dict) -> None:
        """Update position state based on event."""
        event_type = event.get("event_type")
        position_id = event.get("position_id")
        
        if not position_id:
            return
        
        if event_type == "open":
            # Initialize new position
            self.open_positions[position_id] = {
                "position_id": position_id,
                "trader_id": event.get("trader_id"),
                "market_id": event.get("market_id"),
                "product_type": event.get("product_type"),
                "side": event.get("side"),
                "size": abs(event.get("size", 0)),
                "remaining_size": abs(event.get("size", 0)),
                "entry_price": event.get("price"),
                "open_time": event.get("timestamp")
            }
            self.position_histories[position_id] = [event]
        
        elif position_id in self.open_positions:
            position = self.open_positions[position_id]
            self.position_histories.setdefault(position_id, []).append(event)
            
            if "size" in event and event["size"] is not None:
                position["remaining_size"] -= abs(event["size"])
            
            # Remove position if fully closed
            if event_type in {"close", "exercise", "expire", "liquidation"}:
                if position["remaining_size"] <= 1e-6:  # Consider closed
                    del self.open_positions[position_id]


# âœ… NEW: Data Quality Validator
class DataQualityValidator:
    """
    Validate data quality across entire dataset.
    Produces warnings for potential issues.
    """
    
    def __init__(self):
        self.warnings = []
        self.errors = []
        self.metrics = {}
    
    def validate_dataset(self, events: List[dict]) -> Dict[str, Any]:
        """Validate entire dataset and return quality report."""
        self.warnings = []
        self.errors = []
        
        if not events:
            self.warnings.append("Empty dataset")
            return self._get_report()
        
        # Check chronological order
        timestamps = []
        for event in events:
            try:
                ts = event.get("timestamp")
                if ts:
                    if ts.endswith("Z"):
                        ts = ts.replace("Z", "+00:00")
                    timestamps.append(datetime.fromisoformat(ts))
            except (ValueError, AttributeError):
                pass
        
        if len(timestamps) > 1:
            if not all(t1 <= t2 for t1, t2 in zip(timestamps, timestamps[1:])):
                self.warnings.append("Events not in chronological order")
        
        # Check for missing position_id in close events
        close_events = [e for e in events if e.get("event_type") == "close"]
        for event in close_events:
            if "position_id" not in event:
                self.warnings.append(
                    f"Close event {event.get('event_id')} missing position_id"
                )
        
        # Check for zero prices
        for event in events:
            if event.get("price") == 0:
                self.warnings.append(
                    f"Event {event.get('event_id')} has zero price"
                )
            
            # Check for negative sizes
            if event.get("size", 0) < 0:
                self.warnings.append(
                    f"Event {event.get('event_id')} has negative size"
                )
        
        # Calculate metrics
        self.metrics = {
            "total_events": len(events),
            "unique_traders": len(set(e.get("trader_id") for e in events if e.get("trader_id"))),
            "unique_markets": len(set(e.get("market_id") for e in events if e.get("market_id"))),
            "event_type_counts": pd.Series([e.get("event_type") for e in events]).value_counts().to_dict(),
            "product_type_counts": pd.Series([e.get("product_type") for e in events]).value_counts().to_dict()
        }
        
        return self._get_report()
    
    def _get_report(self) -> Dict[str, Any]:
        """Return complete quality report."""
        return {
            "valid": len(self.errors) == 0,
            "errors": self.errors,
            "warnings": self.warnings,
            "metrics": self.metrics
        }


def validate_event(event: dict) -> None:
    """
    Validate event schema and data quality.
    
    Raises:
        EventValidationError: If validation fails
    """
    event_type = event.get("event_type")
    product_type = event.get("product_type")

    # --------------------------------------------------
    # 1ï¸âƒ£ Trade events are informational only - skip position validation
    # --------------------------------------------------
    if event_type == "trade":
        return

    # --------------------------------------------------
    # 2ï¸âƒ£ Validate product type
    # --------------------------------------------------
    if product_type not in PRODUCT_TYPES:
        raise EventValidationError(
            f"Invalid product_type: {product_type}. Allowed: {PRODUCT_TYPES}"
        )

    # --------------------------------------------------
    # 3ï¸âƒ£ Product-specific side validation
    # --------------------------------------------------
    if product_type in ALLOWED_SIDES:
        allowed_sides = ALLOWED_SIDES[product_type]
        side = event.get("side")
        if side and side not in allowed_sides:
            raise EventValidationError(
                f"Invalid side '{side}' for product_type '{product_type}'. "
                f"Must be one of: {allowed_sides}"
            )

    # --------------------------------------------------
    # 4ï¸âƒ£ Build allowed fields based on product type
    # --------------------------------------------------
    allowed_fields = BASE_REQUIRED_FIELDS | BASE_OPTIONAL_FIELDS
    
    if product_type == "option":
        allowed_fields |= OPTION_REQUIRED_FIELDS | OPTION_OPTIONAL_FIELDS
    
    # Add event-specific fields
    if event_type in EVENT_TYPE_SCHEMAS:
        schema = EVENT_TYPE_SCHEMAS[event_type]
        allowed_fields |= schema["required"] | schema["optional"]

    # --------------------------------------------------
    # 5ï¸âƒ£ Check for extra fields (schema drift)
    # --------------------------------------------------
    extra_fields = set(event.keys()) - allowed_fields
    if extra_fields:
        raise EventValidationError(
            f"Unexpected fields detected: {extra_fields}. "
            f"Allowed for {product_type}/{event_type}: {allowed_fields}"
        )

    # --------------------------------------------------
    # 6ï¸âƒ£ Check event-type-specific required fields
    # --------------------------------------------------
    if event_type in EVENT_TYPE_SCHEMAS:
        schema = EVENT_TYPE_SCHEMAS[event_type]
        missing_required = schema["required"] - set(event.keys())
        if missing_required:
            raise EventValidationError(
                f"Event type '{event_type}' missing required fields: {missing_required}"
            )

    # --------------------------------------------------
    # 7ï¸âƒ£ Check option-specific required fields
    # --------------------------------------------------
    if product_type == "option":
        missing_option_required = OPTION_REQUIRED_FIELDS - set(event.keys())
        if missing_option_required:
            raise EventValidationError(
                f"Option product missing required fields: {missing_option_required}"
            )

    # --------------------------------------------------
    # 8ï¸âƒ£ Validate timestamp format
    # --------------------------------------------------
    try:
        timestamp_str = event["timestamp"]
        if timestamp_str.endswith("Z"):
            timestamp_str = timestamp_str.replace("Z", "+00:00")
        datetime.fromisoformat(timestamp_str)
    except (ValueError, AttributeError, TypeError) as e:
        raise EventValidationError(f"Invalid timestamp format: {event.get('timestamp')} - {e}")

    # --------------------------------------------------
    # 9ï¸âƒ£ Validate numeric fields
    # --------------------------------------------------
    numeric_fields = {"price", "size", "fee", "pnl", "strike", "delta", "gamma", 
                     "theta", "vega", "implied_vol", "underlying_price"}
    for field in numeric_fields & event.keys():
        value = event[field]
        if value is not None and not isinstance(value, (int, float)):
            raise EventValidationError(
                f"Field '{field}' must be numeric or null, got {type(value)}: {value}"
            )

    # --------------------------------------------------
    # ðŸ”Ÿ Validate option-specific values
    # --------------------------------------------------
    if product_type == "option":
        # Validate option_type
        option_type = event.get("option_type")
        if option_type and option_type not in {"call", "put"}:
            raise EventValidationError(f"Invalid option_type: {option_type}. Must be 'call' or 'put'")
        
        # Validate expiry format
        expiry = event.get("expiry")
        if expiry:
            try:
                if expiry.endswith("Z"):
                    expiry = expiry.replace("Z", "+00:00")
                datetime.fromisoformat(expiry)
            except (ValueError, AttributeError):
                raise EventValidationError(f"Invalid expiry format: {expiry}")
    
    # --------------------------------------------------
    # 1ï¸âƒ£1ï¸âƒ£ Validate market-specific rules
    # --------------------------------------------------
    market_id = event.get("market_id")
    if market_id in MARKET_SCHEMAS:
        schema = MARKET_SCHEMAS[market_id]
        
        # Check if product is allowed for this market
        if product_type not in schema["product_types"]:
            raise EventValidationError(
                f"Product '{product_type}' not supported for market {market_id}. "
                f"Allowed: {schema['product_types']}"
            )
        
        # Check size limits
        size = event.get("size", 0)
        if size:
            if size < schema["min_size"] - 1e-6:  # Allow small floating point diff
                raise EventValidationError(
                    f"Size {size} below minimum {schema['min_size']} for {market_id}"
                )
            if size > schema["max_size"] + 1e-6:
                raise EventValidationError(
                    f"Size {size} above maximum {schema['max_size']} for {market_id}"
                )


def validate_events_batch(events: List[dict]) -> List[Dict[str, Any]]:
    """
    Validate batch of events, collecting all errors instead of failing fast.
    
    Args:
        events: List of event dictionaries
    
    Returns:
        List of validation results with errors/warnings per event
    """
    results = []
    position_validator = PositionLifecycleValidator()
    
    for event in events:
        result = {
            "event_id": event.get("event_id"),
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Schema validation
        try:
            validate_event(event)
        except EventValidationError as e:
            result["valid"] = False
            result["errors"].append(str(e))
        
        # Position lifecycle validation (if event passes schema)
        if result["valid"]:
            try:
                position_state = position_validator.open_positions.get(event.get("position_id"))
                position_validator.validate_event(event, position_state)
                position_validator.update_position(event)
            except EventValidationError as e:
                result["valid"] = False
                result["errors"].append(str(e))
        
        results.append(result)
    
    return results


def generate_validation_report(events: List[dict]) -> Dict[str, Any]:
    """
    Generate comprehensive validation report for a dataset.
    
    Args:
        events: List of event dictionaries
    
    Returns:
        Complete validation report with schema validation, position lifecycle,
        and data quality results
    """
    # Schema and position validation
    validation_results = validate_events_batch(events)
    
    # Data quality validation
    quality_validator = DataQualityValidator()
    quality_report = quality_validator.validate_dataset(events)
    
    # Compile report
    valid_events = [r for r in validation_results if r["valid"]]
    invalid_events = [r for r in validation_results if not r["valid"]]
    
    report = {
        "summary": {
            "total_events": len(events),
            "valid_events": len(valid_events),
            "invalid_events": len(invalid_events),
            "validity_rate": len(valid_events) / len(events) if events else 0
        },
        "invalid_events": invalid_events,
        "quality_report": quality_report,
        "error_categories": _categorize_errors(invalid_events)
    }
    
    return report


def _categorize_errors(invalid_events: List[Dict]) -> Dict[str, int]:
    """Categorize errors by type for reporting."""
    categories = {}
    
    for event in invalid_events:
        for error in event["errors"]:
            if "missing required fields" in error:
                cat = "missing_fields"
            elif "Unexpected fields" in error:
                cat = "extra_fields"
            elif "Invalid side" in error:
                cat = "invalid_side"
            elif "position" in error.lower():
                cat = "position_lifecycle"
            elif "timestamp" in error.lower():
                cat = "timestamp"
            elif "market" in error.lower():
                cat = "market_validation"
            else:
                cat = "other"
            
            categories[cat] = categories.get(cat, 0) + 1
    
    return categories


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\common\logging.py =====

import logging

def get_logger(name):
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\common\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\normalizer.py =====

# src/ingestion/normalizer.py
from typing import Dict, Any
from datetime import datetime, timezone
import hashlib

def normalize_event(raw_event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalize raw event data into canonical schema.
    - Convert keys to expected schema names
    - Convert timestamps to ISO 8601
    - Ensure event_id exists
    - Handle option-specific fields
    - Normalize position terminology (longâ†’buy, shortâ†’sell for options/spot)
    """
    event = raw_event.copy()

    # --- Normalize timestamp ---
    ts = event.get("timestamp")
    if isinstance(ts, (int, float)):
        # Convert Unix timestamp (seconds) to ISO 8601 UTC
        event["timestamp"] = datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()
    elif isinstance(ts, datetime):
        # Convert datetime object to ISO string
        event["timestamp"] = ts.isoformat()
    elif isinstance(ts, str):
        # Ensure proper ISO format with timezone
        try:
            # Handle different formats
            ts_clean = ts.replace("Z", "+00:00")
            dt = datetime.fromisoformat(ts_clean)
            # Standardize to UTC with Z suffix
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            event["timestamp"] = dt.isoformat().replace("+00:00", "Z")
        except ValueError:
            # Leave as-is; validation will catch errors
            pass

    # --- Normalize keys for backward compatibility ---
    key_mappings = {
        "trader": "trader_id",
        "market": "market_id", 
        "type": "event_type",
        "product": "product_type",
        "optionType": "option_type",  # Handle camelCase
        "impliedVol": "implied_vol"
    }
    
    for old_key, new_key in key_mappings.items():
        if old_key in event and new_key not in event:
            event[new_key] = event.pop(old_key)

    # --- Normalize product_type ---
    if "product_type" in event:
        product = event["product_type"].lower()
        if product in ["perpetual", "future", "futures", "perp"]:
            event["product_type"] = "perp"
        elif product in ["options", "option"]:
            event["product_type"] = "option"
        elif product in ["spot", "cash"]:
            event["product_type"] = "spot"

    # --- Normalize side terminology ---
    # Convert position terms (long/short) to trading terms (buy/sell) for spot and options
    # Keep long/short for perps
    if "side" in event and event.get("product_type") in ["spot", "option"]:
        side = event["side"].lower()
        
        # Only normalize for open/close events, not for exercise/expire
        if event.get("event_type") in ["open", "close", "trade"]:
            if side == "long":
                event["side"] = "buy"
            elif side == "short":
                event["side"] = "sell"
            # Already buy/sell stays as-is

    # --- Normalize option-specific fields ---
    if event.get("product_type") == "option":
        # Normalize option_type
        if "option_type" in event:
            event["option_type"] = event["option_type"].lower()
        
        # Normalize expiry timestamp
        if "expiry" in event and event["expiry"]:
            expiry = event["expiry"]
            if isinstance(expiry, str):
                try:
                    expiry_clean = expiry.replace("Z", "+00:00")
                    dt = datetime.fromisoformat(expiry_clean)
                    # Standardize format
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    event["expiry"] = dt.isoformat().replace("+00:00", "Z")
                except ValueError:
                    # Leave as-is
                    pass

    # --- Ensure event_id exists ---
    if "event_id" not in event:
        # Create deterministic event ID
        raw_parts = [
            str(event.get('event_type', '')),
            str(event.get('timestamp', '')),
            str(event.get('trader_id', '')),
            str(event.get('market_id', '')),
            str(event.get('product_type', ''))
        ]
        raw = "|".join(raw_parts)
        event["event_id"] = hashlib.sha256(raw.encode()).hexdigest()

    # --- Normalize numeric fields ---
    numeric_fields = ["price", "size", "fee", "pnl", "strike", "delta", 
                     "gamma", "theta", "vega", "implied_vol", "underlying_price"]
    
    for field in numeric_fields:
        if field in event and event[field] is not None:
            try:
                event[field] = float(event[field])
            except (ValueError, TypeError):
                # Keep as-is if conversion fails
                pass

    return event


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\pipelines.py =====

# src/ingestion/pipelines.py
import json
import hashlib
from pathlib import Path
from src.ingestion.watermark import WatermarkStore
from src.ingestion.normalizer import normalize_event
from src.analytics.validate import validate_event, EventValidationError


class IngestionPipeline:
    def __init__(self, raw_path: str, output_path: str, checkpoint_path: str):
        self.raw_path = Path(raw_path)
        self.output_path = Path(output_path)
        self.watermark = WatermarkStore(checkpoint_path)

    def run(self) -> int:
        """
        Event-driven ingestion: normalize once, append forever.
        Supports both JSON array and JSONL formats.
        """
        if not self.raw_path.exists():
            raise FileNotFoundError(f"Raw data source not found: {self.raw_path}")

        # Load events based on file format
        if self.raw_path.suffix == '.json':
            # JSON array format (e.g., configs/mock_data.json)
            with self.raw_path.open("r", encoding="utf-8") as f:
                raw_events = json.load(f)
        elif self.raw_path.suffix == '.jsonl':
            # JSONL format (one JSON object per line)
            raw_events = []
            with self.raw_path.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:  # Skip empty lines
                        raw_events.append(json.loads(line))
        else:
            raise ValueError(f"Unsupported file format: {self.raw_path.suffix}")

        new_events = []
        errors = []

        for idx, raw in enumerate(raw_events, 1):
            try:
                # Generate event_id if missing
                if "event_id" not in raw:
                    seed = (
                        f"{raw.get('event_type')}|"
                        f"{raw.get('timestamp')}|"
                        f"{raw.get('trader_id')}|"
                        f"{raw.get('market_id')}|{idx}"
                    )
                    raw["event_id"] = hashlib.sha256(seed.encode()).hexdigest()

                # Skip if already processed
                if not self.watermark.is_new(raw["event_id"]):
                    continue

                # Normalize and validate
                normalized = normalize_event(raw)
                validate_event(normalized)

                new_events.append(normalized)
                self.watermark.mark(raw["event_id"])

            except EventValidationError as e:
                errors.append(f"Event {idx}: Validation failed - {e}")
            except Exception as e:
                errors.append(f"Event {idx}: Unexpected error - {e}")

        # Report errors
        if errors:
            print(f"âš ï¸  {len(errors)} events had issues:")
            for e in errors[:5]:
                print(f"   - {e}")
            if len(errors) > 5:
                print(f"   ... and {len(errors) - 5} more")

        # Write normalized events to output (JSONL format)
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        with self.output_path.open("a", encoding="utf-8") as f:
            for e in new_events:
                f.write(json.dumps(e) + "\n")

        print(f"âœ… Ingested {len(new_events)} valid events")
        return len(new_events)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\watermark.py =====

# src/ingestion/watermark.py
import json
from pathlib import Path
from typing import Set

class WatermarkStore:
    """
    Persistent watermark store to prevent reprocessing events.
    """

    def __init__(self, path: str):
        self.path = Path(path)
        self.seen: Set[str] = set()
        self._load()

    def _load(self):
        if self.path.exists():
            with open(self.path, "r") as f:
                self.seen = set(json.load(f))

    def _save(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.path, "w") as f:
            json.dump(list(self.seen), f)

    def is_new(self, event_id: str) -> bool:
        return event_id not in self.seen

    def mark(self, event_id: str):
        self.seen.add(event_id)
        self._save()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\analytics\test_ingestion.py =====

# Create a test script test_ingestion.py
import json

with open("data/normalized/events.jsonl", "r") as f:
    for i, line in enumerate(f, 1):
        line = line.strip()
        if line:
            try:
                data = json.loads(line)
                print(f"Line {i}: OK - {data.get('event_type', 'N/A')}")
            except json.JSONDecodeError as e:
                print(f"Line {i}: ERROR - {e}")
                print(f"  Content: {line[:50]}...")
        else:
            print(f"Line {i}: EMPTY LINE")


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\analytics\test_pnl_engine.py =====

import pandas as pd
from datetime import datetime, timezone

from src.analytics.pnl_engine import compute_realized_pnl

def test_simple_open_close_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100.0,
            "size": 1,
            "fee": 0.5,
        },
        {
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110.0,
            "size": 1,
            "fee": 0.5,
            "pnl": 9.0,  # truth reference
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 1
    assert positions.iloc[0]["realized_pnl"] == 9.0

def test_open_without_close_has_no_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T1",
            "market_id": "SOL-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 50,
            "size": 2,
            "fee": 0.2,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty

def test_pnl_only_on_close_events():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "trade",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T1",
            "market_id": "SOL/USDC",
            "product_type": "spot",
            "side": "buy",
            "price": 100,
            "size": 1,
            "fee": 0.1,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty
def test_pnl_engine_is_deterministic():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T2",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "short",
            "price": 200,
            "size": 1,
            "fee": 0.3,
        },
        {
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T2",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "short",
            "price": 180,
            "size": 1,
            "fee": 0.3,
            "pnl": 19.4,
        },
    ])

    p1, pnl1 = compute_realized_pnl(events)
    p2, pnl2 = compute_realized_pnl(events)

    pd.testing.assert_frame_equal(p1, p2)
    pd.testing.assert_frame_equal(pnl1, pnl2)

def test_close_without_open_is_rejected():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "close",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T3",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120,
            "size": 1,
            "fee": 0.4,
            "pnl": 0,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty
def test_partial_close_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100.0,
            "size": 10,
            "fee": 1.0,
        },
        {
            # partial close (50%)
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110.0,
            "size": 5,
            "fee": 0.5,
        },
        {
            # final close
            "event_id": "e3",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 3, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120.0,
            "size": 5,
            "fee": 0.5,
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 2

    realized = positions["realized_pnl"].sum()
    assert round(realized, 2) == round(
        ((110 - 100) * 5 + (120 - 100) * 5) - 2.0, 2
    )

def test_multiple_partial_closes_order_independent():
    base_events = [
        {
            "event_id": "o1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100,
            "size": 10,
            "fee": 1.0,
        },
        {
            "event_id": "c1",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110,
            "size": 4,
            "fee": 0.4,
        },
        {
            "event_id": "c2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 3, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120,
            "size": 6,
            "fee": 0.6,
        },
    ]

    df1 = pd.DataFrame(base_events)
    df2 = pd.DataFrame(reversed(base_events))

    p1, pnl1 = compute_realized_pnl(df1)
    p2, pnl2 = compute_realized_pnl(df2)

    pd.testing.assert_frame_equal(p1, p2)
    pd.testing.assert_frame_equal(pnl1, pnl2)
def test_liquidation_is_partial_close():
    events = pd.DataFrame([
        {
            "event_id": "o1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T9",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 200,
            "size": 10,
            "fee": 1.0,
        },
        {
            "event_id": "l1",
            "event_type": "liquidation",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T9",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 150,
            "size": 4,
            "fee": 0.5,
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 1
    assert positions.iloc[0]["close_reason"] == "liquidation"


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\analytics\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\main.py =====

def main():
    print("Hello from deriverse-data-puller!")


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\pyproject.toml =====

[project]
name = "deriverse-data-puller"
version = "0.1.0"
description = "Mock on-chain trading analytics system for Deriverse"
readme = "README.md"
requires-python = ">=3.10"

dependencies = [
    "matplotlib>=3.10.8",
    "pandas>=2.3.3",
    "plotly>=6.5.2",
    "pyyaml",
    "requests>=2.32.5",
    "streamlit",
]

