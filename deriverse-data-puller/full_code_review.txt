

===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\.vscode\settings.json =====

{
    "python.defaultInterpreterPath": "${workspaceFolder}/.venv/Scripts/python.exe",
    "python.analysis.extraPaths": ["./"],
    "python.autoComplete.extraPaths": [
        "${workspaceFolder}/.venv/Lib/site-packages"
    ]
}


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\configs\ingestion.yaml =====

# configs/ingestion.yaml

# Path to raw mock protocol events (JSON array)
raw_data_path: configs/mock_data.json

# Append-only normalized output (JSONL format)
normalized_output_path: data/normalized/events.jsonl

# Watermark / checkpoint store for incremental ingestion
checkpoint_path: data/checkpoints/watermark.json

# Allowed lateness for event-time processing (seconds)
allowed_lateness_seconds: 0

# Optional controls (future-safe)
markets: []
traders: []
max_events_per_run: null


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\configs\loader.py =====

# configs/loader.py

import yaml
from pathlib import Path
import logging  # âœ… ADD THIS

logger = logging.getLogger(__name__) 

def load_config(path: str) -> dict:
    with open(Path(path), "r") as f:
        return yaml.safe_load(f)


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\configs\mock_data.json =====

[
  {
    "event_type": "open",
    "timestamp": "2026-01-19T12:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 100,
    "size": 10,
    "fee_usd": 0.5,
    "event_id": "2ba942db20ceaf7d4fdeccbec1542acfeb3231d7cbb4fc17420f37407259bca9",
    "tx_hash": "2rE5PzT7J3m4FW8uoFUHQm6vK9PNvNWhuVi9iXy7Ngxp6gapQfmLRhK7bzFiscKrGPxiu5FEZh5TCduPJSscMavB",
    "position_id": "7KNXqvHu_SOL/USDC_1768825204766",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-19T14:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 110,
    "size": 10,
    "fee_usd": 0.5,
    "event_id": "0f230eb3fd05d9ec3d25c25ecc84f9f92e79a43227ec76ad58b4e64b65540c99",
    "tx_hash": "3Aqt8G2yb64ka2tLh3uTjm18Zic52LzAZyvtG1YGfHiNH9au9QgTjAGALkmwjLNzXTA78VfxPaSspb3eYnydfN5u",
    "position_id": "7KNXqvHu_SOL/USDC_1768825204766",
    "entry_price": 100,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-21T12:30:04.766082Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 2000,
    "size": 5,
    "fee_usd": 1.0,
    "event_id": "182d6691d5c35ed263b17c2787206b81fe089701bb3a7274a75a4593e1e8ba85",
    "tx_hash": "5ZsDxzrM6hcC9SMrB9SaPVh2mS7achoH6WHe25qsyzdezvzTk6uhNVoy7nTzUBRdZXcJu8sYaUqCRK9QAzPHwc63",
    "position_id": "5FxM2nQw_ETH/USDC_1768998604766",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-21T15:20:04.766082Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 1950,
    "size": 5,
    "fee_usd": 1.0,
    "event_id": "967a7c2db325680408778145f8e3ba2b553bd170e0e665a8ca97910302a382aa",
    "tx_hash": "2vbpoZ4XLELbgfUXohzkABEbub9rucGS8vnJBMTjCq1xxVwb6Ki1ecUEE14XC93zg8QyqC5Hia5xMT4HxHxTthbu",
    "position_id": "5FxM2nQw_ETH/USDC_1768998604766",
    "entry_price": 2000,
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-22T21:20:04.766082Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "AVAX/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 36.0,
    "size": 40,
    "fee_usd": 0.72,
    "event_id": "74117e4a63e38e11b7bda671b43229d5d20299a5c7d6b2aa48f4d11c9253a325",
    "tx_hash": "HHEjrTriRUAvJfyHKMsBhqPQPX3RYFve2GbQc2Fys4H388Ncy2YudTb35kTckiuSKrczR51zmqG4WoeTwZB5XY3",
    "position_id": "3HsJ7yVz_AVAX/USDC_1769116804766",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-23T02:20:04.766082Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "AVAX/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 40.2,
    "size": 40,
    "fee_usd": 0.8,
    "event_id": "ba8df23c7599e4625ac4bcc84b3c441603815c8594e1ac33901943d3dced01b7",
    "tx_hash": "21qrrL75eqywyHeZjdAUmqgfzan6hVLMtwVfaZpJbXp6qRR12H75FnuSH8XY8vfcpkjN5ACM7qBdvn4gpePjoteb",
    "position_id": "3HsJ7yVz_AVAX/USDC_1769116804766",
    "entry_price": 36.0,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-24T22:20:04.766082Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "BTC/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 48500,
    "size": 0.5,
    "fee_usd": 4.85,
    "event_id": "e8ba5ddef227222dc12a453851afb7011becc0ad684159bcedf96330858a67b7",
    "tx_hash": "3qS2gt4sdcrRAHrD4oSwfkc6LJr1Yg67nXogPUF3kaWMvHe9LgVPaoDSvqeB9VDAG4Z3ajCLWhYtXkvHFGk822xo",
    "position_id": "2PrM8xUz_BTC/USDC_1769293204766",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-25T01:20:04.766082Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "BTC/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 49125,
    "size": 0.5,
    "fee_usd": 4.91,
    "event_id": "2fbf22737d612935dc41259ec938c8665df87b93d6dc43903c9d5c796a7bf62a",
    "tx_hash": "5mo1eH3mMFyZX1rLaRpTrTnsGYpYzpCAMusRoXHLLSBS4ckp4JSh6XaG4ynkfRHEKfNTecXRSSY4HA7UpR3ziu9h",
    "position_id": "2PrM8xUz_BTC/USDC_1769293204766",
    "entry_price": 48500,
    "order_type": "limit"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-27T20:20:04.766082Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 108.0,
    "size": 15,
    "fee_usd": 1.08,
    "event_id": "9ee7bae5d29171e66d8c7431684995c1a92c07e4144e9c98b38114eb71291624",
    "tx_hash": "4YQAH7FGFUAyYHKx6kMKhLaVGT5FoTYHvd6C2TiaHLQYNreqqXsjTgN3u6iU624uNRuDXkrtz82G7QKQDVa13Urj",
    "position_id": "8QtN2xWy_SOL/USDC_1769545204766",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-28T02:20:04.766082Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 101.0,
    "size": 15,
    "fee_usd": 1.01,
    "event_id": "ac56511627d45b6d6e2c514788efd0d97158e493b2b8c39f7ba12797b1ad0c97",
    "tx_hash": "yLfMfCMUx6q5dWjevm4XDWGH72JtXt1PrtaRdzNjgDKguyjW4K2R6HcsAKU6L8Zn2dmoFnBtbGeNmbYbNUqLuRh",
    "position_id": "8QtN2xWy_SOL/USDC_1769545204766",
    "entry_price": 108.0,
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-01T06:20:04.766082Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 2050.0,
    "size": 3,
    "fee_usd": 1.54,
    "event_id": "046422b5171acf5a834dfa4f326e85dbb819b382424b929d1c48a56cb6306344",
    "tx_hash": "2ifTuNnyaKakp31WzU2DZDr78fn6GD2QgMperd4yBsvV35aXQDisPat4wvippX7qnPXErufkArzwzSdw3RS2ko1Z",
    "position_id": "5TpQ9yXz_ETH/USDC_1769926804766",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-01T21:20:04.766082Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 2125.0,
    "size": 3,
    "fee_usd": 1.59,
    "event_id": "d4767186fa8c25c349b56136154c98633b8fc1da3e5497a8014ea4957a5b5a17",
    "tx_hash": "5KTqhvB1PYvUxASonno4m3p25PZAiBxh5YjkTCJzxYitX9JBojZNhyukgiwioMBMFkXK9qk6RPza8hDL88HEbTNw",
    "position_id": "5TpQ9yXz_ETH/USDC_1769926804766",
    "entry_price": 2050.0,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-03T23:20:04.766082Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "LINK/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 14.2,
    "size": 100,
    "fee_usd": 1.42,
    "event_id": "79ecaeab1a24b57054e5743af10676cb17a7afd521b69374b6cb0f8577466f86",
    "tx_hash": "F6HSR9uDuDokErw3jpAZyF3DU3vW4rtEjEiuYiDDLwia5Qe9URGH1aTaXyB8aRKgCCeY8P74VUMe3wtRQM8k3nX",
    "position_id": "4WqP8zYx_LINK/USDC_1770160804766",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-04T03:20:04.766082Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "LINK/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 15.8,
    "size": 100,
    "fee_usd": 1.58,
    "event_id": "492cfbbd3b4fd40f3ac919c1176db3c7b18b7658a1d6b7646ce105dc851d2f41",
    "tx_hash": "54yAjQEgoUgWV4WNeTSe1mENd3JzXsbSpKrApTyecqGkqxQnKmiRrxUSsphy692a863nSufdXqgAep7R8V1a43aw",
    "position_id": "4WqP8zYx_LINK/USDC_1770160804766",
    "entry_price": 14.2,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-06T01:20:04.766082Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "MATIC/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 0.92,
    "size": 1000,
    "fee_usd": 0.92,
    "event_id": "236e899800dc3a62af07627826636abb76441a61ddc6fc4063cfe19b4a029ff3",
    "tx_hash": "4LmHvDwRyzketoopDV42sNBebqwpdqJBjRwQrydP3pF7JRHXeWryA9C4CEPVw8cMEA4AuEprBHnbc4FhfYGALwWo",
    "position_id": "4MqL8vYx_MATIC/USDC_1770340804766",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-06T08:20:04.766082Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "MATIC/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 0.81,
    "size": 1000,
    "fee_usd": 0.81,
    "event_id": "35c0f3aa7b211d96c8424dcfeb13813e62d5a0f06d9fcee7a2182b3a63353eb3",
    "tx_hash": "5Yt9EtSx5VhKRrZThbW2KTBeiVUiEBSiByph6vDVjiZphX8dGEt3JcqMpPqg2uehzPs7U36nPhaiXdGujFhWunnB",
    "position_id": "4MqL8vYx_MATIC/USDC_1770340804766",
    "entry_price": 0.92,
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-08T19:20:04.766082Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "DOT/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 7.3,
    "size": 200,
    "fee_usd": 1.46,
    "event_id": "63d1c1ba0dd276c15e68e0154934fc0a5fbb0e2649f8972d06120021ee1c33f1",
    "tx_hash": "Gv9JtiMRQh3nXJMmMMam4VKy2CSj9BrzvFsqyntHci66rK3GhkBW66oYSnGmp6U7N1HSYynQfJh9B2w8UoZ9fwm",
    "position_id": "9DpT3vHx_DOT/USDC_1770578404766",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-09T04:20:04.766082Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "DOT/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 8.1,
    "size": 200,
    "fee_usd": 1.62,
    "event_id": "cb0623ff9cc11e212cb5e98049ac483c2730b2b2ec8ab8ec8956a6b0167317f5",
    "tx_hash": "3LgjpTPCCUC8qTiic7dRi1JnnpSHZwzVo4ERhpCHwoHvfVrFA35UiFqrgBRufe67dpswAyZrdgHpzcAs2UWJhdw5",
    "position_id": "9DpT3vHx_DOT/USDC_1770578404766",
    "entry_price": 7.3,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-12T21:20:04.766082Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 118.0,
    "size": 25,
    "fee_usd": 2.95,
    "event_id": "6621e3130d1c549b7c8ac7b5d9a8307f11e6e41c21b9f22ae2c1b2c7d0ee5ebf",
    "tx_hash": "ySxsTJkcmEXEAZjRf6eRDkWJtLojCZnboe2UaPLoqY6mbFE3xE5hrCHqNRtJcL1W3zZZBwprwvyL1R5cHhgZasR",
    "position_id": "5FxM2nQw_SOL/USDC_1770931204766",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-14T02:20:04.766082Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 127.0,
    "size": 25,
    "fee_usd": 3.18,
    "event_id": "ead10ca45f1e0ec8985b3e66aa29f232de1a4d7bad716b2ed07bb9efe59ef530",
    "tx_hash": "2vZVucmJqvbeTNC187RNAJBBSejBX2hkozucJDkg3iztkRM1cxF7rLJu76Ywb5k6PnKgDeuGKENrVGhGRw1AYtgP",
    "position_id": "5FxM2nQw_SOL/USDC_1770931204766",
    "entry_price": 118.0,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-23T12:40:04.766082Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 100,
    "size": 10,
    "fee_usd": 0.5,
    "event_id": "10a788273283540bad78903a2bf486f4f45cd00cf0a3392b5e0a36ecb6a1be64",
    "tx_hash": "5QqumnBKaxPCUx9SRmhihQ9zHHawXLfGRwQd79nrwJcJAQ4U4kN8MTshcp8GwjVMwEJRkaTyKY3nP7dFD1GRcoUK",
    "position_id": "9DpT3vHx_SOL-PERP_1769172004766",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-23T16:20:04.766082Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 120,
    "size": 10,
    "fee_usd": 0.5,
    "event_id": "a523f102c8a8424fb74e231c3fca7f36c4e4c372c93759a7230b5c2a28c4f5ec",
    "tx_hash": "ZMgaYExDYpoHjtV5iwrWGVz9XQkDGNqVofoUfkCKXD2xt4KJC4Mmf9tc7JNnMenEV4hjUyL2yVKzZgXwyuKXSUF",
    "position_id": "9DpT3vHx_SOL-PERP_1769172004766",
    "entry_price": 100,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-25T12:50:04.766082Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 50000,
    "size": 1,
    "fee_usd": 5.0,
    "event_id": "0d351665c8a05b151bd8130b8ee5c636183145ef3291462f6d93f6de3d5ed617",
    "tx_hash": "2BrYNe2h1mpLtDTsD2CX9zShkGupSMAPLna7b7HJFEN2shSuBfYs1rem4teq55D7cBPVi85SS7JkeSr9irojKRaw",
    "position_id": "4MqL8vYx_BTC-PERP_1769345404766",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-25T17:20:04.766082Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 48000,
    "size": 1,
    "fee_usd": 5.0,
    "event_id": "61eae1023011ff0f7dfcf9af39ccf95da50189d094dfee8c8752ed9bbf6a2ead",
    "tx_hash": "3YmboDgjyJD98jzgVtjeDsUBZwniFQCBQfYePp8MAupv4PKHKfrMeMZc5kqd2YbdKyMX6FHXN3MW6JAAKoSf9VUs",
    "position_id": "4MqL8vYx_BTC-PERP_1769345404766",
    "entry_price": 50000,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-26T13:00:04.766082Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2100,
    "size": 5,
    "fee_usd": 2.0,
    "event_id": "91f56110011a36d6cb662845f07dcea3ab1d83dbf3884a708bcc2a94a42e8d81",
    "tx_hash": "5Kccs4W7FUxZqZwoqGBKESP5MkfpCRKZwsnwT6bANs5L5EjwTMP9ddepWCwBd6nR3r6gw3U1c6YWiXFaN3uNxJPy",
    "position_id": "6NrK9wZx_ETH-PERP_1769432404766",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-26T18:20:04.766082Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2050,
    "size": 5,
    "fee_usd": 2.0,
    "event_id": "48235955047daaf5c0f5f1908a1cfb696f8cf305c84a5f62dcdf530c5ba4d965",
    "tx_hash": "nkC1rqg1tWnTmR4SAap6LPZUExXzzHc5Rz56gBuAc6AXLizKmNJEawCjt1PbxoGWK3Gbc6PTgUcX5gYpv74tAzj",
    "position_id": "6NrK9wZx_ETH-PERP_1769432404766",
    "entry_price": 2100,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-28T13:20:04.766082Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 105,
    "size": 50,
    "fee_usd": 5.0,
    "event_id": "a942ae011061b45f440774cff4497e4642b88fad2524f569942247be9ca4f874",
    "tx_hash": "3XJsXbZ7sPqVEkp3GhQp8dw3f9FnEry4jh8mqKfbfd1cPdc15SkKE15Qk7sm7nmNUrqFZcGawUMx99P8mj1RUHoV",
    "position_id": "4MqL8vYx_SOL-PERP_1769606404766",
    "order_type": "market"
  },
  {
    "event_type": "liquidation",
    "timestamp": "2026-01-28T14:20:04.766082Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 88,
    "size": 50,
    "fee_usd": 25.0,
    "event_id": "747bd447641d307086b3c82380d017ffa3b7fbcd6eb8dc563487b752ebd63255",
    "tx_hash": "4L9d1jsX61ZmK4gzAruGmihnzxLr5D6G1jAPwdX5ojvpG3QFx8au45uPEEs176MgZSStCK3KrprSVTGytqzD943",
    "position_id": "4MqL8vYx_SOL-PERP_1769606404766",
    "entry_price": 105,
    "order_type": "liquidation"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-30T15:20:04.766082Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2200,
    "size": 10,
    "fee_usd": 3.0,
    "event_id": "9748227364a868fb5df21c695679a887d8f0772dabf6b04280368d0f796b55d7",
    "tx_hash": "4rP2GaCi3fxEG6xheasdphZ311DV8muosuPkN4n9g2mYvM1BUeQffvVLZuaerC4FAwwgGEz5M7Bb4mKmdsseECLF",
    "position_id": "5FxM2nQw_ETH-PERP_1769786404766",
    "order_type": "market"
  },
  {
    "event_type": "liquidation",
    "timestamp": "2026-01-30T20:20:04.766082Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2050,
    "size": 10,
    "fee_usd": 20.0,
    "event_id": "44ea425ada4ab7a11b2e71023f252e6fa4abcb67171e9ce1c8eb3ece5457d88c",
    "tx_hash": "27p1mwC39zPUDsCKyUBPAJ4QScA8hLVWAtX6rDPoXVr8qbqE65AEyv6nNDT1ATDert8yC4C7CFQHkywCPLtKDSXH",
    "position_id": "5FxM2nQw_ETH-PERP_1769786404766",
    "entry_price": 2200,
    "order_type": "liquidation"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-02T14:20:04.766082Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 49000,
    "size": 2,
    "fee_usd": 8.0,
    "event_id": "3048170fdad489cbaf9669852565cd2b993d215bdbbae9d022be28dff86c23b7",
    "tx_hash": "2sDKMUQJSgpXAP2BpbSWuJD7GUr5smpbdyDkzE5qd4VhQ8aeH7npy3GFfWK9BAafaypwc9QxUPnWeSUpML7fBzj9",
    "position_id": "6NrK9wZx_BTC-PERP_1770042004766",
    "order_type": "limit"
  },
  {
    "event_type": "liquidation",
    "timestamp": "2026-02-03T00:20:04.766082Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 50250,
    "size": 2,
    "fee_usd": 30.0,
    "event_id": "9052cfbed13654a281a48c5e778804418beb2d8e0f19d33929152f179d199156",
    "tx_hash": "3j9fikMs6nSWzXVP1rTpUbQPkZDDmDww4jXzCadD5MEJ1Y7tLnw6YzShPRbfWay9yx4ozwxdpUcFyyhntyKLNrXR",
    "position_id": "6NrK9wZx_BTC-PERP_1770042004766",
    "entry_price": 49000,
    "order_type": "liquidation"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-05T17:20:04.766082Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "AVAX-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 38.5,
    "size": 100,
    "fee_usd": 4.0,
    "event_id": "5edb2ee81832251ae84d38d03cbc8991856d18e79f9aec540596e85db72e8842",
    "tx_hash": "565B7Rm7SWS6Ly6zAJUhjnKXrYzwNDpiahGtJYnizvz5FVZudPtAMTC6wJ5VwiXgEP7i5jbL78WpUTdDYjcReL1m",
    "position_id": "9DpT3vHx_AVAX-PERP_1770312004766",
    "order_type": "market"
  },
  {
    "event_type": "liquidation",
    "timestamp": "2026-02-06T02:20:04.766082Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "AVAX-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 35.2,
    "size": 100,
    "fee_usd": 12.0,
    "event_id": "55d8f434f2cd98a0b69c2ddb2ea21a5522982076d6e1aa0c24c019f1bceaa426",
    "tx_hash": "4sLWDeoUEcEVysGUpMErxJeiDTdJtsPd9cHWBKdNVa83rDCsPR3SuJRAwKWS9rwpuRZLpksCbjiQhAjrEEuiYNBZ",
    "position_id": "9DpT3vHx_AVAX-PERP_1770312004766",
    "entry_price": 38.5,
    "order_type": "liquidation"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-07T16:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 115,
    "size": 30,
    "fee_usd": 3.5,
    "event_id": "9ae79c558a050e1dea6c381a8ee2d43e36ae5e98835d12d8bf6f1bb1dafd763e",
    "tx_hash": "MNdxTqmbBY4ogoayTCckQULzRsLxEFXEhc45Qbfc21zgH1qpAZzrH83uRvk7BR5At2NJqqA7qQqpY7ac135rqaX",
    "position_id": "7KNXqvHu_SOL-PERP_1770481204766",
    "order_type": "stop"
  },
  {
    "event_type": "liquidation",
    "timestamp": "2026-02-07T21:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 135,
    "size": 30,
    "fee_usd": 18.0,
    "event_id": "4f19116b825e4b8317655764408d71096a4c718d12ac85a155957116b9535770",
    "tx_hash": "qU4E2BkwTp1wWgaCd1sUh5t6qmFQQyTBCCZ9z6fYghYsRUikwKM66ASvm1XyGm4CQMBXK66b9cqqUfe9U1H9zXq",
    "position_id": "7KNXqvHu_SOL-PERP_1770481204766",
    "entry_price": 115,
    "order_type": "liquidation"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-08T13:20:04.766082Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL-CALL-120-JAN15",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "buy",
    "price": 5.0,
    "size": 10,
    "fee_usd": 0.5,
    "delta": 0.65,
    "implied_vol": 0.45,
    "event_id": "0abb3a88355c54dcc7f2a5aa13343402dd324ce8d4203336d89cecea821c6a8a",
    "tx_hash": "5GQaBH35qzUgk3X1vzMhbjZaBH4SybQnP5c4AU9mqy13scgu4MdMny4xbvviV5NHqpFYpVzhtEmZeES7yhSxXevB",
    "position_id": "8QtN2xWy_SOL-CALL-120-JAN15_1770556804766",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-09T12:20:04.766082Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL-CALL-120-JAN15",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "sell",
    "price": 8.0,
    "size": 10,
    "fee_usd": 0.5,
    "delta": 0.85,
    "implied_vol": 0.5,
    "event_id": "a0364a2131ce002c6da26aabf417b3c82ad3f4ca7732657ffc620f7dcb24dc9f",
    "tx_hash": "5KXycWH8DWehaopJis6suJsBy3Rgm7Jd9TBPWqyDgbKr4JqqNt9VDFNKWHK7Tybakz9JLcVuiZAz1BZeyp2TAVxs",
    "position_id": "8QtN2xWy_SOL-CALL-120-JAN15_1770556804766",
    "entry_price": 5.0,
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-09T13:50:04.766082Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "SOL-PUT-90-JAN15",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "sell",
    "price": 4.0,
    "size": 15,
    "fee_usd": 0.7,
    "delta": -0.25,
    "implied_vol": 0.4,
    "event_id": "274173e8401d205ed119bfba1ae38e4fa7fce3eceadc4f25dbcec3af382a0cce",
    "tx_hash": "4vGXBdqSwbAtCvAadBP5f4aiwj7suV1g6GqgvD4vAC2TnMAXmR3ymWdX4ebQ57KtP1XotHzW4SLswK3LMJ445KaF",
    "position_id": "3HsJ7yVz_SOL-PUT-90-JAN15_1770645004766",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T00:20:04.766082Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "SOL-PUT-90-JAN15",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "buy",
    "price": 1.5,
    "size": 15,
    "fee_usd": 0.7,
    "delta": -0.1,
    "implied_vol": 0.3,
    "event_id": "495d8acd7d16049ff0a4fedbafbc200b89d1dd7b22f9907a12b576145d00a1b7",
    "tx_hash": "rmVjpdF8YGKdq7C3G8s1gANQ9mGj5g3BoitCvzZ4R6qD6EmfoLvMYdYjKM3VzZH7WHP1fJjg2Hn4NZ2C79fQCBy",
    "position_id": "3HsJ7yVz_SOL-PUT-90-JAN15_1770645004766",
    "entry_price": 4.0,
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-10T14:20:04.766082Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "ETH-PUT-1900-JAN15",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "buy",
    "price": 45.0,
    "size": 5,
    "fee_usd": 1.0,
    "delta": -0.35,
    "implied_vol": 0.55,
    "event_id": "8258fe211ac2cd25aff70327d2d09d7ea9cc17dd902808a84dda8444b474dd29",
    "tx_hash": "2w3jHEkxEKxP698kXpXu1eQiJGNc8TqZ2tpRtg997G5AUXR2JFXjWEU1HGyyuzar5XWiQoQghECuvQpTHFURrTHq",
    "position_id": "2PrM8xUz_ETH-PUT-1900-JAN15_1770733204766",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-12T12:20:04.766082Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "ETH-PUT-1900-JAN15",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "sell",
    "price": 20.0,
    "size": 5,
    "fee_usd": 1.0,
    "delta": -0.15,
    "implied_vol": 0.4,
    "event_id": "d6d6bc70c0944ef547a97354d63a0184c58ee87815d3ee911689f37925708682",
    "tx_hash": "2NBEYMy7s9BFNRBKeFN2Ma65FxKqaasLk1cCEgKGv5fP6JFssmbzSE9vdWVEMSnntdzjd26WALrQFej5rcgCncWj",
    "position_id": "2PrM8xUz_ETH-PUT-1900-JAN15_1770733204766",
    "entry_price": 45.0,
    "order_type": "limit"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-12T15:20:04.766082Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "BTC-CALL-50000-JAN15",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "buy",
    "price": 2000.0,
    "size": 1,
    "fee_usd": 10.0,
    "event_id": "32c392423e45215665efdddd7cc724efd5a81f726940e7557d7e3d1edca326f2",
    "tx_hash": "5x4jr5q5QXq4Q7HeHExLUSVYj2J3VUBqJoF7FfcudFF6qUYx45qnDDvgdsaKpu9WmWE96Baj3NHEhSDN7Zb3Aa7y",
    "position_id": "5TpQ9yXz_BTC-CALL-50000-JAN15_1770909604766",
    "order_type": "market"
  },
  {
    "event_type": "exercise",
    "timestamp": "2026-02-17T12:20:04.766082Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "BTC-CALL-50000-JAN15",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "exercise",
    "size": 1,
    "fee_usd": 10.0,
    "underlying_price": 55000,
    "event_id": "fa702b65ef49257faaaf0058f7ed3e99d4cbcf78cfa29ccded425df3ab3168ac",
    "tx_hash": "5MjpQtzEZYaFvZZhcpt3WuQTtbChRB7iYyPanJxqMQh1sFVL4b35x7QieFdAG1oGGPPYxBFLBMipLX5G2iEvrZp3",
    "position_id": "5TpQ9yXz_BTC-CALL-50000-JAN15_1770909604766",
    "entry_price": 2000.0
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-13T16:20:04.766082Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "SOL-PUT-80-JAN15",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "buy",
    "price": 3.0,
    "size": 20,
    "fee_usd": 0.2,
    "event_id": "62a5e39524bc5c23a2d5e0e767d9ee679dd3b9889721c7b1e0f94f51e969c48a",
    "tx_hash": "2z7ACx8n5XN8MCiZfWAoaQd3D71jmhvt6TVAgrQj7A2dfR3tXzW6qNHJksmCiuQVur3WhhpmKb546a2FTFAoSbrw",
    "position_id": "4WqP8zYx_SOL-PUT-80-JAN15_1770999604766",
    "order_type": "market"
  },
  {
    "event_type": "expire",
    "timestamp": "2026-02-18T00:20:04.766082Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "SOL-PUT-80-JAN15",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "expire",
    "price": 0.0,
    "size": 20,
    "fee_usd": 0.0,
    "underlying_price": 95,
    "event_id": "74c59ff68227e9c6a7a1ad68ed9c1f257b7870f78676ea72f489cd5e9143d095",
    "tx_hash": "5r2Hujpyepu2mjW2fPbcCUtCeVmXodi3imtjn2GPPnww4Bz7nz3bQhyDLoUhK4ryDEMKiHCzQmt9jrBThbKcKn55",
    "position_id": "4WqP8zYx_SOL-PUT-80-JAN15_1770999604766",
    "entry_price": 3.0
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-14T17:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-JAN15",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "buy",
    "price": 8.0,
    "size": 20,
    "fee_usd": 1.0,
    "event_id": "9b00885e26d8a54a245e5de0ae98c10cf4150ec0261c8b0a936fd6c82be972e2",
    "tx_hash": "5VEAevvf3ZHtfagHGNjXKS19ugcuSs4Wbin8FAVmumB8N98No7WKcJZgjVSaABUFgEwPd6ZVperfreXg5B6FnUjh",
    "position_id": "7KNXqvHu_SOL-CALL-110-JAN15_1771089604766",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-15T14:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-JAN15",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "sell",
    "price": 12.0,
    "size": 10,
    "fee_usd": 0.5,
    "event_id": "16488d793cae686857e7c4e522e8a35871ecf4f6a92671d8f163122376c53d91",
    "tx_hash": "4DSaHUtm6UaxDrHxQcpi9eiRDJPXgFYxzUd2y4HdRKSZuEFY6mnxzbCSgKUr61XyUVrpaax2L6XrepzLd8dmr5kB",
    "position_id": "7KNXqvHu_SOL-CALL-110-JAN15_1771089604766",
    "entry_price": 8.0,
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-16T14:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-JAN15",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-02-26T12:20:04.766082Z",
    "side": "sell",
    "price": 15.0,
    "size": 10,
    "fee_usd": 0.5,
    "event_id": "35bfe8b64e947ae5c9f1b93c96659feaa64aa0c34ec286b3fd90d6cfcf46f909",
    "tx_hash": "26Bk4gdWrKNYHMTu48EN718xH5PS5bAh5x7Cn48VTrGVn2dRY3PtcFkeC5M8H9GfMfq6o6oWZBnjJzGJZPTtnRCj",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-16T05:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "BTC/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 51000,
    "size": 0.5,
    "fee_usd": 5.0,
    "event_id": "2b09da4098e33533c078f224aa49a4209b406c76b900985cddc5a39b198322f6",
    "tx_hash": "4qQavL6AssQo3fKUYzgGd2KNgZLDv1jYc9ozXCwVxXWinbYdqUXGyLDMxH1Kk765XZZbmBQesxxHvewLVagsV6mu",
    "position_id": "7KNXqvHu_BTC/USDC_1771219204766",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-17T04:20:04.766082Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "AVAX-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 35.5,
    "size": 100,
    "fee_usd": 1.5,
    "event_id": "5d19a5c42bdb89510fbf04db71b299661c6499a7c0bf1bd04cc78b86e9206028",
    "tx_hash": "32C1DgjDWVCj47RGaKfrUn2F2aUZJjr2RVXWx21U5ae4oFuCcVv72aYoQgk8ucMjnDrc6U2C45GSMYsg8a9zyEco",
    "position_id": "5FxM2nQw_AVAX-PERP_1771302004766",
    "order_type": "limit"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-18T00:20:04.766082Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "ETH-CALL-2200-FEB13",
    "product_type": "option",
    "option_type": "call",
    "strike": 2200,
    "expiry": "2026-03-05T12:20:04.766082Z",
    "side": "buy",
    "price": 85.0,
    "size": 3,
    "fee_usd": 0.5,
    "event_id": "70f02f6bb1f2d8784f7695139fb604ec89c13fec3a10113224b8591d6578b33c",
    "tx_hash": "2t4fvS63z1PaP9VbPZEkDdDRMw8AYTVwwrx35rjSgenfJYXELVRVu7pbUyHtHLVqb7yDJEhbFJQ81WsVtd3qRRU3",
    "position_id": "9DpT3vHx_ETH-CALL-2200-FEB13_1771374004766",
    "order_type": "limit"
  },
  {
    "event_type": "open",
    "timestamp": "2026-01-24T13:20:04.766082Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 105,
    "size": 5,
    "fee_usd": 0.3,
    "event_id": "5360ea9c4a0586f8a7e796cdeb185e622ad0368abf6e17b9b6866ef7e1ba9841",
    "tx_hash": "125SSbwcisZdaT6iwV7uBSKm3wt3CS8tJ9ard3hsg3RRPqKDZWpqFP7rNfbngWAnpZELFDRsYBizXDLUquDLWKrf",
    "position_id": "7KNXqvHu_SOL/USDC_1769260804766",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-01-29T22:20:04.766082Z",
    "trader_id": "GhostWallet1111111111111111111111111111",
    "market_id": "GHOST-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 999,
    "size": 1,
    "fee_usd": 0.1,
    "event_id": "27d4b73de17a6479fbce9cb41304fe6f926ba67a39dc7b25346153fa7bbd3587",
    "tx_hash": "4BBfjM9Pmrrm7a8vJnKNkCVqM4wxcX87voP8DJEU7rJ4B5sc1D5H3EfJ4AL4nx7YByib96hLUn3DHGFWTnHMLFeo",
    "order_type": "stop"
  },
  {
    "event_type": "trade",
    "timestamp": "2026-01-22T12:35:04.766082Z",
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 101,
    "size": 100,
    "fee_usd": 1.0,
    "event_id": "cbd7160cb889090c12677a75fdbce458af92e8580b4a63d8bc587b9821269b1d",
    "tx_hash": "5zbYEELBA2sq6SKv3fdCCNQmCNJK7d1eAAoMdv7MCGcMMP6qjBJiRU2qmmaKSvQuk9ggfxnoEyEctSAf2Q8XTvm5"
  },
  {
    "event_type": "trade",
    "timestamp": "2026-01-27T13:05:04.766082Z",
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "sell",
    "price": 2105,
    "size": 50,
    "fee_usd": 5.0,
    "event_id": "9b5e01cf2a3eb0e6cc8a596a6312ad7f6bd8f8efd2ed39bd83faa20b62ffce0c",
    "tx_hash": "3QPMnkva4p6BJrVhsWQimF1r7SesC7d2JH6Dsib5pszj1DbGDXDjkpMiAxkH3fNcDucetZs4Jw65GoofcpqFzpZV"
  }
]


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\configs\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\dashboards\.streamlit\config.toml =====

[theme]
base = "dark"
backgroundColor = "#0f172a"
secondaryBackgroundColor = "#1e293b"
textColor = "#f1f5f9"
primaryColor = "#6366f1"


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\dashboards\app.py =====

# dashboards/app.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
from datetime import datetime, timedelta
import requests
from plotly.subplots import make_subplots
import json
import os
import time
from dotenv import load_dotenv

load_dotenv()

# ============================================================================
# CONFIGURATION
# ============================================================================

DATA_DIR = Path("data/analytics_output")
ADMIN_PASSWORD = os.getenv("ADMIN_PASSWORD", "ADMIN_PASSWORD")

st.set_page_config(
    page_title="Deriverse Trading Analytics",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# URL PARAMETER HANDLER - For secret admin activation
# ============================================================================

def check_url_for_admin():
    """Check if URL contains admin activation parameter."""
    try:
        query_params = st.query_params
        if "admin" in query_params and query_params["admin"] == "1":
            return True
    except:
        pass
    return False

# ============================================================================
# MINIMAL CSS - Only essential styling for now
# ============================================================================

st.markdown("""
<style>
    /* Hide Streamlit's default header only */
    header[data-testid="stHeader"] { 
        display: none !important; 
    }
    
    /* KPI cards styling */
    .metric-major {
        background: linear-gradient(135deg, #1e293b 0%, #334155 100%);
        padding: 16px 20px;
        border-radius: 12px;
        border: 1px solid rgba(99, 102, 241, 0.4);
        text-align: center;
        box-shadow: 0 4px 12px rgba(0,0,0,0.2);
        margin-bottom: 16px;
    }

    .metric-major-label {
        font-size: 0.85rem;
        color: #94a3b8;
        font-weight: 700;
        text-transform: uppercase;
        letter-spacing: 0.06em;
        margin-bottom: 4px;
    }

    .metric-major-value {
        font-size: 1.8rem;
        font-weight: 700;
        color: #f1f5f9;
        line-height: 1.2;
        font-family: 'IBM Plex Mono', monospace;
    }
    
    /* Profile badge */
    .profile-badge {
        background: linear-gradient(135deg, #10b981, #059669);
        color: white;
        padding: 4px 12px;
        border-radius: 20px;
        font-weight: 600;
        font-size: 0.85rem;
        display: inline-block;
        margin: 8px 0;
    }
    
    /* Transaction table */
    .tx-table { 
        width: 100%; 
        border-collapse: collapse; 
        font-size: 0.85rem; 
    }
    
    .tx-table th { 
        background: #1e293b; 
        color: #94a3b8; 
        padding: 10px 12px;
        text-align: left; 
        border-bottom: 2px solid #334155; 
    }
    
    .tx-table td { 
        padding: 8px 12px; 
        border-bottom: 1px solid rgba(51,65,85,0.4); 
        color: #e2e8f0; 
    }
    
    .tx-table tr:hover td { 
        background: rgba(99,102,241,0.1); 
    }

    /* Verify links */
    .verify-link { 
        color: #10b981 !important; 
        text-decoration: none; 
        font-weight: 600; 
    }
    
    .verify-link:hover { 
        text-decoration: underline; 
        color: #34d399 !important; 
    }
    
    /* Debug container - only visible to admins */
    .debug-info {
        background: #1e293b;
        border-left: 4px solid #f59e0b;
        padding: 8px 16px;
        border-radius: 4px;
        margin: 10px 0;
        font-size: 0.85rem;
        color: #e2e8f0;
    }
    
    /* ===== LARGER NAVIGATION TABS ===== */
    div[data-baseweb="tab-list"] button {
        font-size: 1.1rem !important;
        font-weight: 600 !important;
        padding: 12px 20px !important;
    }
    
    div[data-baseweb="tab-list"] {
        gap: 8px !important;
    }
    
    /* Active tab styling */
    div[data-baseweb="tab"][aria-selected="true"] {
        background: linear-gradient(135deg, #6366f1, #8b5cf6) !important;
        color: white !important;
        border-radius: 8px 8px 0 0 !important;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================================
# ADAPTIVE VISUALIZATION FRAMEWORK
# ============================================================================

def get_data_density(df):
    """Classify data density for adaptive visualization."""
    count = len(df)
    if count == 0:
        return "empty"
    elif count == 1:
        return "single"
    elif count < 5:
        return "sparse"
    elif count < 15:
        return "moderate"
    else:
        return "dense"

def context_note(msg):
    """Display a contextual note for adaptive views."""
    st.info(f"â„¹ï¸ {msg}")

def should_show_chart(df, min_points=5, min_variance=0.1):
    """Determine if a chart is meaningful based on data."""
    if len(df) < min_points:
        return False
    if df['realized_pnl'].std() < min_variance:
        return False
    return True

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def mask_trader_id(trader_id):
    """Format trader wallet address for privacy."""
    if pd.isna(trader_id):
        return "Unknown"
    s = str(trader_id)
    return f"{s[:4]}..{s[-4:]}" if len(s) > 8 else s

def simplify_symbol(market_id):
    """Extract base symbol from market identifier."""
    if pd.isna(market_id):
        return market_id
    s = str(market_id)
    return s.split('/')[0].split('-')[0]

def get_top_traders(positions_df, n=5, by='profit'):
    """Get top N traders by specified criteria."""
    if positions_df.empty:
        return []
    
    trader_stats = positions_df.groupby('trader_id')['realized_pnl'].agg(['sum', 'count'])
    
    if by == 'profit':
        return trader_stats.nlargest(n, 'sum').index.tolist()
    elif by == 'loss':
        return trader_stats.nsmallest(n, 'sum').index.tolist()
    return trader_stats.nlargest(n, 'count').index.tolist()

def load_trader_notes(trader_id):
    """Load trade notes from JSON file."""
    notes_dir = Path("data/trader_notes")
    notes_dir.mkdir(parents=True, exist_ok=True)
    notes_file = notes_dir / f"{trader_id}.json"
    
    if notes_file.exists():
        with open(notes_file, 'r') as f:
            return json.load(f)
    return {}

def save_trader_notes(trader_id, notes):
    """Save trade notes to JSON file."""
    notes_dir = Path("data/trader_notes")
    notes_dir.mkdir(parents=True, exist_ok=True)
    with open(notes_dir / f"{trader_id}.json", 'w') as f:
        json.dump(notes, f, indent=2)

def calculate_volume_usd(df):
    """Calculate USD volume from price and size."""
    df = df.copy()
    df['volume_usd'] = df['exit_price'] * df['size']
    return df

# Chart styling constants
CHART_BG = dict(
    template='plotly_dark',
    plot_bgcolor='rgba(15,23,42,0.9)',
    paper_bgcolor='rgba(15,23,42,0.9)'
)

# ============================================================================
# TRADER PERFORMANCE SUMMARY
# ============================================================================

def create_trader_summary_table(equity_df, positions_df):
    """Trader summary table with actual equity sparklines."""
    
    st.markdown("### ðŸ“‹ Trader Performance Summary")
    
    if equity_df.empty or positions_df.empty:
        st.info("No performance data available")
        return
    
    traders = []
    for trader in positions_df['trader_id'].unique():
        te = equity_df[equity_df['trader_id'] == trader].sort_values('timestamp')
        tp = positions_df[positions_df['trader_id'] == trader]
        
        if te.empty or tp.empty:
            continue
        
        total_pnl = tp['realized_pnl'].sum()
        win_rate = (tp['realized_pnl'] > 0).mean() * 100 if len(tp) > 0 else 0
        max_dd = te['drawdown'].min()
        
        timestamps = te['timestamp'].values
        equity_values = te['cumulative_pnl'].values
        
        if len(equity_values) > 1:
            min_val = equity_values.min()
            max_val = equity_values.max()
            norm_curve = (equity_values - min_val) / (max_val - min_val) if max_val > min_val else np.ones_like(equity_values) * 0.5
        else:
            norm_curve = np.array([0.5])
            timestamps = [0]
        
        traders.append({
            'trader_masked': mask_trader_id(trader),
            'pnl': total_pnl,
            'win_rate': win_rate,
            'max_dd': abs(max_dd),
            'trades': len(tp),
            'equity_curve': norm_curve,
            'timestamps': timestamps,
            'raw_equity': equity_values,
            'trader_id': trader
        })
    
    traders.sort(key=lambda x: x['pnl'], reverse=True)
    
    if not traders:
        st.info("No trader data available")
        return
    
    cols = st.columns([1.2, 2.0, 0.8, 0.8, 0.8, 0.8])
    headers = ["**Trader**", "**Equity Curve**", "**PnL**", "**Win Rate**", "**Max DD**", "**Trades**"]
    for col, header in zip(cols, headers):
        col.markdown(header)
    
    st.divider()
    
    for i, t in enumerate(traders):
        cols = st.columns([1.2, 2.0, 0.8, 0.8, 0.8, 0.8])
        
        cols[0].markdown(f"`{t['trader_masked']}`")
        
        fig = go.Figure()
        
        x_values = t['timestamps'] if len(t['timestamps']) > 1 else list(range(len(t['equity_curve'])))
        
        color = '#10b981' if t['pnl'] > 0 else '#ef4444'
        
        fig.add_trace(go.Scatter(
            x=x_values,
            y=t['equity_curve'],
            mode='lines',
            line=dict(color=color, width=2),
            showlegend=False,
            hovertemplate='<b>Equity Curve</b><br>Value: %{customdata[0]:,.0f}<extra></extra>',
            customdata=list(zip(t['raw_equity'])) if len(t['raw_equity']) > 0 else None
        ))
        
        fig.update_layout(
            height=45,
            margin=dict(l=0, r=0, t=0, b=0),
            xaxis=dict(showticklabels=False, showgrid=False, zeroline=False),
            yaxis=dict(showticklabels=False, showgrid=False, zeroline=False, range=[0, 1]),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)'
        )
        
        cols[1].plotly_chart(
            fig, width='stretch',
            config={'displayModeBar': False},
            key=f"equity_curve_fixed_{i}"
        )
        
        pnl_color = '#10b981' if t['pnl'] > 0 else '#ef4444'
        cols[2].markdown(f"<span style='color:{pnl_color};font-weight:600;'>${t['pnl']:,.0f}</span>", unsafe_allow_html=True)
        cols[3].markdown(f"{t['win_rate']:.0f}%")
        cols[4].markdown(f"${t['max_dd']:,.0f}")
        cols[5].markdown(f"{t['trades']}")

# ============================================================================
# PROTOCOL EQUITY CHART
# ============================================================================

def create_protocol_equity_charts(positions_df, compact=False):
    """Protocol equity + drawdown as two separate charts â€” with compact option."""
    
    ps = positions_df.sort_values('close_time').copy()
    ps['cumulative_pnl'] = ps['realized_pnl'].cumsum()
    
    
    eq_height = 250 if compact else 350
    dd_height = 180 if compact else 250
    
    fig_eq = go.Figure()
    fig_eq.add_trace(go.Scatter(
        x=ps['close_time'],
        y=ps['cumulative_pnl'],
        line=dict(color='#6366f1', width=3),
        fill='tozeroy',
        fillcolor='rgba(99,102,241,0.1)',
        showlegend=False,
        hovertemplate='Date: %{x}<br>PnL: $%{y:,.2f}<extra></extra>'
    ))
    fig_eq.update_layout(
        title="ðŸ“ˆ Protocol PnL" if compact else "ðŸ“ˆ Protocol Cumulative PnL",
        xaxis_title="Date" if not compact else "",
        yaxis_title="PnL ($)",
        height=eq_height,
        margin=dict(l=40, r=40, t=40 if compact else 40, b=40),
        **CHART_BG
    )
    
    rolling_max = ps['cumulative_pnl'].cummax()
    drawdown = ps['cumulative_pnl'] - rolling_max
    max_dd = drawdown.min()
    
    fig_dd = go.Figure()
    fig_dd.add_trace(go.Scatter(
        x=ps['close_time'], y=drawdown,
        line=dict(color='#ef4444', width=2.5),
        fill='tozeroy', fillcolor='rgba(239,68,68,0.15)',
        showlegend=False
    ))
    fig_dd.add_hline(y=max_dd, line_dash="dash", line_color="#ef4444",
                    annotation_text=f"Max: ${max_dd:,.0f}" if compact else f"Max DD: ${max_dd:,.0f}",
                    annotation_position="bottom right")
    fig_dd.update_layout(
        title="ðŸ“‰ Drawdown" if compact else "ðŸ“‰ Drawdown from Peak",
        xaxis_title="Date" if not compact else "",
        yaxis_title="Drawdown ($)",
        height=dd_height,
        margin=dict(l=40, r=40, t=40 if compact else 40, b=40),
        **CHART_BG
    )
    
    return fig_eq, fig_dd

# ============================================================================
# PERSONAL EQUITY CHART
# ============================================================================

def create_personal_equity_chart(trader_positions, is_sparse_mode=False, compact=False):
    """Adaptive equity chart for personal mode with drawdown option."""
    
    if is_sparse_mode:
        # Just show a simple bar chart for sparse data
        fig = go.Figure()
        
        # Sort by date for timeline
        df = trader_positions.sort_values('close_time')
        
        colors = ['#10b981' if x > 0 else '#ef4444' for x in df['realized_pnl']]
        
        fig.add_trace(go.Bar(
            x=df['close_time'],
            y=df['realized_pnl'],
            marker_color=colors,
            text=df['realized_pnl'].apply(lambda x: f"${x:,.0f}"),
            textposition='outside',
            name='Trade PnL'
        ))
        
        fig.add_hline(y=0, line_dash="solid", line_color="gray", opacity=0.3)
        
        fig.update_layout(
            title="ðŸ“Š Your Trades (Individual)",
            xaxis_title="Date",
            yaxis_title="PnL ($)",
            height=300,
            showlegend=False,
            margin=dict(l=40, r=40, t=40, b=40),
            **CHART_BG
        )
        
        return fig, None  # Return None for drawdown chart
    
    # Original adaptive logic for non-sparse mode
    density = get_data_density(trader_positions)
    
    if density == "single":
        pnl = trader_positions['realized_pnl'].iloc[0]
        fig = go.Figure(go.Bar(
            x=['Your Trade'], y=[pnl],
            marker_color='#10b981' if pnl > 0 else '#ef4444',
            text=[f"${pnl:,.2f}"], textposition='outside', width=0.4
        ))
        fig.update_layout(
            title=f"Trade Result: {'ðŸŸ¢ Profit' if pnl > 0 else 'ðŸ”´ Loss'}",
            yaxis_title="PnL ($)", height=300, showlegend=False,
            margin=dict(l=40, r=40, t=40, b=40), **CHART_BG
        )
        
        # No drawdown for single trade
        return fig, None
    
    tp = trader_positions.sort_values('close_time').copy()
    tp['cumulative'] = tp['realized_pnl'].cumsum()
    
    if density == "sparse":
        # Step chart with markers
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=tp['close_time'], y=tp['cumulative'],
            mode='lines+markers',
            line=dict(shape='hv', width=3, color='#6366f1'),
            marker=dict(size=12, symbol='diamond', color='#6366f1'),
            fill='tozeroy', fillcolor='rgba(99,102,241,0.1)', name='Your PnL'
        ))
        fig.update_layout(
            title="ðŸ“ˆ Your Trading Performance",
            xaxis_title="Date", yaxis_title="Cumulative PnL ($)",
            height=300, margin=dict(l=40, r=40, t=40, b=40), **CHART_BG
        )
        
        # Calculate drawdown for sparse data
        rolling_max = tp['cumulative'].cummax()
        tp['drawdown'] = tp['cumulative'] - rolling_max
        max_dd = tp['drawdown'].min()
        
        fig_dd = go.Figure()
        fig_dd.add_trace(go.Scatter(
            x=tp['close_time'],
            y=tp['drawdown'],
            line=dict(color='#ef4444', width=2.5),
            fill='tozeroy',
            fillcolor='rgba(239,68,68,0.15)',
            showlegend=False
        ))
        fig_dd.add_hline(
            y=max_dd,
            line_dash="dash",
            line_color="#ef4444",
            annotation_text=f"Max DD: ${max_dd:,.0f}",
            annotation_position="bottom right"
        )
        fig_dd.update_layout(
            title="ðŸ“‰ Your Drawdown",
            xaxis_title="Date",
            yaxis_title="Drawdown ($)",
            height=200,
            margin=dict(l=40, r=40, t=40, b=40),
            **CHART_BG
        )
        
        return fig, fig_dd
    
    # Dense: full equity curve
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=tp['close_time'], y=tp['cumulative'],
        line=dict(color='#6366f1', width=3),
        fill='tozeroy', fillcolor='rgba(99,102,241,0.1)', name='Your PnL'
    ))
    
    # Set heights based on compact mode
    height_eq = 250 if compact else 300
    height_dd = 150 if compact else 200
    
    fig.update_layout(
        title="ðŸ“ˆ Your Equity Curve",
        xaxis_title="Date", yaxis_title="Cumulative PnL ($)",
        height=height_eq,  # â† USE THE VARIABLE HERE
        margin=dict(l=40, r=40, t=40, b=40),
        **CHART_BG
    )
    
    # Calculate drawdown for dense data
    rolling_max = tp['cumulative'].cummax()
    tp['drawdown'] = tp['cumulative'] - rolling_max
    max_dd = tp['drawdown'].min()
    
    fig_dd = go.Figure()
    fig_dd.add_trace(go.Scatter(
        x=tp['close_time'],
        y=tp['drawdown'],
        line=dict(color='#ef4444', width=2.5),
        fill='tozeroy',
        fillcolor='rgba(239,68,68,0.15)',
        showlegend=False
    ))
    
    fig_dd.add_hline(
        y=max_dd,
        line_dash="dash",
        line_color="#ef4444",
        annotation_text=f"Max DD: ${max_dd:,.0f}",
        annotation_position="bottom right"
    )
    
    fig_dd.update_layout(
        title="ðŸ“‰ Your Drawdown from Peak",
        xaxis_title="Date",
        yaxis_title="Drawdown ($)",
        height=height_dd,  # â† USE THE VARIABLE HERE
        margin=dict(l=40, r=40, t=40, b=40),
        **CHART_BG
    )
    
    return fig, fig_dd
    
# ============================================================================
# ADAPTIVE INFORMATION CARDS 
# ============================================================================

def display_trade_summary_cards(positions_df, title="Trade Summary"):
    """Display key trade metrics as information cards when charts aren't meaningful."""
    
    if positions_df.empty:
        st.info("No trade data available")
        return
    
    st.subheader(f"ðŸ“Š {title}")
    st.caption("Detailed trade information (chart not shown due to limited data)")
    
    # Key metrics in cards
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_trades = len(positions_df)
        st.metric("Total Trades", total_trades)
    
    with col2:
        winning_trades = (positions_df['realized_pnl'] > 0).sum()
        st.metric("Winning Trades", winning_trades)
    
    with col3:
        losing_trades = (positions_df['realized_pnl'] < 0).sum()
        st.metric("Losing Trades", losing_trades)
    
    with col4:
        win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0
        st.metric("Win Rate", f"{win_rate:.1f}%")
    
    # Trade list in expander
    with st.expander("ðŸ“‹ View Individual Trades", expanded=True):
        display_df = positions_df.copy()
        display_df['symbol'] = display_df['market_id'].apply(simplify_symbol)
        display_df = display_df[['close_time', 'symbol', 'product_type', 'side', 
                                 'entry_price', 'exit_price', 'size', 'realized_pnl', 'fees']]
        
        # Format for display
        display_df['close_time'] = pd.to_datetime(display_df['close_time']).dt.strftime('%Y-%m-%d %H:%M')
        display_df['entry_price'] = display_df['entry_price'].apply(lambda x: f"${x:,.2f}")
        display_df['exit_price'] = display_df['exit_price'].apply(lambda x: f"${x:,.2f}")
        display_df['size'] = display_df['size'].apply(lambda x: f"{x:,.4f}")
        display_df['realized_pnl'] = display_df['realized_pnl'].apply(lambda x: f"${x:,.2f}")
        display_df['fees'] = display_df['fees'].apply(lambda x: f"${x:,.2f}")
        
        st.dataframe(display_df, width='stretch', hide_index=True)


def display_performance_cards(positions_df, title="Performance Summary"):
    """Display performance metrics as cards for sparse data."""
    
    if positions_df.empty:
        st.info("No performance data available")
        return
    
    st.subheader(f"ðŸ“ˆ {title}")
    
    # Calculate metrics
    total_pnl = positions_df['realized_pnl'].sum()
    avg_win = positions_df[positions_df['realized_pnl'] > 0]['realized_pnl'].mean() if (positions_df['realized_pnl'] > 0).any() else 0
    avg_loss = positions_df[positions_df['realized_pnl'] < 0]['realized_pnl'].mean() if (positions_df['realized_pnl'] < 0).any() else 0
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total PnL", f"${total_pnl:,.2f}")
    
    with col2:
        st.metric("Avg Win", f"${avg_win:,.2f}" if avg_win != 0 else "N/A")
    
    with col3:
        st.metric("Avg Loss", f"${avg_loss:,.2f}" if avg_loss != 0 else "N/A")
    
    with col4:
        profit_factor = abs(avg_win / avg_loss) if avg_loss != 0 else float('inf')
        st.metric("Profit Factor", f"{profit_factor:.2f}x" if profit_factor != float('inf') else "âˆž")
    
    # Trade timeline
    st.subheader("ðŸ“… Trade Timeline")
    timeline_df = positions_df.sort_values('close_time')[['close_time', 'market_id', 'realized_pnl']].copy()
    timeline_df['market_id'] = timeline_df['market_id'].apply(simplify_symbol)
    timeline_df['close_time'] = pd.to_datetime(timeline_df['close_time']).dt.strftime('%Y-%m-%d')
    timeline_df.columns = ['Date', 'Symbol', 'PnL']
    
    st.dataframe(timeline_df, width='stretch', hide_index=True)
    
# ============================================================================
# PERSONAL DRAWDOWN CHART 
# ============================================================================

def create_personal_drawdown_chart(trader_positions):
    """Create drawdown chart for personal trader dashboard."""
    
    if trader_positions.empty:
        return None
    
    # Calculate cumulative PnL and drawdown
    df = trader_positions.sort_values('close_time').copy()
    df['cumulative_pnl'] = df['realized_pnl'].cumsum()
    rolling_max = df['cumulative_pnl'].cummax()
    df['drawdown'] = df['cumulative_pnl'] - rolling_max
    max_dd = df['drawdown'].min()
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=df['close_time'],
        y=df['drawdown'],
        line=dict(color='#ef4444', width=2.5),
        fill='tozeroy',
        fillcolor='rgba(239,68,68,0.15)',
        showlegend=False,
        hovertemplate='Date: %{x}<br>Drawdown: $%{y:,.2f}<extra></extra>'
    ))
    
    fig.add_hline(
        y=max_dd,
        line_dash="dash",
        line_color="#ef4444",
        annotation_text=f"Max DD: ${max_dd:,.0f}",
        annotation_position="bottom right"
    )
    
    fig.update_layout(
        title="ðŸ“‰ Your Drawdown from Peak",
        xaxis_title="Date",
        yaxis_title="Drawdown ($)",
        height=250,
        margin=dict(l=40, r=40, t=40, b=40),
        **CHART_BG
    )
    
    return fig

# ============================================================================
# LIQUIDATION ANALYTICS
# ============================================================================

def display_liquidation_analytics(positions_df, is_personal_mode=False, trader_id=None):
    """Liquidation analysis with close_reason handling and adaptive logic."""
    
    st.header("âš ï¸ Liquidation Risk Monitoring")
    
    if 'close_reason' not in positions_df.columns:
        st.info("â„¹ï¸ Liquidation tracking not available")
        return
    
    # Get symbol filter state from session
    has_symbol_filter = False
    if 'selected_symbols' in st.session_state:
        has_symbol_filter = len(st.session_state.selected_symbols) > 0
    
    # SPARSE DATA DETECTION - Check if we should show simplified view
    trade_count = len(positions_df)
    is_sparse_mode = False
    sparse_reason = ""
    
    # Very few trades overall
    if trade_count < 3:
        is_sparse_mode = True
        sparse_reason = "Very few trades available"
    # Symbol filter with few trades
    elif has_symbol_filter and trade_count < 8:
        is_sparse_mode = True
        symbol_text = f"for selected symbol{'s' if len(st.session_state.selected_symbols) > 1 else ''}"
        sparse_reason = f"Limited data {symbol_text}"
    # Date range with few trades
    elif 'start_date' in st.session_state and 'end_date' in st.session_state:
        days_selected = (st.session_state.end_date - st.session_state.start_date).days
        if days_selected <= 7 and trade_count < 10:
            is_sparse_mode = True
            sparse_reason = "Limited data for selected period"
    
    # Personal mode handling
    if is_personal_mode and trader_id:
        tp = positions_df[positions_df['trader_id'] == trader_id]
        liq = tp[tp['close_reason'] == 'liquidation']
        
        st.markdown("### âš ï¸ Your Riskiest Trades")
        
        if liq.empty:
            st.success("âœ… No liquidations in your history!")
            return
        
        # Even in personal mode, check if we should show simplified view
        if is_sparse_mode:
            context_note(f"{sparse_reason} - showing your loss-making trades")
            worst = tp.nsmallest(min(5, len(tp)), 'realized_pnl').copy()
            worst['symbol'] = worst['market_id'].apply(simplify_symbol)
            
            # Show as table instead of chart for sparse data
            st.dataframe(
                worst[['close_time', 'symbol', 'side', 'realized_pnl']].assign(
                    close_time=pd.to_datetime(worst['close_time']).dt.strftime('%Y-%m-%d %H:%M'),
                    realized_pnl=worst['realized_pnl'].apply(lambda x: f"${x:,.2f}")
                ),
                width='stretch',
                hide_index=True,
                column_config={
                    "close_time": "Time",
                    "symbol": "Symbol",
                    "side": "Side",
                    "realized_pnl": "Loss"
                }
            )
            return
        
        # Normal personal mode with chart
        worst = tp.nsmallest(5, 'realized_pnl').copy()
        worst['symbol'] = worst['market_id'].apply(simplify_symbol)
        
        fig = px.bar(worst, x='symbol', y='realized_pnl',
                    title='Your Top 5 Loss-Making Trades',
                    color='realized_pnl', color_continuous_scale='Reds_r',
                    labels={'realized_pnl': 'Loss ($)'})
        fig.update_layout(height=350, **CHART_BG)
        st.plotly_chart(fig, width='stretch', key="personal_liq_bar")
        return
    
    # Protocol mode
    liq = positions_df[positions_df['close_reason'] == 'liquidation']
    
    if liq.empty:
        st.success("âœ… No liquidations in selected period")
        return
    
    # SPARSE MODE - Show simplified view
    if is_sparse_mode:
        context_note(f"{sparse_reason} - showing liquidation summary")
        
        # Show key metrics
        c1, c2, c3 = st.columns(3)
        c1.metric("Total Liquidations", len(liq))
        c2.metric("Affected Traders", liq['trader_id'].nunique())
        c3.metric("Total Loss", f"${abs(liq['realized_pnl'].sum()):,.0f}")
        
        # Show liquidation list instead of charts
        st.subheader("ðŸ“‹ Liquidation Events")
        liq_display = liq[['close_time', 'trader_id', 'market_id', 'side', 'realized_pnl']].copy()
        liq_display['trader'] = liq_display['trader_id'].apply(mask_trader_id)
        liq_display['symbol'] = liq_display['market_id'].apply(simplify_symbol)
        liq_display['close_time'] = pd.to_datetime(liq_display['close_time']).dt.strftime('%Y-%m-%d %H:%M')
        liq_display['realized_pnl'] = liq_display['realized_pnl'].apply(lambda x: f"${x:,.2f}")
        
        st.dataframe(
            liq_display[['close_time', 'trader', 'symbol', 'side', 'realized_pnl']],
            width='stretch',
            hide_index=True
        )
        return
    
    # NORMAL MODE - Full analytics
    c1, c2, c3 = st.columns(3)
    c1.metric("Total Liquidations", len(liq))
    c2.metric("Affected Traders", liq['trader_id'].nunique())
    c3.metric("Total Loss", f"${abs(liq['realized_pnl'].sum()):,.0f}")
    
    st.subheader("ðŸ“Š Liquidation Distribution by Trader")
    
    liq_by_trader = liq.groupby('trader_id').agg({
        'realized_pnl': lambda x: abs(x.sum()),
        'position_id': 'count'
    }).reset_index()
    liq_by_trader.columns = ['trader_id', 'loss', 'count']
    liq_by_trader['trader'] = liq_by_trader['trader_id'].apply(mask_trader_id)
    liq_by_trader = liq_by_trader.sort_values('loss', ascending=False)
    
    fig = go.Figure(data=[go.Pie(
        labels=liq_by_trader['trader'],
        values=liq_by_trader['loss'],
        textinfo='label+percent', textposition='outside',
        insidetextorientation='radial',
        marker=dict(colors=px.colors.sequential.Reds_r,
                   line=dict(color='#1e293b', width=2)),
        hovertemplate='<b>%{label}</b><br>Loss: $%{value:,.0f}<br>Liquidations: %{customdata}<extra></extra>',
        customdata=liq_by_trader['count']
    )])
    
    fig.update_layout(height=400, showlegend=False, **CHART_BG)
    st.plotly_chart(fig, width='stretch', key="liq_pie")
    
    st.subheader("ðŸ“Š Liquidation Rate by Trader")
    
    stats = []
    for trader in positions_df['trader_id'].unique():
        td = positions_df[positions_df['trader_id'] == trader]
        
        total = len(td[td['close_reason'].isin(['close', 'liquidation'])])
        
        if total > 0:
            liq_n = len(td[td['close_reason'] == 'liquidation'])
            close_n = len(td[td['close_reason'] == 'close'])
            
            stats.append({
                'trader': mask_trader_id(trader),
                'liq_rate': (liq_n / total) * 100,
                'liq_count': liq_n,
                'close_count': close_n,
                'total_trades': total
            })
    
    if stats:
        df = pd.DataFrame(stats)
        
        df_with_liq = df[df['liq_count'] > 0].copy()
        
        if not df_with_liq.empty:
            df_top5 = df_with_liq.sort_values('liq_rate', ascending=False).head(5)
            df_top5 = df_top5.sort_values('liq_rate', ascending=True)
            
            colors = []
            for rate in df_top5['liq_rate']:
                if rate < 2:
                    colors.append('#10b981')
                elif rate < 5:
                    colors.append('#f59e0b')
                else:
                    colors.append('#ef4444')
            
            fig = go.Figure()
            fig.add_trace(go.Bar(
                y=df_top5['trader'], x=df_top5['liq_rate'],
                orientation='h', marker_color=colors,
                text=df_top5['liq_rate'].apply(lambda x: f"{x:.1f}%"),
                textposition='outside',
                hovertemplate='<b>%{y}</b><br>Rate: %{x:.1f}%<br>Liquidations: %{customdata[0]}/%{customdata[1]} trades<extra></extra>',
                customdata=df_top5[['liq_count', 'total_trades']].values
            ))
            
            fig.add_vline(x=2, line_dash="dash", line_color="#10b981",
                          annotation_text="Low Risk", annotation_position="top")
            fig.add_vline(x=5, line_dash="dash", line_color="#ef4444",
                          annotation_text="High Risk", annotation_position="top")
            
            fig.update_layout(
                title="Top 5 Traders by Liquidation Rate",
                xaxis_title="Liquidation Rate (%)", yaxis_title="",
                height=250, margin=dict(l=120, r=40, t=60, b=40), **CHART_BG
            )
            fig.update_xaxes(range=[0, 100])
            
            st.plotly_chart(fig, width='stretch', key="liq_rate_top5")
            
            excluded_count = len(df[df['liq_count'] == 0])
            if excluded_count > 0:
                st.caption(f"â„¹ï¸ {excluded_count} traders with 0% liquidation rate not shown")
        else:
            st.info("No traders with liquidations to display")
    
    st.subheader("ðŸ’° Financial Impact")
    c1, c2 = st.columns(2)
    
    with c1:
        bm = liq.groupby('market_id')['realized_pnl'].sum().abs().reset_index()
        bm['symbol'] = bm['market_id'].apply(simplify_symbol)
        bm = bm.sort_values('realized_pnl', ascending=False).head(5)
        fig = px.bar(bm, x='symbol', y='realized_pnl',
                    title='Top 5 Markets by Liq Loss',
                    color='realized_pnl', color_continuous_scale='Reds')
        fig.update_layout(height=300, **CHART_BG)
        st.plotly_chart(fig, width='stretch', key="liq_mkt")
    
    with c2:
        bt = liq.groupby('trader_id')['realized_pnl'].sum().abs().reset_index()
        bt['trader'] = bt['trader_id'].apply(mask_trader_id)
        bt = bt.sort_values('realized_pnl', ascending=False).head(5)
        fig = px.bar(bt, x='trader', y='realized_pnl',
                    title='Top 5 Traders by Liq Loss',
                    color='realized_pnl', color_continuous_scale='Reds')
        fig.update_layout(height=300, **CHART_BG)
        st.plotly_chart(fig, width='stretch', key="liq_trader")

# ============================================================================
# TIME-BASED PERFORMANCE ANALYSIS
# ============================================================================

def display_time_performance(positions_df, pnl_day_df=None, pnl_hour_df=None):
    """Daily and hourly performance analysis."""
    
    st.header("ðŸ“… Time-Based Performance")
    
    if positions_df.empty:
        st.info("No performance data available")
        return
    
    # Daily PnL chart
    if pnl_day_df is not None and not pnl_day_df.empty:
        st.subheader("ðŸ“Š Daily Performance")
        
        # Ensure date column is datetime
        if 'date' in pnl_day_df.columns:
            pnl_day_df['date'] = pd.to_datetime(pnl_day_df['date'])
        
        # Daily PnL bar chart
        fig = go.Figure()
        
        colors = ['#10b981' if x > 0 else '#ef4444' for x in pnl_day_df['daily_pnl']]
        
        fig.add_trace(go.Bar(
            x=pnl_day_df['date'],
            y=pnl_day_df['daily_pnl'],
            marker_color=colors,
            text=pnl_day_df['daily_pnl'].apply(lambda x: f"${x:,.0f}"),
            textposition='outside',
            hovertemplate='Date: %{x}<br>PnL: $%{y:,.2f}<br>Trades: %{customdata}<extra></extra>',
            customdata=pnl_day_df['trade_count'] if 'trade_count' in pnl_day_df.columns else None
        ))
        
        fig.add_hline(y=0, line_dash="solid", line_color="gray", opacity=0.3)
        
        fig.update_layout(
            title="Daily PnL",
            xaxis_title="Date",
            yaxis_title="PnL ($)",
            height=350,
            **CHART_BG
        )
        
        st.plotly_chart(fig, width='stretch', key="daily_pnl")
        
        # Daily statistics
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Best Day", f"${pnl_day_df['daily_pnl'].max():,.0f}")
        c2.metric("Worst Day", f"${pnl_day_df['daily_pnl'].min():,.0f}")
        c3.metric("Avg Daily PnL", f"${pnl_day_df['daily_pnl'].mean():,.0f}")
        winning_days = (pnl_day_df['daily_pnl'] > 0).sum()
        total_days = len(pnl_day_df)
        c4.metric("Winning Days", f"{winning_days}/{total_days} ({winning_days/total_days*100:.0f}%)")
    
    else:
        # Generate from positions if pnl_day not available
        st.subheader("ðŸ“Š Daily Performance")
        
        daily = positions_df.copy()
        daily['date'] = pd.to_datetime(daily['close_time']).dt.date
        
        daily_pnl = daily.groupby('date').agg({
            'realized_pnl': ['sum', 'count']
        }).reset_index()
        daily_pnl.columns = ['date', 'daily_pnl', 'trade_count']
        
        colors = ['#10b981' if x > 0 else '#ef4444' for x in daily_pnl['daily_pnl']]
        
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=daily_pnl['date'],
            y=daily_pnl['daily_pnl'],
            marker_color=colors,
            text=daily_pnl['daily_pnl'].apply(lambda x: f"${x:,.0f}"),
            textposition='outside',
            hovertemplate='Date: %{x}<br>PnL: $%{y:,.2f}<br>Trades: %{customdata}<extra></extra>',
            customdata=daily_pnl['trade_count']
        ))
        
        fig.add_hline(y=0, line_dash="solid", line_color="gray", opacity=0.3)
        fig.update_layout(
            title="Daily PnL",
            xaxis_title="Date",
            yaxis_title="PnL ($)",
            height=350,
            **CHART_BG
        )
        
        st.plotly_chart(fig, width='stretch', key="daily_pnl_gen")
    
    # Hourly performance
    if pnl_hour_df is not None and not pnl_hour_df.empty and 'hour' in pnl_hour_df.columns:
        st.subheader("ðŸ• Hourly Performance Pattern")
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            x=pnl_hour_df['hour'],
            y=pnl_hour_df['avg_pnl'] if 'avg_pnl' in pnl_hour_df.columns else pnl_hour_df['total_pnl'],
            marker_color='#6366f1',
            text=pnl_hour_df['trade_count'] if 'trade_count' in pnl_hour_df.columns else None,
            texttemplate='%{text} trades',
            textposition='outside',
            hovertemplate='Hour: %{x}:00<br>Avg PnL: $%{y:,.2f}<extra></extra>'
        ))
        
        fig.add_hline(y=0, line_dash="solid", line_color="gray", opacity=0.3)
        
        fig.update_layout(
            title="Average PnL by Hour of Day (UTC)",
            xaxis_title="Hour (24h format)",
            yaxis_title="Average PnL ($)",
            height=300,
            **CHART_BG
        )
        fig.update_xaxes(tickmode='linear', dtick=2)
        
        st.plotly_chart(fig, width='stretch', key="hourly_pnl")
        
        # Best/worst trading hours
        if 'avg_pnl' in pnl_hour_df.columns:
            best_hour = pnl_hour_df.loc[pnl_hour_df['avg_pnl'].idxmax()]
            worst_hour = pnl_hour_df.loc[pnl_hour_df['avg_pnl'].idxmin()]
            
            c1, c2 = st.columns(2)
            c1.metric(
                "Best Trading Hour", 
                f"{int(best_hour['hour'])}:00 UTC",
                f"${best_hour['avg_pnl']:,.0f} avg"
            )
            c2.metric(
                "Worst Trading Hour",
                f"{int(worst_hour['hour'])}:00 UTC",
                f"${worst_hour['avg_pnl']:,.0f} avg"
            )
    
    else:
        # Generate from positions
        st.subheader("ðŸ• Hourly Performance Pattern")
        
        hourly = positions_df.copy()
        hourly['hour'] = pd.to_datetime(hourly['close_time']).dt.hour
        
        hourly_pnl = hourly.groupby('hour').agg({
            'realized_pnl': ['mean', 'count']
        }).reset_index()
        hourly_pnl.columns = ['hour', 'avg_pnl', 'trade_count']
        
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=hourly_pnl['hour'],
            y=hourly_pnl['avg_pnl'],
            marker_color='#6366f1',
            text=hourly_pnl['trade_count'],
            texttemplate='%{text} trades',
            textposition='outside',
            hovertemplate='Hour: %{x}:00<br>Avg PnL: $%{y:,.2f}<extra></extra>'
        ))
        
        fig.add_hline(y=0, line_dash="solid", line_color="gray", opacity=0.3)
        fig.update_layout(
            title="Average PnL by Hour of Day (UTC)",
            xaxis_title="Hour (24h format)",
            yaxis_title="Average PnL ($)",
            height=300,
            **CHART_BG
        )
        fig.update_xaxes(tickmode='linear', dtick=2)
        
        st.plotly_chart(fig, width='stretch', key="hourly_pnl_gen")
                
# ============================================================================
# VOLUME ANALYSIS
# ============================================================================

def display_volume_analysis(positions_df):
    """Volume analysis with product tabs, progress bars, and trade duration."""
    
    st.header("ðŸ“Š Trading Volume Analysis")
    
    if positions_df.empty:
        st.info("No volume data available")
        return
    
    # Check if we have symbol filter applied and sparse data
    has_symbol_filter = len(selected_symbols) > 0
    trade_count = len(positions_df)
    
    if has_symbol_filter and trade_count < 5:
        context_note(f"Limited volume data for selected symbol{'s' if len(selected_symbols)>1 else ''} - showing summary cards")
        display_trade_summary_cards(positions_df, "Volume Summary")
        return
    
    positions_df = calculate_volume_usd(positions_df)  
    product_counts = positions_df['product_type'].value_counts()
    st.caption(f"Product types present: {', '.join([f'{k}({v})' for k, v in product_counts.items()])}")
    
    total_vol = positions_df['volume_usd'].sum()
    total_fees = positions_df['fees'].sum()
    unique_sym = positions_df['market_id'].apply(simplify_symbol).nunique()
    
    vol_shares = positions_df.groupby(
        positions_df['market_id'].apply(simplify_symbol))['volume_usd'].sum() / total_vol
    hhi = (vol_shares ** 2).sum() * 10000
    
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Total Volume", f"${total_vol:,.0f}")
    c2.metric("Total Fees", f"${total_fees:,.0f}")
    c3.metric("Active Symbols", unique_sym)
    
    conc = "Low" if hhi < 1500 else "Medium" if hhi < 2500 else "High"
    c4.metric("Concentration", f"{hhi:.0f} ({conc})")
    
    density = get_data_density(positions_df)
    
    if density in ["sparse", "moderate", "dense"]:
        st.subheader("ðŸ“Š Trade Size Distribution")
        
        c1, c2 = st.columns(2)
        with c1:
            fig = px.box(
                positions_df, x='product_type', y='volume_usd', points='all',
                title='Trade Size by Product Type', color='product_type',
                color_discrete_map={'spot':'#10b981','perp':'#6366f1','option':'#f59e0b'}
            )
            fig.update_layout(height=300, showlegend=False, **CHART_BG)
            st.plotly_chart(fig, width='stretch', key="box_overall")
        
        with c2:
            fig = px.histogram(
                positions_df, x='volume_usd', nbins=30,
                title='Trade Size Histogram',
                color_discrete_sequence=['#6366f1']
            )
            fig.update_layout(height=300, showlegend=False, **CHART_BG)
            st.plotly_chart(fig, width='stretch', key="hist_overall")
        
        vals = positions_df['volume_usd']
        s1, s2, s3, s4, s5 = st.columns(5)
        s1.metric("Median", f"${vals.median():,.0f}")
        s2.metric("Mean", f"${vals.mean():,.0f}")
        s3.metric("P25", f"${vals.quantile(0.25):,.0f}")
        s4.metric("P75", f"${vals.quantile(0.75):,.0f}")
        s5.metric("Max", f"${vals.max():,.0f}")
    
    # ==========================================================================
    # TRADE DURATION ANALYSIS 
    # ==========================================================================
    if 'duration_seconds' in positions_df.columns:
        st.subheader("â±ï¸ Trade Duration Analysis")
        
        # Convert to hours for better readability
        positions_df['duration_hours'] = positions_df['duration_seconds'] / 3600
        
        # Duration metrics
        c1, c2, c3 = st.columns(3)
        
        avg_duration = positions_df['duration_hours'].mean()
        median_duration = positions_df['duration_hours'].median()
        max_duration = positions_df['duration_hours'].max()
        
        c1.metric("Average Duration", f"{avg_duration:.1f}h")
        c2.metric("Median Duration", f"{median_duration:.1f}h")
        c3.metric("Longest Trade", f"{max_duration:.1f}h")
        
        # Duration by product type - box plot
        fig = px.box(
            positions_df,
            x='product_type',
            y='duration_hours',
            points='all',
            title='Trade Duration by Product Type',
            color='product_type',
            color_discrete_map={'spot':'#10b981','perp':'#6366f1','option':'#f59e0b'},
            labels={'duration_hours': 'Duration (hours)', 'product_type': 'Product Type'}
        )
        fig.update_layout(height=300, showlegend=False, **CHART_BG)
        st.plotly_chart(fig, width='stretch', key="duration_box")
        
        # Duration categories for PnL analysis
        def categorize_duration(hours):
            if hours < 1:
                return 'Scalp (<1h)'
            elif hours < 24:
                return 'Intraday (1-24h)'
            elif hours < 168:  # 7 days
                return 'Swing (1-7d)'
            else:
                return 'Position (>7d)'
        
        positions_df['duration_category'] = positions_df['duration_hours'].apply(categorize_duration)
        
        # Calculate statistics by category
        cat_stats = positions_df.groupby('duration_category').agg({
            'realized_pnl': ['count', 'mean', 'sum']
        }).round(2)
        cat_stats.columns = ['Trades', 'Avg PnL', 'Total PnL']
        cat_stats = cat_stats.reset_index()
        
        # Add win rate
        win_rates = positions_df.groupby('duration_category')['realized_pnl'].apply(
            lambda x: (x > 0).mean() * 100
        ).values
        cat_stats['Win Rate'] = win_rates
        
        # Sort categories in logical order
        category_order = ['Scalp (<1h)', 'Intraday (1-24h)', 'Swing (1-7d)', 'Position (>7d)']
        cat_stats['duration_category'] = pd.Categorical(
            cat_stats['duration_category'], 
            categories=category_order, 
            ordered=True
        )
        cat_stats = cat_stats.sort_values('duration_category')
        
        # Bar chart of PnL by duration category
        fig = px.bar(
            cat_stats,
            x='duration_category',
            y='Total PnL',
            text='Trades',
            title='PnL by Trade Duration Category',
            color='Total PnL',
            color_continuous_scale='RdYlGn',
            labels={
                'duration_category': 'Duration Category', 
                'Total PnL': 'Total PnL ($)'
            }
        )
        fig.update_traces(
            texttemplate='%{text} trades', 
            textposition='outside',
            textfont=dict(size=12)
        )
        fig.update_layout(height=300, **CHART_BG)
        st.plotly_chart(fig, width='stretch', key="duration_pnl")
        
        # Optional: Show the detailed table in an expander
        with st.expander("ðŸ“‹ View Duration Category Details"):
            display_df = cat_stats.copy()
            display_df['Avg PnL'] = display_df['Avg PnL'].apply(lambda x: f"${x:,.2f}")
            display_df['Total PnL'] = display_df['Total PnL'].apply(lambda x: f"${x:,.0f}")
            display_df['Win Rate'] = display_df['Win Rate'].apply(lambda x: f"{x:.1f}%")
            st.dataframe(
                display_df[['duration_category', 'Trades', 'Win Rate', 'Avg PnL', 'Total PnL']],
                width='stretch',
                hide_index=True,
                column_config={
                    "duration_category": "Duration Category",
                    "Trades": "Trade Count",
                    "Win Rate": "Win Rate",
                    "Avg PnL": "Avg PnL",
                    "Total PnL": "Total PnL"
                }
            )
    
    # ==========================================================================
    # PRODUCT TABS 
    # ==========================================================================
    tabs = st.tabs(["ðŸ“ˆ All", "ðŸ“ Spot", "âš¡ Perp", "ðŸŽ¯ Options"])
    products = {
        "All": positions_df,
        "Spot": positions_df[positions_df['product_type'] == 'spot'],
        "Perp": positions_df[positions_df['product_type'] == 'perp'],
        "Options": positions_df[positions_df['product_type'] == 'option']
    }
    
    for tidx, (tab, (pname, pdf)) in enumerate(zip(tabs, products.items())):
        with tab:
            if pdf.empty:
                st.info(f"No {pname} trades in selected period")
                continue
            
            st.caption(f"{len(pdf)} trades")
            c1, c2 = st.columns(2)
            
            with c1:
                st.markdown("#### Volume by Symbol - Top 5")
                
                sym_vol = pdf.groupby(pdf['market_id'].apply(simplify_symbol)).agg(
                    volume_usd=('volume_usd','sum'),
                    realized_pnl=('realized_pnl','sum')
                ).sort_values('volume_usd', ascending=False).head(5)
                
                if not sym_vol.empty:
                    total = sym_vol['volume_usd'].sum()
                    for sym, row in sym_vol.iterrows():
                        pct = (row['volume_usd'] / total * 100) if total > 0 else 0
                        pc = "#10b981" if row['realized_pnl'] > 0 else "#ef4444"
                        
                        st.markdown(f"""
                        <div style='background:rgba(30,41,59,0.4); border-radius:8px; padding:10px; margin-bottom:8px;'>
                            <div style='display:flex; justify-content:space-between; margin-bottom:4px;'>
                                <span style='color:#94a3b8; font-size:0.85rem;'>{sym}</span>
                                <span style='color:#f1f5f9; font-size:0.9rem; font-weight:600;'>
                                    ${row['volume_usd']:,.0f} ({pct:.1f}%)
                                    <span style='color:{pc};'>${row['realized_pnl']:,.0f}</span>
                                </span>
                            </div>
                            <div style='background:rgba(100,116,139,0.3); border-radius:4px; height:6px;'>
                                <div style='background:#6366f1; width:{pct}%; height:100%; border-radius:4px;'></div>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
                
                st.markdown("#### Fee Generation")
                fsym = pdf.groupby(pdf['market_id'].apply(simplify_symbol))['fees'].sum()\
                    .sort_values(ascending=False).head(5)
                
                if not fsym.empty:
                    fig = px.bar(x=fsym.values, y=fsym.index, orientation='h',
                                title='Top 5 Symbols by Fees',
                                color=fsym.values, color_continuous_scale='Reds')
                    fig.update_layout(height=200, **CHART_BG, margin=dict(l=80))
                    st.plotly_chart(fig, width='stretch', key=f"fee_{tidx}")
            
            with c2:
                st.markdown("#### Long vs Short Distribution")
                
                long_vol = pdf[pdf['side'].str.lower().isin(['long','buy'])]['volume_usd'].sum()
                short_vol = pdf[pdf['side'].str.lower().isin(['short','sell'])]['volume_usd'].sum()
                total_v = long_vol + short_vol
                
                if total_v > 0:
                    lp, sp = long_vol/total_v*100, short_vol/total_v*100
                    
                    fig = go.Figure()
                    fig.add_trace(go.Bar(
                        y=['Direction'], x=[lp], name='Long', orientation='h',
                        marker_color='#10b981', text=f'{lp:.1f}%',
                        textposition='inside', textfont=dict(color='white', size=14)
                    ))
                    fig.add_trace(go.Bar(
                        y=['Direction'], x=[sp], name='Short', orientation='h',
                        marker_color='#ef4444', text=f'{sp:.1f}%',
                        textposition='inside', textfont=dict(color='white', size=14)
                    ))
                    
                    fig.update_layout(
                        barmode='stack', height=100,
                        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                        margin=dict(l=40, r=20, t=30, b=10), **CHART_BG
                    )
                    st.plotly_chart(fig, width='stretch', key=f"ls_{tidx}")
                    
                    ratio = long_vol / short_vol if short_vol > 0 else float('inf')
                    st.metric("Long/Short Ratio", f"{ratio:.2f}x" if ratio != float('inf') else "(No shorts)")
                
                st.markdown("#### PnL Distribution")
                
                if should_show_chart(pdf, min_points=3):
                    fig = px.histogram(pdf, x='realized_pnl', nbins=20,
                                      title='PnL Distribution',
                                      color_discrete_sequence=['#6366f1'])
                    fig.add_vline(x=0, line_dash="dash", line_color="gray")
                    fig.update_layout(height=250, **CHART_BG)
                    st.plotly_chart(fig, width='stretch', key=f"pnl_hist_{tidx}")
                else:
                    context_note("Too few trades for distribution chart - showing individual trades")
                    st.dataframe(
                        pdf[['market_id','side','realized_pnl']].assign(
                            market_id=pdf['market_id'].apply(simplify_symbol),
                            realized_pnl=pdf['realized_pnl'].apply(lambda x: f"${x:,.2f}")
                        ),
                        width='stretch', hide_index=True, key=f"pnl_list_{tidx}"
                    )
                    
# ============================================================================
# ORDER TYPE PERFORMANCE
# ============================================================================

def display_order_type_performance(order_df, positions_df=None):
    """Enhanced order type performance with multiple visualizations."""
    
    st.header("ðŸ“Š Order Type Performance Analysis")
    
    # Check if we have any positions data
    if positions_df is None or positions_df.empty:
        st.info("â„¹ï¸ No trades in the selected period to analyze order types")
        return
     
    # Get symbol filter state from session
    has_symbol_filter = False
    if 'selected_symbols' in st.session_state:
        has_symbol_filter = len(st.session_state.selected_symbols) > 0
    
    # SPARSE DATA DETECTION - Check if we should show simplified view
    is_sparse_mode = False
    sparse_reason = ""
    
    if positions_df is not None and not positions_df.empty:
        trade_count = len(positions_df)
        
        # Very few trades overall
        if trade_count < 3:
            is_sparse_mode = True
            sparse_reason = "Very few trades available"
        # Symbol filter with few trades
        elif has_symbol_filter and trade_count < 8:
            is_sparse_mode = True
            symbol_text = f"for selected symbol{'s' if len(st.session_state.selected_symbols) > 1 else ''}"
            sparse_reason = f"Limited data {symbol_text}"
        # Date range with few trades
        elif 'start_date' in st.session_state and 'end_date' in st.session_state:
            days_selected = (st.session_state.end_date - st.session_state.start_date).days
            if days_selected <= 7 and trade_count < 10:
                is_sparse_mode = True
                sparse_reason = "Limited data for selected period"
    
    # If sparse mode, show simplified card view
    if is_sparse_mode and positions_df is not None and not positions_df.empty:
        context_note(f"{sparse_reason} - showing individual trade breakdown")
        
        # Prepare data for display
        display_df = positions_df.copy()
        display_df['symbol'] = display_df['market_id'].apply(simplify_symbol)
        display_df = display_df[['close_time', 'symbol', 'product_type', 'side', 
                                 'entry_price', 'exit_price', 'size', 'realized_pnl', 'fees']]
        
        # Format for display
        display_df['close_time'] = pd.to_datetime(display_df['close_time']).dt.strftime('%Y-%m-%d %H:%M')
        display_df['entry_price'] = display_df['entry_price'].apply(lambda x: f"${x:,.2f}")
        display_df['exit_price'] = display_df['exit_price'].apply(lambda x: f"${x:,.2f}")
        display_df['size'] = display_df['size'].apply(lambda x: f"{x:,.4f}")
        display_df['realized_pnl'] = display_df['realized_pnl'].apply(lambda x: f"${x:,.2f}")
        display_df['fees'] = display_df['fees'].apply(lambda x: f"${x:,.2f}")
        
        st.subheader("ðŸ“‹ Individual Trades by Type")
        st.dataframe(display_df, width='stretch', hide_index=True)
        
        # Show summary metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Trades", len(positions_df))
        with col2:
            win_rate = (positions_df['realized_pnl'] > 0).mean() * 100
            st.metric("Win Rate", f"{win_rate:.1f}%")
        with col3:
            total_pnl = positions_df['realized_pnl'].sum()
            st.metric("Total PnL", f"${total_pnl:,.2f}")
        with col4:
            product_types = positions_df['product_type'].nunique()
            st.metric("Product Types", product_types)
        
        return
    
    # NORMAL MODE - Full analysis
    # ALWAYS derive from positions_df if available
    if positions_df is not None and not positions_df.empty:
        df = positions_df.copy()
        
        if 'volume_usd' not in df.columns:
            df['volume_usd'] = df['exit_price'] * df['size']
        
        if 'product_type' in df.columns:
            df['order_category'] = df['product_type']
            category_name = "Product Type"
            product_counts = df['product_type'].value_counts()
            st.caption(f"ðŸ“Š Distribution: {', '.join([f'{k}({v})' for k, v in product_counts.items()])}")
        else:
            df['order_category'] = df.apply(lambda row: 
                'scalp' if row.get('duration_seconds', 0) < 300 else
                'intraday' if row.get('duration_seconds', 0) < 3600 else
                'swing' if row.get('duration_seconds', 0) < 86400 else
                'position', axis=1
            )
            category_name = "Trade Duration"
        
        # Calculate metrics
        order_stats = df.groupby('order_category').agg({
            'realized_pnl': ['count', 'mean', 'sum'],
            'fees': 'sum',
            'volume_usd': 'sum'
        }).round(2)
        
        order_stats.columns = ['trade_count', 'avg_pnl', 'total_pnl', 'total_fees', 'total_volume']
        order_stats = order_stats.reset_index()
        
        order_stats['win_rate'] = df.groupby('order_category')['realized_pnl'].apply(
            lambda x: (x > 0).mean() * 100
        ).values
        
        order_stats['fee_ratio'] = (order_stats['total_fees'] / order_stats['total_volume'] * 100).fillna(0)
        order_stats.rename(columns={'order_category': 'order_type'}, inplace=True)
        
        st.info(f"ðŸ“Œ Classified by: **{category_name}**")
        order_df = order_stats
    
    # SAFETY CHECK - if still no data or missing columns
    if order_df is None or order_df.empty:
        st.warning("âš ï¸ No order type data available for selected filters.")
        st.info("ðŸ’¡ Try selecting a wider date range or different symbols.")
        return
    
    # Ensure required columns exist
    required_cols = ['order_type', 'trade_count', 'win_rate', 'avg_pnl']
    missing_cols = [col for col in required_cols if col not in order_df.columns]
    
    if missing_cols:
        st.error(f"âŒ Missing required columns: {', '.join(missing_cols)}")
        st.info("ðŸ’¡ This usually happens when the date filter excludes all trades.")
        return
    
    # Add total_pnl 
    if 'total_pnl' not in order_df.columns:
        order_df['total_pnl'] = order_df['avg_pnl'] * order_df['trade_count']
    
    # Add volume/fee columns 
    if 'total_volume' not in order_df.columns:
        order_df['total_volume'] = 0
    if 'total_fees' not in order_df.columns:
        order_df['total_fees'] = 0
    if 'fee_ratio' not in order_df.columns:
        order_df['fee_ratio'] = 0
           
    # Create four columns for key metrics
    col1, col2, col3, col4 = st.columns(4)
    
    total_trades = order_df['trade_count'].sum()
    avg_win_rate = order_df['win_rate'].mean()
    best_order = order_df.loc[order_df['win_rate'].idxmax(), 'order_type']
    worst_order = order_df.loc[order_df['win_rate'].idxmin(), 'order_type']
    
    col1.metric("Total Orders", f"{total_trades}")
    col2.metric("Avg Win Rate", f"{avg_win_rate:.1f}%")
    col3.metric("Best Performer", best_order.upper())
    col4.metric("Worst Performer", worst_order.upper())
    
    # Tabs for different visualizations
    tab1, tab2, tab3, tab4 = st.tabs([
        "ðŸ“Š Performance Matrix", 
        "ðŸ“ˆ Win Rate Analysis", 
        "ðŸ’° PnL Breakdown",
        "ðŸ“‹ Detailed Table"
    ])
    
    with tab1:
        # Performance Matrix - Bubble chart
        fig = px.scatter(
            order_df,
            x='win_rate',
            y='avg_pnl',
            size='trade_count',
            color='order_type',
            text='order_type',
            title="Order Type Performance Matrix",
            labels={
                'win_rate': 'Win Rate (%)',
                'avg_pnl': 'Average PnL ($)',
                'trade_count': 'Number of Trades'
            },
            size_max=60,
            color_discrete_map={
                'spot': '#10b981',
                'perp': '#6366f1',
                'option': '#f59e0b'
            }
        )
        
        fig.update_traces(
            textposition='top center',
            textfont=dict(size=12, color='white')
        )
        
        # Add quadrant lines
        fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5)
        fig.add_vline(x=50, line_dash="dash", line_color="gray", opacity=0.5)
        
        # Annotate quadrants
        fig.add_annotation(
            x=75, 
            y=order_df['avg_pnl'].max() * 0.8, 
            text="ðŸŒŸ STAR PERFORMERS", 
            showarrow=False,
            font=dict(color="#10b981", size=14)
        )
        fig.add_annotation(
            x=25, 
            y=order_df['avg_pnl'].min() * 0.8, 
            text="âš ï¸ NEEDS REVIEW", 
            showarrow=False,
            font=dict(color="#ef4444", size=14)
        )
        
        fig.update_layout(height=500, **CHART_BG)
        st.plotly_chart(fig, width='stretch', key="order_matrix")
        
        # Add explanation
        with st.expander("ðŸ“– How to read this chart"):
            st.markdown("""
            - **Top Right Quadrant** ðŸŒŸ: High win rate + positive PnL (Best performers)
            - **Top Left Quadrant** ðŸ“ˆ: Low win rate but positive PnL (Few big wins)
            - **Bottom Right Quadrant** ðŸ“‰: High win rate but negative PnL (Many small losses)
            - **Bottom Left Quadrant** âš ï¸: Low win rate + negative PnL (Needs review)
            
            Bubble size = Number of trades
            """)
    
    with tab2:
        # Win Rate Analysis - Horizontal bar chart with risk coloring
        df_sorted = order_df.sort_values('win_rate', ascending=True)
        
        colors = []
        for rate in df_sorted['win_rate']:
            if rate >= 60:
                colors.append('#10b981')  # Green - Good
            elif rate >= 40:
                colors.append('#f59e0b')  # Orange - Medium
            else:
                colors.append('#ef4444')  # Red - Poor
        
        fig = go.Figure()
        fig.add_trace(go.Bar(
            y=df_sorted['order_type'],
            x=df_sorted['win_rate'],
            orientation='h',
            marker_color=colors,
            text=df_sorted['win_rate'].apply(lambda x: f"{x:.1f}%"),
            textposition='outside',
            hovertemplate='<b>%{y}</b><br>Win Rate: %{x:.1f}%<br>Trades: %{customdata}<extra></extra>',
            customdata=df_sorted['trade_count']
        ))
        
        fig.add_vline(
            x=50, 
            line_dash="dash", 
            line_color="gray", 
            annotation_text="50% Benchmark", 
            annotation_position="top"
        )
        
        fig.update_layout(
            title="Win Rate by Product Type",
            xaxis_title="Win Rate (%)",
            yaxis_title="",
            height=300,
            margin=dict(l=100, r=40, t=50, b=40),
            **CHART_BG
        )
        fig.update_xaxes(range=[0, 100])
        
        st.plotly_chart(fig, width='stretch', key="order_winrate")
        
        # Win rate confidence intervals
        st.subheader("ðŸ“Š Statistical Confidence")
        
        for _, row in order_df.iterrows():
            trades = row['trade_count']
            win_rate = row['win_rate'] / 100
            
            # Calculate confidence interval (simplified)
            if trades > 0:
                std_error = np.sqrt(win_rate * (1 - win_rate) / trades)
                ci_lower = max(0, (win_rate - 1.96 * std_error) * 100)
                ci_upper = min(100, (win_rate + 1.96 * std_error) * 100)
                
                st.markdown(f"""
                <div style='background:rgba(30,41,59,0.4); padding:10px; border-radius:8px; margin-bottom:8px;'>
                    <div style='display:flex; justify-content:space-between;'>
                        <span style='color:#94a3b8;'><b>{row['order_type'].upper()}</b></span>
                        <span style='color:#f1f5f9;'>{row['trade_count']} trades</span>
                    </div>
                    <div style='margin-top:5px;'>
                        <div style='background:#1e293b; height:20px; border-radius:10px; position:relative;'>
                            <div style='background:#6366f1; width:{win_rate*100}%; height:20px; border-radius:10px;'></div>
                        </div>
                        <div style='display:flex; justify-content:space-between; margin-top:3px;'>
                            <span style='color:#94a3b8;'>95% CI: {ci_lower:.1f}% - {ci_upper:.1f}%</span>
                        </div>
                    </div>
                </div>
                """, unsafe_allow_html=True)
    
    with tab3:
        # PnL Breakdown - Dual axis chart
        fig = make_subplots(specs=[[{"secondary_y": True}]])
        
        # Bar chart for total PnL
        fig.add_trace(
            go.Bar(
                x=order_df['order_type'],
                y=order_df['total_pnl'],
                name='Total PnL',
                marker_color='#6366f1',
                text=order_df['total_pnl'].apply(lambda x: f"${x:,.0f}"),
                textposition='outside',
            ),
            secondary_y=False,
        )
        
        # Line chart for avg PnL
        colors = ['#10b981' if x > 0 else '#ef4444' for x in order_df['avg_pnl']]
        
        fig.add_trace(
            go.Scatter(
                x=order_df['order_type'],
                y=order_df['avg_pnl'],
                name='Avg PnL',
                mode='lines+markers',
                line=dict(color='#f1f5f9', width=3),
                marker=dict(size=12, color=colors),
                text=order_df['avg_pnl'].apply(lambda x: f"${x:,.0f}"),
                textposition='top center',
            ),
            secondary_y=True,
        )
        
        fig.update_layout(
            title="PnL Analysis by Product Type",
            xaxis_title="Product Type",
            hovermode='x unified',
            height=400,
            **CHART_BG,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
        )
        
        fig.update_yaxes(title_text="Total PnL ($)", secondary_y=False)
        fig.update_yaxes(title_text="Average PnL ($)", secondary_y=True)
        fig.add_hline(y=0, line_dash="dash", line_color="gray", opacity=0.5, secondary_y=True)
        
        st.plotly_chart(fig, width='stretch', key="order_pnl")
        
        # Fee analysis
        if 'total_fees' in order_df.columns and 'total_volume' in order_df.columns:
            st.subheader("ðŸ’° Fee Efficiency")
            
            fig = px.bar(
                order_df,
                x='order_type',
                y='fee_ratio',
                title='Fee Ratio by Product Type (% of Volume)',
                color='fee_ratio',
                color_continuous_scale='Reds',
                text=order_df['fee_ratio'].apply(lambda x: f"{x:.2f}%"),
                labels={'order_type': 'Product Type', 'fee_ratio': 'Fee Ratio (%)'}
            )
            fig.update_traces(textposition='outside')
            fig.update_layout(height=300, **CHART_BG)
            st.plotly_chart(fig, width='stretch', key="order_fees")
            
            st.caption("ðŸ’° Lower fee ratio means more cost-efficient trading")
    
    with tab4:
        # Detailed table
        st.subheader("ðŸ“‹ Detailed Statistics")
        
        display_df = order_df.copy()
        display_df['win_rate'] = display_df['win_rate'].apply(lambda x: f"{x:.1f}%")
        display_df['avg_pnl'] = display_df['avg_pnl'].apply(lambda x: f"${x:,.2f}")
        display_df['total_pnl'] = display_df['total_pnl'].apply(lambda x: f"${x:,.2f}")
        display_df['total_fees'] = display_df['total_fees'].apply(lambda x: f"${x:,.2f}")
        display_df['total_volume'] = display_df['total_volume'].apply(lambda x: f"${x:,.0f}")
        display_df['fee_ratio'] = display_df['fee_ratio'].apply(lambda x: f"{x:.2f}%")
        
        column_order = ['order_type', 'trade_count', 'win_rate', 'avg_pnl', 
                       'total_pnl', 'total_volume', 'total_fees', 'fee_ratio']
        
        st.dataframe(
            display_df[column_order], 
            width='stretch', 
            hide_index=True,
            column_config={
                "order_type": "Product Type",
                "trade_count": "Trades",
                "win_rate": "Win Rate",
                "avg_pnl": "Avg PnL",
                "total_pnl": "Total PnL",
                "total_volume": "Volume",
                "total_fees": "Fees",
                "fee_ratio": "Fee %"
            }
        )
        
        csv = order_df.to_csv(index=False)
        st.download_button(
            "ðŸ“¥ Download Order Data",
            csv,
            f"order_analysis_{datetime.now().strftime('%Y%m%d')}.csv",
            "text/csv"
        )
        
# ============================================================================
# GREEKS ANALYSIS
# ============================================================================

def display_greeks_analysis(greeks_df, positions_df, is_personal=False):
    """Greeks analysis with symbol filter adaptation."""
    
    st.header("ðŸ”¬ Options Greeks Exposure")
    
    # Get symbol filter state from session
    has_symbol_filter = False
    if 'selected_symbols' in st.session_state:
        has_symbol_filter = len(st.session_state.selected_symbols) > 0
    
    # If we have positions data, recalculate Greeks based on filtered positions
    if positions_df is not None and not positions_df.empty:
        # Filter to only option positions
        option_positions = positions_df[positions_df['product_type'] == 'option'].copy()
        
        if option_positions.empty:
            st.info("No options positions match the current filters")
            return
        
        # Calculate per-position Greeks using the helper function
        per_position_greeks = compute_greeks_per_position(option_positions)
        
        if per_position_greeks.empty:
            st.info("No Greeks data available for filtered options")
            return
        
        # Aggregate by trader
        trader_greeks = per_position_greeks.groupby('trader_id').agg({
            'delta': 'sum'
        }).reset_index()
        trader_greeks.columns = ['trader_id', 'net_delta']
        trader_greeks['total_option_positions'] = per_position_greeks.groupby('trader_id').size().values
        
        # Use this filtered data instead of the pre-calculated greeks_df
        greeks_df = trader_greeks.copy()
    
    # If still no data, show message
    if greeks_df.empty:
        st.info("No options Greeks data available for current filters")
        return
    
    # SPARSE DATA DETECTION
    trade_count = len(positions_df) if positions_df is not None else 0
    is_sparse_mode = False
    sparse_reason = ""
    
    if has_symbol_filter and trade_count < 8:
        is_sparse_mode = True
        symbol_text = f"for selected symbol{'s' if len(st.session_state.selected_symbols) > 1 else ''}"
        sparse_reason = f"Limited options data {symbol_text}"
    elif trade_count < 5:
        is_sparse_mode = True
        sparse_reason = "Very few options trades"
    
    # SPARSE MODE - Show simplified view
    if is_sparse_mode:
        st.info(f"â„¹ï¸ {sparse_reason} - showing per-position breakdown")
        
        # Show per-position details instead of aggregated charts
        if positions_df is not None and not positions_df.empty:
            option_details = positions_df[positions_df['product_type'] == 'option'].copy()
            
            if not option_details.empty:
                option_details['symbol'] = option_details['market_id'].apply(simplify_symbol)
                option_details['delta'] = option_details.apply(
                    lambda row: compute_single_delta(row), axis=1
                )
                
                display_df = option_details[['close_time', 'symbol', 'side', 'size', 'delta', 'realized_pnl']].copy()
                display_df['close_time'] = pd.to_datetime(display_df['close_time']).dt.strftime('%Y-%m-%d %H:%M')
                display_df['realized_pnl'] = display_df['realized_pnl'].apply(lambda x: f"${x:,.2f}")
                
                st.subheader("ðŸ“‹ Individual Option Positions")
                st.dataframe(display_df, width='stretch', hide_index=True)
                
                # Show simple totals
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Options", len(option_details))
                with col2:
                    st.metric("Net Delta", f"{greeks_df['net_delta'].sum():.2f}")
                with col3:
                    total_pnl = option_details['realized_pnl'].sum()
                    st.metric("Options PnL", f"${total_pnl:,.2f}")
                return
    
    # NORMAL MODE - Full Greeks analysis
    total_delta = greeks_df['net_delta'].sum()
    delta_color = "#10b981" if total_delta > 0 else "#ef4444"
    total_pos = greeks_df['total_option_positions'].sum()
    
    # Show symbol filter context
    if has_symbol_filter:
        st.caption(f"ðŸ“Š Showing Greeks for selected symbol{'s' if len(st.session_state.selected_symbols) > 1 else ''}")
    
    # Metrics cards
    c1, c2, c3, c4 = st.columns(4)
    
    with c1:
        st.metric("Net Delta", f"{total_delta:,.2f}", delta=None)
    with c2:
        st.metric("Gamma", "ðŸ”œ Soon", delta=None)
    with c3:
        st.metric("Theta", "ðŸ”œ Soon", delta=None)
    with c4:
        st.metric("Positions", f"{int(total_pos)}", delta=None)
    
    # Prepare display data
    disp = greeks_df.copy()
    disp['trader'] = disp['trader_id'].apply(mask_trader_id)
    disp = disp.sort_values('net_delta', ascending=False)
    
    # Limit to top 5 traders in multi-view
    if not is_personal and len(disp) > 5:
        st.info("Showing top 5 traders by delta exposure")
        disp = disp.head(5)
    
    # Single trader or personal mode
    if is_personal or disp['trader_id'].nunique() == 1:
        st.subheader("ðŸ“Š Per-Position Delta")
        
        # Show per-position breakdown
        if positions_df is not None and not positions_df.empty:
            option_details = positions_df[positions_df['product_type'] == 'option'].copy()
            if not option_details.empty:
                option_details['symbol'] = option_details['market_id'].apply(simplify_symbol)
                option_details['delta'] = option_details.apply(
                    lambda row: compute_single_delta(row), axis=1
                )
                
                display_df = option_details[['close_time', 'symbol', 'side', 'size', 'delta', 'realized_pnl']].copy()
                display_df['close_time'] = pd.to_datetime(display_df['close_time']).dt.strftime('%Y-%m-%d %H:%M')
                display_df['realized_pnl'] = display_df['realized_pnl'].apply(lambda x: f"${x:,.2f}")
                
                st.dataframe(display_df, width='stretch', hide_index=True)
        
        # Delta gauge
        max_range = max(abs(total_delta) * 2, 10) if total_delta != 0 else 10
        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=total_delta,
            title={'text': "Net Delta Exposure"},
            gauge={
                'axis': {'range': [-max_range, max_range]},
                'bar': {'color': "#10b981" if total_delta >= 0 else "#ef4444"},
                'steps': [
                    {'range': [-max_range, 0], 'color': 'rgba(239,68,68,0.1)'},
                    {'range': [0, max_range], 'color': 'rgba(16,185,129,0.1)'}
                ]
            }
        ))
        fig.update_layout(height=280, **CHART_BG)
        st.plotly_chart(fig, width='stretch', key="delta_gauge")
    
    else:
        # Multi-trader view - bar chart
        st.subheader("ðŸ“Š Delta Exposure by Trader")
        
        fig = px.bar(disp, x='trader', y='net_delta', color='net_delta',
                    color_continuous_scale='RdBu', color_continuous_midpoint=0,
                    title='Net Delta by Trader')
        fig.update_layout(height=350, **CHART_BG)
        fig.update_xaxes(tickangle=-45)
        st.plotly_chart(fig, width='stretch', key="delta_bar_multi")
    
    # Full table
    st.subheader("ðŸ“‹ Greeks Breakdown")
    st.dataframe(
        disp[['trader','total_option_positions','net_delta']].style.format(
            {'net_delta':'{:,.2f}','total_option_positions':'{:.0f}'}
        ),
        width='stretch', hide_index=True
    )

def compute_single_delta(row):
    """Compute delta for a single option position."""
    is_call = 'CALL' in str(row['market_id']).upper()
    if row['side'] == 'buy':
        return abs(row['size']) if is_call else -abs(row['size'])
    else:
        return -abs(row['size']) if is_call else abs(row['size'])
    
def compute_greeks_per_position(positions_df):
    """Compute delta for each option position."""
    opts = positions_df[positions_df['product_type'] == 'option'].copy()
    if opts.empty:
        return pd.DataFrame()
    
    rows = []
    for _, pos in opts.iterrows():
        is_call = 'CALL' in str(pos['market_id']).upper()
        if pos['side'] == 'buy':
            delta = abs(pos['size']) if is_call else -abs(pos['size'])
        else:
            delta = -abs(pos['size']) if is_call else abs(pos['size'])
        rows.append({
            'position_id': pos['position_id'],
            'trader_id': pos['trader_id'],
            'delta': delta
        })
    return pd.DataFrame(rows)
    
# ============================================================================
# TRANSACTION HISTORY
# ============================================================================

def display_transaction_history(positions_df):
    """Transaction history with pagination and blockchain verify links ahead of real injection."""
    
    st.markdown("### ðŸ“‹ Transaction History")
    
    # Add info about expired options
    if not positions_df.empty and 'close_reason' in positions_df.columns:
        if (positions_df['close_reason'] == 'expire').any():
            st.info("â„¹ï¸ **Expired Options:** When options expire worthless, exit price = $0, so volume = $0. The PnL shows the premium paid + fees.")
    
    if positions_df.empty:
        st.info("No transactions to display")
        return
           
    df = positions_df.copy()
    
    df['symbol'] = df['market_id'].apply(simplify_symbol)
    df['trader'] = df['trader_id'].apply(mask_trader_id)
    df['volume_usd'] = df['exit_price'] * df['size']
    
    df = df.sort_values('close_time', ascending=False)
    
    page_size = 10
    total_pages = max(1, (len(df) - 1) // page_size + 1)
    page = st.number_input("Page", 1, total_pages, 1, key="tx_page")
    
    start = (page - 1) * page_size
    end = min(page * page_size, len(df))
    ddf = df.iloc[start:end].copy()
    
    ddf['close_time'] = pd.to_datetime(ddf['close_time']).dt.strftime('%Y-%m-%d %H:%M')
    ddf['entry_price'] = ddf['entry_price'].apply(lambda x: f"${x:,.2f}")
    ddf['exit_price'] = ddf['exit_price'].apply(lambda x: f"${x:,.2f}")
    ddf['size'] = ddf['size'].apply(lambda x: f"{x:,.4f}")
    ddf['volume_usd'] = ddf['volume_usd'].apply(lambda x: f"${x:,.0f}")
    ddf['realized_pnl'] = ddf['realized_pnl'].apply(lambda x: f"${x:,.2f}")
    ddf['fees'] = ddf['fees'].apply(lambda x: f"${x:,.2f}")
    
    cols = ['close_time','trader','symbol','product_type','side',
            'entry_price','exit_price','size','volume_usd','realized_pnl','fees','close_reason']
    
    if 'close_tx_hash' in ddf.columns:
        ddf['Verify'] = ddf['close_tx_hash'].apply(
            lambda tx: f'<a href="https://solscan.io/tx/{tx}" target="_blank" class="verify-link">ðŸ”— Verify</a>'
            if pd.notna(tx) and str(tx).strip() else 'â€”'
        )
        cols.append('Verify')
    
    def color_pnl(val):
        if isinstance(val, str):
            if '$-' in val:
                return 'color: #ef4444; font-weight: bold;'
            elif '$' in val and val != '$0.00':
                return 'color: #10b981; font-weight: bold;'
        return ''
    
    html = '<div style="overflow-x: auto; margin: 10px 0;"><table class="tx-table">'
    html += '<thead><tr>'
    for col in cols:
        html += f'<th>{col}</th>'
    html += '</tr></thead><tbody>'
    
    for _, row in ddf[cols].iterrows():
        html += '<tr>'
        for col in cols:
            cell = str(row[col])
            style = color_pnl(cell) if col == 'realized_pnl' else ''
            html += f'<td style="{style}">{cell}</td>'
        html += '</tr>'
    
    html += '</tbody></table></div>'
    
    st.markdown(html, unsafe_allow_html=True)
    st.caption(f"Showing {start+1}â€“{end} of {len(df)} transactions")
    
    csv = positions_df.to_csv(index=False)
    st.download_button(
        "ðŸ“¥ Download CSV", csv,
        f"transactions_{datetime.now().strftime('%Y%m%d')}.csv", "text/csv"
    )

# ============================================================================
# GLOBAL KPIs
# ============================================================================

def compute_ratios(positions_df):
    """Calculate Sharpe and Sortino from actual filtered positions."""
    if positions_df.empty or len(positions_df) < 2:
        return 0, 0
    returns = positions_df['realized_pnl'].values
    mean_r = returns.mean()
    std_r = returns.std()
    sharpe = round(mean_r / std_r, 2) if std_r > 0 else 0
    downside = returns[returns < 0]
    sortino = round(mean_r / downside.std(), 2) if len(downside) > 1 and downside.std() > 0 else 0
    return sharpe, sortino

def display_global_kpis(closed_positions, summary_df, selected_trader=None, is_personal_mode=False):
    """Display global KPIs - ratios computed live from filtered positions."""

    if is_personal_mode and selected_trader:
        trader_positions = closed_positions[closed_positions['trader_id'] == selected_trader]
        total_pnl = trader_positions['realized_pnl'].sum() if not trader_positions.empty else 0
        win_rate = (trader_positions['realized_pnl'] > 0).mean() * 100 if not trader_positions.empty else 0
        trade_count = len(trader_positions)
        sharpe, sortino = compute_ratios(trader_positions)
        mode_label = "YOUR PERFORMANCE"
    else:
        total_pnl = closed_positions['realized_pnl'].sum() if not closed_positions.empty else 0
        win_rate = (closed_positions['realized_pnl'] > 0).mean() * 100 if not closed_positions.empty else 0
        trade_count = len(closed_positions)
        sharpe, sortino = compute_ratios(closed_positions)
        mode_label = "PROTOCOL PERFORMANCE"

    if is_personal_mode:
        st.markdown(f"<div style='text-align:center; color:#10b981; font-size:0.75rem; font-weight:600; margin-bottom:8px;'>ðŸ“Š {mode_label}</div>", unsafe_allow_html=True)

    cols = st.columns(5)
    sharpe_display = f"{sharpe:.2f}" if trade_count > 0 else "N/A"
    sortino_display = f"{sortino:.2f}" if trade_count > 0 else "N/A"

    values = [f"${total_pnl:,.2f}", f"{win_rate:.1f}%", str(trade_count), sharpe_display, sortino_display]
    labels = ["NET PNL", "WIN RATE", "TRADES", "SHARPE", "SORTINO"]

    for col, label, val in zip(cols, labels, values):
        col.markdown(f"""
        <div class='metric-major'>
            <div class='metric-major-label'>{label}</div>
            <div class='metric-major-value'>{val}</div>
        </div>
        """, unsafe_allow_html=True)
        
# ============================================================================
# DATA LOADING
# ============================================================================

@st.cache_data
def load_logo(url):
    """Load Deriverse logo."""
    try:
        r = requests.get(url, timeout=5)
        return r.content if r.status_code == 200 else None
    except Exception:
        return None

@st.cache_data
def load_data():
    """Load all analytics data."""
    try:
        return {
            'equity': pd.read_csv(DATA_DIR / "equity_curve.csv", parse_dates=["timestamp"]),
            'positions': pd.read_csv(DATA_DIR / "positions.csv", parse_dates=["open_time","close_time"]),
            'summary': pd.read_csv(DATA_DIR / "summary_metrics.csv"),
            'fees': pd.read_csv(DATA_DIR / "fees_breakdown.csv"),
            'volume': pd.read_csv(DATA_DIR / "volume_by_market.csv"),
            'pnl_day': pd.read_csv(DATA_DIR / "pnl_by_day.csv", parse_dates=["date"]),
            'pnl_hour': pd.read_csv(DATA_DIR / "pnl_by_hour.csv"),
            'directional': pd.read_csv(DATA_DIR / "directional_bias.csv"),
            'order_perf': pd.read_csv(DATA_DIR / "order_type_performance.csv"),
            'greeks': pd.read_csv(DATA_DIR / "greeks_exposure.csv"),
            'open_positions': pd.read_csv(DATA_DIR / "open_positions.csv", parse_dates=["open_time"])
        }
    except FileNotFoundError as e:
        st.error(f"âŒ Data files not found: {e}")
        return None

# Load data
with st.spinner('ðŸ”„ Loading analytics...'):
    data = load_data()

if data is None or (data['positions'].empty and data['open_positions'].empty):
    st.error("âŒ No analytics data found")
    st.info("ðŸ’¡ Run: `python -m scripts.run_analytics`")
    st.stop()

# ============================================================================
# INITIALIZE ADMIN STATE - HIDDEN FROM REGULAR USERS
# ============================================================================

# Check URL for admin activation
if check_url_for_admin():
    st.session_state.show_admin = True

# Initialize admin states
if "admin_authenticated" not in st.session_state:
    st.session_state.admin_authenticated = False
if "show_admin" not in st.session_state:
    st.session_state.show_admin = False

is_admin = st.session_state.admin_authenticated

# ============================================================================
# SIDEBAR
# ============================================================================

logo_url = ("https://deriverse.gitbook.io/deriverse-v1/~gitbook/image"
            "?url=https%3A%2F%2F3705106568-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com"
            "%2Fo%2Forganizations%252FVbKUpgicSXo9QHWM7uzI%252Fsites%252Fsite_oPxtF%252Ficon%252FNsfAUtLJH778Cn5Dd7zK"
            "%252Ffavicon.ico%3Falt%3Dmedia%26token%3D4099bf73-ccd6-4d9f-8bbb-01cdc664ddb0"
            "&width=32&dpr=3&quality=100&sign=13d31bb2&sv=2")

logo_bytes = load_logo(logo_url)
if logo_bytes:
    st.sidebar.image(logo_bytes, width=60)
else:
    st.sidebar.markdown("### ðŸ”· **Deriverse Analytics**")

st.sidebar.markdown("---")
st.sidebar.success("ðŸ”’ **Secure & Private**\nRead-only â€¢ Local-first")
st.sidebar.markdown("---")

# ============================================================================
# TRADER ACCESS 
# ============================================================================

st.sidebar.header("ðŸ‘¤ Trader Access")

all_traders = sorted(pd.concat([
    data['positions']['trader_id'] if not data['positions'].empty else pd.Series([]),
    data['open_positions']['trader_id'] if not data['open_positions'].empty else pd.Series([])
]).unique())

if "view_mode" not in st.session_state:
    st.session_state.view_mode = "all_traders"

if st.session_state.view_mode == "all_traders":
    st.sidebar.info("ðŸŒ **Mode:** All Traders View")
    
    wallet_input = st.sidebar.text_input("Enter Your Wallet Address",
                                        placeholder="7KNXqvHu2QWvDq8cGPGvKZhFvYnz...")
    
    if st.sidebar.button("ðŸ”‘ Enter Personal Dashboard"):
        if wallet_input and len(wallet_input) > 32:
            if wallet_input in all_traders:
                st.session_state.authenticated_trader = wallet_input
                st.session_state.view_mode = "personal"
                st.rerun()
            else:
                st.sidebar.error("âŒ Wallet not found in trading data")
        else:
            st.sidebar.warning("âš ï¸ Please enter a valid wallet address")

else:
    if "authenticated_trader" in st.session_state:
        st.sidebar.success(f"âœ… **Personal Mode:** {mask_trader_id(st.session_state.authenticated_trader)}")
        if st.sidebar.button("ðŸ‘¥ Return to All Traders View"):
            st.session_state.view_mode = "all_traders"
            st.rerun()

st.sidebar.markdown("---")

# ============================================================================
# FILTERS - Admin features only visible when authenticated
# ============================================================================

st.sidebar.header("ðŸŽ›ï¸ Filters")
st.sidebar.markdown("**ðŸ“… Date Range**")

# Regular users see limited options
if is_admin:
    date_option = st.sidebar.radio("Range",
        ["Last 7 Days", "Last 30 Days", "All Time", "Custom"],
        index=1, horizontal=True, label_visibility="collapsed")
else:
    date_option = st.sidebar.radio("Range",
        ["Last 7 Days", "Last 30 Days"],
        index=1, horizontal=True, label_visibility="collapsed")

from datetime import date

if not data['positions'].empty:
    min_date = data['positions']['close_time'].min().date()
    max_date = data['positions']['close_time'].max().date()
    today = date.today()

    if date_option == "Last 7 Days":
        start_date, end_date = today - timedelta(7), today
    elif date_option == "Last 30 Days":
        start_date, end_date = today - timedelta(30), today
    elif date_option == "All Time" and is_admin:
        start_date, end_date = min_date, max_date
    elif date_option == "Custom" and is_admin:
        sc1, sc2 = st.sidebar.columns(2)
        start_date = sc1.date_input("From", min_date, min_value=min_date, max_value=max_date)
        end_date = sc2.date_input("To", max_date, min_value=min_date, max_value=max_date)
    else:
        start_date, end_date = today - timedelta(30), today

all_markets = sorted(data['positions']['market_id'].unique()) if not data['positions'].empty else []
unique_symbols = sorted(set(simplify_symbol(m) for m in all_markets))
selected_symbols = st.sidebar.multiselect("Symbols", unique_symbols, default=[])
selected_markets = [m for m in all_markets if simplify_symbol(m) in selected_symbols] if selected_symbols else []

st.sidebar.markdown("---")

# ============================================================================
# ADMIN ACCESS - COMPLETELY HIDDEN FROM REGULAR USERS
# ============================================================================

# Initialize admin attempts if not exists
if "admin_attempts" not in st.session_state:
    st.session_state.admin_attempts = 0

# Only show admin section if activated via URL
if st.session_state.show_admin:
    st.sidebar.header("ðŸ” Admin Access")
    
    if not st.session_state.admin_authenticated:
        with st.sidebar.expander("Admin Login", expanded=False):
            st.caption("Internal use only")
            
            # Check for rate limiting
            if st.session_state.admin_attempts >= 5:
                st.error("Too many attempts. Try again in 30 seconds.")
                time.sleep(30)
                st.session_state.admin_attempts = 0  # Reset after wait
            else:
                password = st.text_input("Password", type="password", key="admin_pass_hidden")
                if st.button("Authenticate", key="admin_auth_hidden"):
                    if password == ADMIN_PASSWORD:
                        st.session_state.admin_authenticated = True
                        st.session_state.admin_attempts = 0  # Reset on success
                        st.success("âœ… Admin access granted")
                        st.rerun()
                    else:
                        st.session_state.admin_attempts += 1  # Increment on failure
                        remaining = 5 - st.session_state.admin_attempts
                        st.error(f"âŒ Invalid password ({remaining} attempts remaining)")
    else:
        # Show logout when authenticated
        st.sidebar.success("âœ… Admin Mode Active")
        if st.sidebar.button("ðŸ”“ Logout", key="admin_logout_hidden"):
            st.session_state.admin_authenticated = False
            st.rerun()

# Update is_admin after authentication check
is_admin = st.session_state.admin_authenticated

# ============================================================================
# APPLY FILTERS - Admin debug info completely hidden
# ============================================================================

filtered_positions = data['positions'].copy() if not data['positions'].empty else pd.DataFrame()
filtered_open = data['open_positions'].copy() if not data['open_positions'].empty else pd.DataFrame()

# Admin debug info - COMPLETELY HIDDEN from regular users
if is_admin:
    with st.sidebar.expander("ðŸ“Š Data Debug (Admin)", expanded=False):
        st.write(f"Total positions: {len(filtered_positions)}")
        if not filtered_positions.empty:
            st.write(f"Spot: {len(filtered_positions[filtered_positions['product_type'] == 'spot'])}")
            st.write(f"Perp: {len(filtered_positions[filtered_positions['product_type'] == 'perp'])}")
            st.write(f"Option: {len(filtered_positions[filtered_positions['product_type'] == 'option'])}")
        st.write(f"Open positions: {len(filtered_open)}")
        st.write(f"Date range: {start_date} to {end_date}")

# Trader filter
if st.session_state.view_mode == "personal" and "authenticated_trader" in st.session_state:
    selected_trader = st.session_state.authenticated_trader
    if not filtered_positions.empty:
        filtered_positions = filtered_positions[filtered_positions['trader_id'] == selected_trader]
    if not filtered_open.empty:
        filtered_open = filtered_open[filtered_open['trader_id'] == selected_trader]
else:
    selected_trader = None

# Date filter
if not filtered_positions.empty:
    filtered_positions = filtered_positions[
        (filtered_positions['close_time'].dt.date >= start_date) &
        (filtered_positions['close_time'].dt.date <= end_date)
    ]

# Symbol filter
if selected_markets:
    if not filtered_positions.empty:
        filtered_positions = filtered_positions[filtered_positions['market_id'].isin(selected_markets)]
    if not filtered_open.empty:
        filtered_open = filtered_open[filtered_open['market_id'].isin(selected_markets)]

if not filtered_positions.empty:
    filtered_positions = calculate_volume_usd(filtered_positions)

# ============================================================================
# MAIN DASHBOARD LAYOUT - Using native Streamlit
# ============================================================================

# Header section
col1, col2 = st.columns([1, 5])
with col1:
    if logo_bytes:
        st.image(logo_bytes, width=80)
    else:
        st.markdown("### ðŸ”·")
with col2:
    st.title("Deriverse Trading Analytics")
    st.caption("Real-time performance insights â€¢ Local-first security")

if st.session_state.view_mode == "personal" and "authenticated_trader" in st.session_state:
    st.markdown(f"<div class='profile-badge'>ðŸ” {mask_trader_id(st.session_state.authenticated_trader)}</div>", 
                unsafe_allow_html=True)

# Global KPIs

# Global KPIs - MUST use filtered_positions, not closed_positions

is_personal = (st.session_state.view_mode == "personal" and "authenticated_trader" in st.session_state)
display_global_kpis(
    filtered_positions, 
    data['summary'], 
    selected_trader,
    is_personal_mode=is_personal
)

# Navigation tabs

tab_overview, tab_performance, tab_time, tab_risk, tab_volume, tab_orders, tab_greeks, tab_journal = st.tabs([
    "ðŸ“Š Overview", "ðŸ“ˆ Performance", "ðŸ“… Time Analysis", "âš ï¸ Risk", "ðŸ“Š Volume", "ðŸ“‹ Orders", "ðŸ”¬ Greeks", "ðŸ“ Journal"
])

# ============================================================================
# OVERVIEW TAB
# ============================================================================

with tab_overview:
    if not filtered_positions.empty and not selected_trader:
        st.markdown("## ðŸ† Top Performers Analysis")
        c1, c2 = st.columns(2)
        
        with c1:
            st.markdown("### ðŸ“ˆ Top 5 Profitable Traders")
            top_winners = get_top_traders(filtered_positions, n=5, by='profit')
            
            if top_winners:
                rows = []
                for t in top_winners:
                    tp = filtered_positions[filtered_positions['trader_id'] == t]
                    rows.append({
                        'Trader': mask_trader_id(t),
                        'Total PnL': tp['realized_pnl'].sum(),
                        'Trades': len(tp),
                        'Win Rate': (tp['realized_pnl'] > 0).mean() * 100
                    })
                
                df2 = pd.DataFrame(rows)
                fig = px.bar(df2, x='Trader', y='Total PnL',
                            color='Total PnL', color_continuous_scale='Greens',
                            text='Total PnL')
                fig.update_traces(texttemplate='$%{text:.0f}', textposition='outside')
                fig.update_layout(height=200, showlegend=False, **CHART_BG)
                st.plotly_chart(fig, width='stretch', key="top_profit")
                
                st.dataframe(
                    df2.style.format({'Total PnL':'${:,.2f}','Win Rate':'{:.1f}%'}),
                    width='stretch', hide_index=True
                )
        
        with c2:
            st.markdown("### ðŸ“‰ Top 5 Loss-Making Traders")
            top_losers = get_top_traders(filtered_positions, n=5, by='loss')
            
            if top_losers:
                rows = []
                for t in top_losers:
                    tp = filtered_positions[filtered_positions['trader_id'] == t]
                    rows.append({
                        'Trader': mask_trader_id(t),
                        'Total PnL': tp['realized_pnl'].sum(),
                        'Trades': len(tp),
                        'Win Rate': (tp['realized_pnl'] > 0).mean() * 100
                    })
                
                df2 = pd.DataFrame(rows)
                fig = px.bar(df2, x='Trader', y='Total PnL',
                            color='Total PnL', color_continuous_scale='Reds_r',
                            text='Total PnL')
                fig.update_traces(texttemplate='$%{text:.0f}', textposition='outside')
                fig.update_layout(height=200, showlegend=False, **CHART_BG)
                st.plotly_chart(fig, width='stretch', key="top_loss")
                
                st.dataframe(
                    df2.style.format({'Total PnL':'${:,.2f}','Win Rate':'{:.1f}%'}),
                    width='stretch', hide_index=True
                )
        
        st.markdown("---")
    
    display_transaction_history(filtered_positions)
    
    if not filtered_open.empty:
        st.markdown("### ðŸ“Š Open Positions")
        st.warning(f"âš ï¸ **{len(filtered_open)} Open Positions** - Unrealized PnL not included")
        
        od = filtered_open.copy()
        od['symbol'] = od['market_id'].apply(simplify_symbol)
        od['trader'] = od['trader_id'].apply(mask_trader_id)
        
        st.dataframe(
            od[['trader','symbol','product_type','side','entry_price','size']],
            width='stretch', hide_index=True
        )

# ============================================================================
# PERFORMANCE TAB
# ============================================================================

# ============================================================================
# PERFORMANCE TAB - FIXED logic
# ============================================================================

with tab_performance:
    if filtered_positions.empty:
        st.info("ðŸ“ˆ No performance data available for the selected filters")
        st.caption("Try expanding your date range or selecting different symbols")
    else:
        # Determine if we should use sparse mode - now includes symbol filter
        days_selected = (end_date - start_date).days
        trade_count = len(filtered_positions)
        has_symbol_filter = len(selected_symbols) > 0
        
        is_sparse_mode = False
        
        # Check various conditions that indicate sparse data:
        # 1. Very short date range (< 7 days) with few trades
        # 2. Any date range but very few trades (< 5)
        # 3. Symbol filter applied AND few trades for that symbol
        # 4. Personal mode with few trades
        
        if trade_count < 5:
            is_sparse_mode = True
            context_note("Very few trades in selected period - showing compact view with trade details")
        elif days_selected <= 7 and trade_count < 10:
            is_sparse_mode = True
            context_note("Limited data for selected period - showing compact charts + trade cards")
        elif has_symbol_filter and trade_count < 8:
            is_sparse_mode = True
            context_note(f"Limited data for selected symbol{'s' if len(selected_symbols)>1 else ''} - showing compact view")
        
        if st.session_state.view_mode == "personal" and selected_trader:
            # Personal mode - show equity + drawdown
            fig, fig_dd = create_personal_equity_chart(filtered_positions, is_sparse_mode, compact=is_sparse_mode)
            
            if is_sparse_mode and fig_dd is not None:
                # Side-by-side charts for sparse data
                col_left, col_right = st.columns(2)
                with col_left:
                    st.plotly_chart(fig, width='stretch', key="personal_eq")
                with col_right:
                    st.plotly_chart(fig_dd, width='stretch', key="personal_dd")
                st.caption("Equity (left) and Drawdown (right) - compact view")
            else:
                # Stacked charts for normal data
                st.plotly_chart(fig, width='stretch', key="personal_eq")
                if fig_dd is not None:
                    st.plotly_chart(fig_dd, width='stretch', key="personal_dd")
                    st.caption("Drawdown from peak equity")
            
            # Show performance cards for sparse mode
            if is_sparse_mode:
                display_performance_cards(filtered_positions, "Your Performance Details")
        else:
            # Protocol mode - standard equity + drawdown
            fig_eq, fig_dd = create_protocol_equity_charts(filtered_positions, compact=is_sparse_mode)
            
            if is_sparse_mode:
                # Side-by-side charts for sparse protocol data
                col_left, col_right = st.columns(2)
                with col_left:
                    st.plotly_chart(fig_eq, width='stretch', key="proto_eq")
                with col_right:
                    st.plotly_chart(fig_dd, width='stretch', key="proto_dd")
                st.caption("Protocol PnL (left) and Drawdown (right) - compact view")
            else:
                # Stacked charts for normal data
                st.plotly_chart(fig_eq, width='stretch', key="proto_eq")
                st.caption("Protocol cumulative PnL")
                st.plotly_chart(fig_dd, width='stretch', key="proto_dd")
                st.caption("Drawdown from peak equity")
            
            # Show performance cards for sparse protocol data
            if is_sparse_mode:
                display_performance_cards(filtered_positions, "Protocol Performance Details")
        
        # Trader summary table (only for all traders mode with enough data)
        if not selected_trader and not data['equity'].empty and not is_sparse_mode:
            create_trader_summary_table(data['equity'], filtered_positions)
                                    
# ============================================================================
# TIME ANALYSIS TAB 
# ============================================================================

with tab_time:
    if filtered_positions.empty:
        st.info("ðŸ“… No time-based data available for the selected filters")
        st.caption("Try expanding your date range or selecting different symbols")
    else:
        # Determine if we should use sparse mode - now includes symbol filter
        days_selected = (end_date - start_date).days
        trade_count = len(filtered_positions)
        has_symbol_filter = len(selected_symbols) > 0
        
        is_sparse_mode = False
        
        if trade_count < 5:
            is_sparse_mode = True
            context_note("Very few trades - showing trade timeline instead of daily/hourly charts")
        elif days_selected <= 7 and trade_count < 10:
            is_sparse_mode = True
            context_note("Limited time data - showing trade timeline instead of daily/hourly charts")
        elif has_symbol_filter and trade_count < 8:
            is_sparse_mode = True
            context_note(f"Limited data for selected symbol{'s' if len(selected_symbols)>1 else ''} - showing individual trades")
        
        if is_sparse_mode:
            display_trade_summary_cards(filtered_positions, "Trade Timeline")
        else:
            # Filter the daily/hourly data by date range
            trader_pnl_day = None
            trader_pnl_hour = None
            
            if st.session_state.view_mode == "personal" and selected_trader:
                # Personal mode - filter to trader AND date range
                trader_positions = filtered_positions.copy()
                
                if data.get('pnl_day') is not None and not data['pnl_day'].empty:
                    day_df = data['pnl_day'].copy()
                    if 'trader_id' in day_df.columns:
                        day_df = day_df[day_df['trader_id'] == selected_trader]
                    if 'date' in day_df.columns:
                        day_df['date'] = pd.to_datetime(day_df['date'])
                        trader_pnl_day = day_df[
                            (day_df['date'].dt.date >= start_date) & 
                            (day_df['date'].dt.date <= end_date)
                        ].copy()
                
                if data.get('pnl_hour') is not None and not data['pnl_hour'].empty:
                    hour_df = data['pnl_hour'].copy()
                    if 'trader_id' in hour_df.columns:
                        trader_pnl_hour = hour_df[hour_df['trader_id'] == selected_trader].copy()
                
                display_time_performance(
                    trader_positions,
                    trader_pnl_day,
                    trader_pnl_hour
                )
            else:
                # Protocol view - filter by date range only
                pnl_day_filtered = None
                if data.get('pnl_day') is not None and not data['pnl_day'].empty:
                    day_df = data['pnl_day'].copy()
                    if 'date' in day_df.columns:
                        day_df['date'] = pd.to_datetime(day_df['date'])
                        pnl_day_filtered = day_df[
                            (day_df['date'].dt.date >= start_date) & 
                            (day_df['date'].dt.date <= end_date)
                        ].copy()
                
                display_time_performance(
                    filtered_positions,
                    pnl_day_filtered,
                    data.get('pnl_hour')
                )

# ============================================================================
# RISK TAB
# ============================================================================

# ============================================================================
# RISK TAB - Already has check, but ensure it's there
# ============================================================================

with tab_risk:
    if filtered_positions.empty:
        st.info("âš ï¸ No risk data available for the selected filters")
        st.caption("Try expanding your date range or selecting different symbols")
    else:
        display_liquidation_analytics(
            filtered_positions,
            is_personal_mode=(st.session_state.view_mode == "personal"),
            trader_id=selected_trader
        )
# ============================================================================
# VOLUME TAB
# ============================================================================

with tab_volume:
    if filtered_positions.empty:
        st.info("ðŸ“Š No volume data available for the selected filters")
        st.caption("Try expanding your date range or selecting different symbols")
    else:
        display_volume_analysis(filtered_positions)
        
# ============================================================================
# ORDERS TAB
# ============================================================================

with tab_orders:
    # Check if we have any trades in the filtered period
    if filtered_positions.empty:
        st.info("ðŸ“Š No order data available for the selected filters")
        st.caption("Try expanding your date range or selecting different symbols")
    else:
        display_order_type_performance(data['order_perf'], filtered_positions)

# ============================================================================
# GREEKS TAB
# ============================================================================

with tab_greeks:
    # Check if we have any options in the filtered period
    has_options = False
    if not filtered_positions.empty:
        has_options = (filtered_positions['product_type'] == 'option').any()
    
    if filtered_positions.empty or not has_options:
        st.info("ðŸ“Š No options data available for the selected filters")
        st.caption("Try expanding your date range or selecting different symbols")
    else:
        # Pass both the pre-calculated Greeks and the filtered positions
        display_greeks_analysis(
            data['greeks'].copy() if not data['greeks'].empty else pd.DataFrame(),
            filtered_positions,
            is_personal=(selected_trader is not None)
        )        
# ============================================================================
# JOURNAL TAB 
# ============================================================================

with tab_journal:
    st.header("ðŸ“ Trade Journal with Annotations")
    
    if filtered_positions.empty:
        st.info("No trades to journal")
    
    elif st.session_state.view_mode == "personal" and selected_trader:
        trader = selected_trader
        
        st.info("ðŸ“Œ Type your notes and press Enter to save. Changes are saved automatically.")
        
        # Load notes ONLY for this trader
        trader_notes = load_trader_notes(trader)
        
        if 'journal_last_saved' not in st.session_state:
            st.session_state.journal_last_saved = trader_notes.copy()
        
        # Get ONLY this trader's positions
        jdf = filtered_positions[filtered_positions['trader_id'] == trader].sort_values('close_time', ascending=False).copy()
        
        # If still empty after filtering, show message
        if jdf.empty:
            st.info("No trades found for this trader in the selected date range")
            st.stop()
        
        jdf['symbol'] = jdf['market_id'].apply(simplify_symbol)
        jdf['volume_usd'] = jdf['exit_price'] * jdf['size']
        jdf['notes'] = jdf['position_id'].map(lambda pid: trader_notes.get(str(pid), ""))
        
        # Calculate Greeks for option positions if needed
        option_positions = jdf[jdf['product_type'] == 'option'].copy()
        if not option_positions.empty:
            per_position_greeks = compute_greeks_per_position(option_positions)
            if not per_position_greeks.empty:
                jdf = jdf.merge(per_position_greeks[['position_id', 'delta']], on='position_id', how='left')
        
        # Remove duplicate position_ids BEFORE counting
        jdf_unique = jdf.drop_duplicates(subset=['position_id']).copy()
        
        # Count notes correctly - only count non-empty notes in the unique dataframe
        notes_count = sum(1 for n in jdf_unique['notes'].values if n and str(n).strip() != "")
        total_trades = len(jdf_unique)
                
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PAGINATION FOR PERSONAL JOURNAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        page_size = 10  
        total_pages = max(1, (len(jdf_unique) - 1) // page_size + 1)
        
        # Pagination controls
        col_p1, col_p2, col_p3 = st.columns([2, 1, 2])
        with col_p2:
            journal_page = st.number_input(
                "Page", 
                min_value=1, 
                max_value=total_pages, 
                value=1, 
                key="journal_personal_page"
            )
        
        # Slice data for current page from the unique dataframe
        start_idx = (journal_page - 1) * page_size
        end_idx = min(journal_page * page_size, len(jdf_unique))
        jdf_page = jdf_unique.iloc[start_idx:end_idx].copy()
        
        st.caption(f"Showing trades {start_idx + 1}â€“{end_idx} of {len(jdf_unique)}")
        
                
        avail_cols = ['close_time','symbol','product_type','side',
                      'entry_price','exit_price','size','volume_usd','realized_pnl','fees']

        if 'delta' in jdf_page.columns and (jdf_page['product_type'] == 'option').any():
            avail_cols.append('delta')
            
        avail_cols.append('notes')

        col_cfg = {
            "close_time": st.column_config.DatetimeColumn("Closed At", format="DD/MM/YYYY HH:mm"),
            "symbol": "Symbol",
            "product_type": "Type",
            "side": "Direction",
            "entry_price": st.column_config.NumberColumn("Entry", format="$%.2f"),
            "exit_price": st.column_config.NumberColumn("Exit", format="$%.2f"),
            "size": st.column_config.NumberColumn("Size", format="%.4f"),
            "volume_usd": st.column_config.NumberColumn("Volume", format="$%.0f"),
            "realized_pnl": st.column_config.NumberColumn("PnL", format="$%.2f"),
            "fees": st.column_config.NumberColumn("Fees", format="$%.2f"),
            "notes": st.column_config.TextColumn("ðŸ“ Your Notes", max_chars=500, width="large"),
        }

        if 'delta' in avail_cols:
            col_cfg["delta"] = st.column_config.NumberColumn("Delta", format="%.2f")
        
        # jdf_page is already unique, so no need for drop_duplicates again
        editor_key = f"journal_editor_{selected_trader}_{journal_page}"

        edited = st.data_editor(
            jdf_page[avail_cols],
            column_config=col_cfg,
            width='stretch', 
            hide_index=True, 
            num_rows="fixed",
            disabled=[c for c in avail_cols if c != 'notes'],
            key=editor_key
        )
        
        # AUTO-SAVE - Only save notes for this trader
        updated = {}
        has_changes = False
        
        # Use iloc to safely access by position, not by index label
        for position_idx in range(len(edited)):
            # Get the original position_id from jdf_page using iloc
            pid = str(jdf_page.iloc[position_idx]['position_id'])
            note = str(edited.iloc[position_idx]['notes']).strip() if 'notes' in edited.columns else ""
            
            if note:
                updated[pid] = note
            
            old_note = st.session_state.journal_last_saved.get(pid, "")
            if note != old_note:
                has_changes = True
        
        if has_changes:
            # Save ONLY this trader's notes
            all_notes = load_trader_notes(trader)
            all_notes.update(updated)
            save_trader_notes(trader, all_notes)
            st.session_state.journal_last_saved = all_notes.copy()
            st.success("âœ… Notes saved automatically!")
            st.rerun()
        
        # Action buttons
        col1, col2, col3 = st.columns([3, 1, 1])
        with col2:
            csv_data = jdf_unique[avail_cols].copy()
            csv_data['close_time'] = pd.to_datetime(csv_data['close_time']).dt.strftime('%Y-%m-%d %H:%M:%S')
            st.download_button(
                "ðŸ“¥ Export All",
                csv_data.to_csv(index=False),
                f"journal_{mask_trader_id(trader)}.csv",
                "text/csv"
            )
        with col3:
            if st.button("ðŸ—‘ï¸ Clear All Notes"):
                save_trader_notes(trader, {})
                st.session_state.journal_last_saved = {}
                st.success("ðŸ—‘ï¸ All notes cleared")
                st.rerun()
    
    else:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ALL TRADERS VIEW WITH PAGINATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if selected_trader:
            st.info(f"ðŸ‘ï¸ Read-Only View: {mask_trader_id(selected_trader)}")
        else:
            st.info("ðŸ“– Viewing all traders' annotated trades")
        
        # Load notes from ALL traders
        all_notes = {}
        notes_dir = Path("data/trader_notes")
        if notes_dir.exists():
            for notes_file in notes_dir.glob("*.json"):
                trader_id = notes_file.stem
                try:
                    with open(notes_file, 'r') as f:
                        trader_notes_data = json.load(f)
                        for pos_id, note in trader_notes_data.items():
                            if note and str(note).strip():
                                all_notes[pos_id] = {'trader_id': trader_id, 'note': note}
                except Exception:
                    continue
        
        jdf = filtered_positions.sort_values('close_time', ascending=False).copy()
        jdf['trader'] = jdf['trader_id'].apply(mask_trader_id)
        jdf['symbol'] = jdf['market_id'].apply(simplify_symbol)
        jdf['volume_usd'] = jdf['exit_price'] * jdf['size']
        jdf['notes'] = jdf['position_id'].map(
            lambda pid: all_notes.get(str(pid), {}).get('note', '')
        )
        
        jdf_unique = jdf.drop_duplicates(subset=['position_id']).copy()
        
        annotated_count = sum(1 for n in jdf_unique['notes'].values if n and str(n).strip() != "")
        total_count = len(jdf_unique)
        
        if annotated_count > 0:
            st.success(f"ðŸ“ **{annotated_count}** of **{total_count}** trades have annotations ({annotated_count/total_count*100:.1f}%)")
        else:
            st.info("ðŸ“ No trades have been annotated yet")
        
        show_all = st.checkbox("Show all trades", value=True, key="show_all_trades")
        
        if not show_all:
            jdf_unique = jdf_unique[jdf_unique['notes'].str.strip() != '']
            if jdf_unique.empty:
                st.info("No annotated trades to display")
                st.stop()
            st.caption(f"Showing {len(jdf_unique)} annotated trades")
        
        # PAGINATION
        page_size = 10
        total_pages = max(1, (len(jdf_unique) - 1) // page_size + 1)
        
        col_p1, col_p2, col_p3 = st.columns([2, 1, 2])
        with col_p2:
            all_journal_page = st.number_input(
                "Page", 
                min_value=1, 
                max_value=total_pages, 
                value=1, 
                key="journal_all_page"
            )
        
        start_idx = (all_journal_page - 1) * page_size
        end_idx = min(all_journal_page * page_size, len(jdf_unique))
        jdf_page = jdf_unique.iloc[start_idx:end_idx].copy()
        
        st.caption(f"Showing trades {start_idx + 1}â€“{end_idx} of {len(jdf_unique)}")
        
        display_cols = ['close_time','trader','symbol','product_type','side',
                       'entry_price','exit_price','size','volume_usd','realized_pnl','fees','notes']
        
        display_df = jdf_page[display_cols].copy()
        display_df['close_time'] = pd.to_datetime(display_df['close_time']).dt.strftime('%Y-%m-%d %H:%M')
        
        st.dataframe(
            display_df.style.format({
                'entry_price': '${:,.2f}', 
                'exit_price': '${:,.2f}',
                'size': '{:,.4f}', 
                'volume_usd': '${:,.0f}',
                'realized_pnl':'${:,.2f}', 
                'fees': '${:,.2f}'
            }).apply(lambda x: ['background-color: rgba(99,102,241,0.1)' if x['notes'] else '' for _ in x], axis=1),
            width='stretch', 
            hide_index=True,
            column_config={
                "notes": st.column_config.TextColumn("ðŸ“ Trader Notes", width="large")
            }
        )
        
        # Download ALL trades 
        csv_data = jdf_unique[display_cols].copy()
        csv_data['close_time'] = pd.to_datetime(csv_data['close_time']).dt.strftime('%Y-%m-%d %H:%M:%S')
        st.download_button(
            "ðŸ“¥ Download All Trades with Notes",
            csv_data.to_csv(index=False),
            f"all_trades_with_notes_{datetime.now().strftime('%Y%m%d')}.csv",
            "text/csv"
        )
                         
# ============================================================================
# FOOTER
# ============================================================================

st.markdown("---")
fc1, fc2, fc3 = st.columns([2, 1, 1])
fc1.caption(f"ðŸ• Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
fc2.caption("ðŸ” **Admin Mode**" if is_admin else "ðŸ”’ **Secure** â€¢ Local-first")
fc3.caption("v8.0 Native Layout")

st.markdown("""
<div style='text-align:center;padding:20px;color:#64748b;font-size:12px;'>
    <strong>Deriverse Analytics Dashboard</strong><br>
    Read-only â€¢ No private keys required â€¢ Data stays on your machine
</div>
""", unsafe_allow_html=True)


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\dashboards\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\scripts\diagnose_data.py =====

# scripts/diagnose_data.py
"""
Data quality diagnostic tool.
Run after analytics to verify all data is properly processed.
"""

import pandas as pd
from pathlib import Path
import json

print("=" * 60)
print("DATA QUALITY DIAGNOSTIC")
print("=" * 60)

# Add at the top of diagnose_data.py, before any other logic
positions_path = Path("data/analytics_output/positions.csv")

if not positions_path.exists():
    print("âš™ï¸  Output missing â€” running full pipeline...\n")
    import subprocess, sys
    subprocess.run([sys.executable, "-m", "scripts.run_ingestion"], check=True)
    subprocess.run([sys.executable, "scripts/run_analytics.py"], check=True)
    print()
    
# Check positions file
positions_path = Path("data/analytics_output/positions.csv")
if positions_path.exists():
    positions = pd.read_csv(positions_path)
    
    print(f"\nðŸ“Š POSITIONS SUMMARY ({len(positions)} total)")
    print("\nâœ… By Product Type:")
    product_counts = positions['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} positions")
    
    print("\nâœ… By Market:")
    market_counts = positions['market_id'].value_counts()
    for market, count in market_counts.items():
        print(f"  {market:25} {count:>3} positions")
    
    print("\nâœ… By Trader:")
    trader_counts = positions['trader_id'].value_counts()
    for trader, count in trader_counts.items():
        print(f"  {trader:10} {count:>3} positions")
    
    print("\nðŸ’° PnL BY PRODUCT TYPE:")
    pnl_by_product = positions.groupby('product_type')['realized_pnl'].sum()
    for product, pnl in pnl_by_product.items():
        print(f"  {product:10} ${pnl:>12,.2f}")
    
    print("\nðŸ” OPTION POSITIONS DETAIL:")
    option_positions = positions[positions['product_type'] == 'option']
    if not option_positions.empty:
        print(f"  Found {len(option_positions)} option positions")
        for _, row in option_positions.iterrows():
            print(f"    â€¢ {row['market_id']:30} {row['trader_id']:10} ${row['realized_pnl']:>10,.2f}")
    else:
        print("  âŒ NO OPTION POSITIONS FOUND")
    
    print("\nðŸ“‹ ALL POSITIONS SUMMARY:")
    # Check which columns exist
    available_cols = ['position_id', 'trader_id', 'market_id', 'product_type', 'side', 'realized_pnl']
    if 'close_reason' in positions.columns:
        available_cols.append('close_reason')
    
    print(positions[available_cols].to_string(index=False))
    
else:
    print("âŒ positions.csv not found")

# Check normalized events
events_path = Path("data/normalized/events.jsonl")
if events_path.exists():
    events = []
    with open(events_path) as f:
        for line in f:
            line = line.strip()
            if line:
                events.append(json.loads(line))
    
    df = pd.DataFrame(events)
    
    print(f"\nðŸ“¥ NORMALIZED EVENTS ({len(df)} total)")
    print("\nâœ… By Event Type:")
    event_counts = df['event_type'].value_counts()
    for event_type, count in event_counts.items():
        print(f"  {event_type:10} {count:>3} events")
    
    print("\nâœ… By Product Type:")
    product_counts = df['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} events")
    
    print("\nðŸŽ¯ OPTION EVENTS BREAKDOWN:")
    option_events = df[df['product_type'] == 'option']
    print(f"  Total option events: {len(option_events)}")
    if not option_events.empty:
        print("\n  By event type:")
        option_event_counts = option_events['event_type'].value_counts()
        for event_type, count in option_event_counts.items():
            print(f"    {event_type:10} {count:>3}")
        
        print("\n  By market:")
        option_market_counts = option_events['market_id'].value_counts()
        for market, count in option_market_counts.items():
            print(f"    {market:30} {count:>3}")
    else:
        print("  âŒ NO OPTION EVENTS")
else:
    print("âŒ events.jsonl not found")

# Check raw mock data
mock_path = Path("configs/mock_data.json")
if mock_path.exists():
    with open(mock_path) as f:
        mock_data = json.load(f)
    
    print(f"\nðŸ“¦ RAW MOCK DATA ({len(mock_data)} events)")
    mock_df = pd.DataFrame(mock_data)
    
    print("\nâœ… By Event Type:")
    event_counts = mock_df['event_type'].value_counts()
    for event_type, count in event_counts.items():
        print(f"  {event_type:10} {count:>3} events")
    
    print("\nâœ… By Product Type:")
    product_counts = mock_df['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} events")
else:
    print("\nâŒ configs/mock_data.json not found")

print("\n" + "=" * 60)

# Check for duplicates
if events_path.exists():
    print("\nðŸ” CHECKING FOR DUPLICATES...")
    event_ids = [e['event_id'] for e in events]
    unique_ids = set(event_ids)
    
    if len(event_ids) != len(unique_ids):
        print(f"  âš ï¸  WARNING: Found {len(event_ids) - len(unique_ids)} duplicate events!")
        print(f"  Total events: {len(event_ids)}, Unique: {len(unique_ids)}")
    else:
        print(f"  âœ… No duplicates found ({len(event_ids)} unique events)")

print("=" * 60)


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\scripts\generate_mock_data.py =====

# scripts/generate_mock_data.py 
"""
Generate curated mock trading data for Deriverse analytics demo.

Spot trades: 10 closed positions across 3 batches
  Batch 1 (original): 1 win, 1 loss
  Batch 2:            3 wins, 1 loss
  Batch 3 (NEW):      3 wins, 1 loss
"""

import json
import hashlib
import base58
from datetime import datetime, timezone, timedelta
from pathlib import Path

OUTPUT_PATH = Path("configs/mock_data.json")
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

now = datetime.now(timezone.utc)
base_date = now - timedelta(days=30)

events = []

WALLETS = {
    "alice":   "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "bob":     "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "charlie": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "diana":   "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "evan":    "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "fiona":   "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "george":  "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "hannah":  "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "ivan":    "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "julia":   "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
}

position_counter = {}


def generate_event_id(event_data, index):
    seed_parts = [
        str(event_data.get('event_type', '')),
        str(event_data.get('timestamp', '') if isinstance(event_data.get('timestamp'), str) else ''),
        str(event_data.get('trader_id', '')),
        str(event_data.get('market_id', '')),
        str(index)
    ]
    return hashlib.sha256("|".join(seed_parts).encode()).hexdigest()


def generate_tx_signature(event_data, index):
    seed_parts = [
        str(event_data.get('event_type', '')),
        str(event_data.get('timestamp', '')),
        str(event_data.get('trader_id', '')),
        str(event_data.get('market_id', '')),
        str(event_data.get('price', '')),
        str(index)
    ]
    seed = "|".join(seed_parts)
    hash_bytes = hashlib.sha256(seed.encode()).digest()
    padded = hash_bytes + bytes(32)
    return base58.b58encode(padded).decode()[:88]


def generate_position_id(trader_id, market_id, timestamp):
    trader_prefix = trader_id[:8]
    timestamp_ms = int(timestamp.timestamp() * 1000)
    return f"{trader_prefix}_{market_id}_{timestamp_ms}"


def emit(event, order_type="market", position_id=None):
    if 'timestamp' in event and isinstance(event['timestamp'], datetime):
        event['timestamp'] = event['timestamp'].isoformat().replace("+00:00", "Z")

    if 'event_id' not in event:
        event['event_id'] = generate_event_id(event, len(events) + 1)

    event['tx_hash'] = generate_tx_signature(event, len(events) + 1)

    if event['event_type'] == 'open':
        timestamp = datetime.fromisoformat(event['timestamp'].replace('Z', '+00:00'))
        new_position_id = generate_position_id(event['trader_id'], event['market_id'], timestamp)
        event['position_id'] = new_position_id
        key = f"{event['trader_id']}:{event['market_id']}"
        if key not in position_counter:
            position_counter[key] = []
        position_counter[key].append({
            'position_id': new_position_id,
            'entry_price': event['price'],
            'timestamp': event['timestamp']
        })

    elif event['event_type'] in ['close', 'liquidation', 'exercise', 'expire']:
        if position_id:
            event['position_id'] = position_id
        else:
            key = f"{event['trader_id']}:{event['market_id']}"
            if key in position_counter and position_counter[key]:
                position_info = position_counter[key][-1]
                event['position_id'] = position_info['position_id']
                if 'entry_price' not in event:
                    event['entry_price'] = position_info['entry_price']
                if event['event_type'] in ['close', 'liquidation', 'expire']:
                    position_counter[key].pop()

    if event['event_type'] in ['open', 'close', 'liquidation']:
        event['order_type'] = order_type

    events.append(event)


# ================================================================================
# SPOT TRADES â€” Batch 1 (original 2)
# ================================================================================

# WIN â€” Alice: SOL/USDC buy@$100 -> sell@$110  (+$99)
emit({
    "event_type": "open", "timestamp": base_date,
    "trader_id": WALLETS["alice"], "market_id": "SOL/USDC",
    "product_type": "spot", "side": "buy", "price": 100, "size": 10, "fee_usd": 0.5
}, order_type="stop")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(hours=2),
    "trader_id": WALLETS["alice"], "market_id": "SOL/USDC",
    "product_type": "spot", "side": "sell", "price": 110, "size": 10, "fee_usd": 0.5
}, order_type="market")

# LOSS â€” Bob: ETH/USDC buy@$2000 -> sell@$1950  (-$252)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=2, minutes=10),
    "trader_id": WALLETS["bob"], "market_id": "ETH/USDC",
    "product_type": "spot", "side": "buy", "price": 2000, "size": 5, "fee_usd": 1.0
}, order_type="market")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=2, hours=3),
    "trader_id": WALLETS["bob"], "market_id": "ETH/USDC",
    "product_type": "spot", "side": "sell", "price": 1950, "size": 5, "fee_usd": 1.0
}, order_type="stop")


# ================================================================================
# SPOT TRADES â€” Batch 2 (3 wins, 1 loss)
# ================================================================================

# WIN â€” George: AVAX/USDC buy@$36 -> sell@$40.20  (+$168)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=3, hours=9),
    "trader_id": WALLETS["george"], "market_id": "AVAX/USDC",
    "product_type": "spot", "side": "buy", "price": 36.0, "size": 40, "fee_usd": 0.72
}, order_type="limit")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=3, hours=14),
    "trader_id": WALLETS["george"], "market_id": "AVAX/USDC",
    "product_type": "spot", "side": "sell", "price": 40.2, "size": 40, "fee_usd": 0.80
}, order_type="market")

# WIN â€” Hannah: BTC/USDC buy@$48500 -> sell@$49125  (+$312.50)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=5, hours=10),
    "trader_id": WALLETS["hannah"], "market_id": "BTC/USDC",
    "product_type": "spot", "side": "buy", "price": 48500, "size": 0.5, "fee_usd": 4.85
}, order_type="market")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=5, hours=13),
    "trader_id": WALLETS["hannah"], "market_id": "BTC/USDC",
    "product_type": "spot", "side": "sell", "price": 49125, "size": 0.5, "fee_usd": 4.91
}, order_type="limit")

# LOSS â€” Fiona: SOL/USDC buy@$108 -> stop@$101  (-$108)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=8, hours=8),
    "trader_id": WALLETS["fiona"], "market_id": "SOL/USDC",
    "product_type": "spot", "side": "buy", "price": 108.0, "size": 15, "fee_usd": 1.08
}, order_type="market")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=8, hours=14),
    "trader_id": WALLETS["fiona"], "market_id": "SOL/USDC",
    "product_type": "spot", "side": "sell", "price": 101.0, "size": 15, "fee_usd": 1.01
}, order_type="stop")

# WIN â€” Ivan: ETH/USDC buy@$2050 -> sell@$2125 (overnight)  (+$225)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=12, hours=18),
    "trader_id": WALLETS["ivan"], "market_id": "ETH/USDC",
    "product_type": "spot", "side": "buy", "price": 2050.0, "size": 3, "fee_usd": 1.54
}, order_type="limit")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=13, hours=9),
    "trader_id": WALLETS["ivan"], "market_id": "ETH/USDC",
    "product_type": "spot", "side": "sell", "price": 2125.0, "size": 3, "fee_usd": 1.59
}, order_type="market")


# ================================================================================
# SPOT TRADES â€” Batch 3 (NEW: 3 wins, 1 loss)
# ================================================================================

# WIN â€” Julia: LINK/USDC buy@$14.20 -> sell@$15.80 (quick scalp)  (+$160)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=15, hours=11),
    "trader_id": WALLETS["julia"], "market_id": "LINK/USDC",
    "product_type": "spot", "side": "buy", "price": 14.20, "size": 100, "fee_usd": 1.42
}, order_type="limit")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=15, hours=15),
    "trader_id": WALLETS["julia"], "market_id": "LINK/USDC",
    "product_type": "spot", "side": "sell", "price": 15.80, "size": 100, "fee_usd": 1.58
}, order_type="market")

# LOSS â€” Diana: MATIC/USDC buy@$0.92 -> stop@$0.81 (gap down)  (-$110)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=17, hours=13),
    "trader_id": WALLETS["diana"], "market_id": "MATIC/USDC",
    "product_type": "spot", "side": "buy", "price": 0.92, "size": 1000, "fee_usd": 0.92
}, order_type="market")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=17, hours=20),
    "trader_id": WALLETS["diana"], "market_id": "MATIC/USDC",
    "product_type": "spot", "side": "sell", "price": 0.81, "size": 1000, "fee_usd": 0.81
}, order_type="stop")

# WIN â€” Charlie: DOT/USDC buy@$7.30 -> sell@$8.10 (news rally)  (+$160)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=20, hours=7),
    "trader_id": WALLETS["charlie"], "market_id": "DOT/USDC",
    "product_type": "spot", "side": "buy", "price": 7.30, "size": 200, "fee_usd": 1.46
}, order_type="limit")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=20, hours=16),
    "trader_id": WALLETS["charlie"], "market_id": "DOT/USDC",
    "product_type": "spot", "side": "sell", "price": 8.10, "size": 200, "fee_usd": 1.62
}, order_type="market")

# WIN â€” Bob: SOL/USDC buy@$118 -> sell@$127 (swing trade)  (+$225)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=24, hours=9),
    "trader_id": WALLETS["bob"], "market_id": "SOL/USDC",
    "product_type": "spot", "side": "buy", "price": 118.0, "size": 25, "fee_usd": 2.95
}, order_type="limit")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=25, hours=14),
    "trader_id": WALLETS["bob"], "market_id": "SOL/USDC",
    "product_type": "spot", "side": "sell", "price": 127.0, "size": 25, "fee_usd": 3.18
}, order_type="market")


# ================================================================================
# PERPETUAL TRADES
# ================================================================================

# WIN â€” Charlie: SOL-PERP long@$100 -> close@$120  (+$199)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=4, minutes=20),
    "trader_id": WALLETS["charlie"], "market_id": "SOL-PERP",
    "product_type": "perp", "side": "long", "price": 100, "size": 10, "fee_usd": 0.5
}, order_type="limit")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=4, hours=4),
    "trader_id": WALLETS["charlie"], "market_id": "SOL-PERP",
    "product_type": "perp", "side": "long", "price": 120, "size": 10, "fee_usd": 0.5
}, order_type="market")

# WIN â€” Diana: BTC-PERP short@$50000 -> cover@$48000  (+$1990)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=6, minutes=30),
    "trader_id": WALLETS["diana"], "market_id": "BTC-PERP",
    "product_type": "perp", "side": "short", "price": 50000, "size": 1, "fee_usd": 5.0
}, order_type="market")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=6, hours=5),
    "trader_id": WALLETS["diana"], "market_id": "BTC-PERP",
    "product_type": "perp", "side": "short", "price": 48000, "size": 1, "fee_usd": 5.0
}, order_type="market")

# LOSS â€” Evan: ETH-PERP long@$2100 -> close@$2050  (-$254)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=7, minutes=40),
    "trader_id": WALLETS["evan"], "market_id": "ETH-PERP",
    "product_type": "perp", "side": "long", "price": 2100, "size": 5, "fee_usd": 2.0
}, order_type="stop")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=7, hours=6),
    "trader_id": WALLETS["evan"], "market_id": "ETH-PERP",
    "product_type": "perp", "side": "long", "price": 2050, "size": 5, "fee_usd": 2.0
}, order_type="market")


# ================================================================================
# LIQUIDATION EVENTS (5 total)
# ================================================================================

# LIQ 1 â€” Diana: SOL-PERP long@$105 -> liq@$88  (-$880)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=9, hours=1),
    "trader_id": WALLETS["diana"], "market_id": "SOL-PERP",
    "product_type": "perp", "side": "long", "price": 105, "size": 50, "fee_usd": 5.0
}, order_type="market")
emit({
    "event_type": "liquidation", "timestamp": base_date + timedelta(days=9, hours=2),
    "trader_id": WALLETS["diana"], "market_id": "SOL-PERP",
    "product_type": "perp", "side": "long", "price": 88, "size": 50, "fee_usd": 25.0
}, order_type="liquidation")

# LIQ 2 â€” Bob: ETH-PERP long@$2200 -> liq@$2050  (-$1520)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=11, hours=3),
    "trader_id": WALLETS["bob"], "market_id": "ETH-PERP",
    "product_type": "perp", "side": "long", "price": 2200, "size": 10, "fee_usd": 3.0
}, order_type="market")
emit({
    "event_type": "liquidation", "timestamp": base_date + timedelta(days=11, hours=8),
    "trader_id": WALLETS["bob"], "market_id": "ETH-PERP",
    "product_type": "perp", "side": "long", "price": 2050, "size": 10, "fee_usd": 20.0
}, order_type="liquidation")

# LIQ 3 â€” Evan: BTC-PERP short@$49000 -> liq@$50250  (-$2530)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=14, hours=2),
    "trader_id": WALLETS["evan"], "market_id": "BTC-PERP",
    "product_type": "perp", "side": "short", "price": 49000, "size": 2, "fee_usd": 8.0
}, order_type="limit")
emit({
    "event_type": "liquidation", "timestamp": base_date + timedelta(days=14, hours=12),
    "trader_id": WALLETS["evan"], "market_id": "BTC-PERP",
    "product_type": "perp", "side": "short", "price": 50250, "size": 2, "fee_usd": 30.0
}, order_type="liquidation")

# LIQ 4 â€” Charlie: AVAX-PERP long@$38.5 -> liq@$35.2  (-$342)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=17, hours=5),
    "trader_id": WALLETS["charlie"], "market_id": "AVAX-PERP",
    "product_type": "perp", "side": "long", "price": 38.5, "size": 100, "fee_usd": 4.0
}, order_type="market")
emit({
    "event_type": "liquidation", "timestamp": base_date + timedelta(days=17, hours=14),
    "trader_id": WALLETS["charlie"], "market_id": "AVAX-PERP",
    "product_type": "perp", "side": "long", "price": 35.2, "size": 100, "fee_usd": 12.0
}, order_type="liquidation")

# LIQ 5 â€” Alice: SOL-PERP short@$115 -> liq@$135  (-$615)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=19, hours=4),
    "trader_id": WALLETS["alice"], "market_id": "SOL-PERP",
    "product_type": "perp", "side": "short", "price": 115, "size": 30, "fee_usd": 3.5
}, order_type="stop")
emit({
    "event_type": "liquidation", "timestamp": base_date + timedelta(days=19, hours=9),
    "trader_id": WALLETS["alice"], "market_id": "SOL-PERP",
    "product_type": "perp", "side": "short", "price": 135, "size": 30, "fee_usd": 18.0
}, order_type="liquidation")


# ================================================================================
# OPTION TRADES â€” Full lifecycle
# ================================================================================

# Fiona: long SOL call, profitable close  (+$29)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=20, hours=1),
    "trader_id": WALLETS["fiona"], "market_id": "SOL-CALL-120-JAN15",
    "product_type": "option", "option_type": "call", "strike": 120,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "buy", "price": 5.0, "size": 10, "fee_usd": 0.5, "delta": 0.65, "implied_vol": 0.45
}, order_type="stop")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=21, hours=0),
    "trader_id": WALLETS["fiona"], "market_id": "SOL-CALL-120-JAN15",
    "product_type": "option", "option_type": "call", "strike": 120,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "sell", "price": 8.0, "size": 10, "fee_usd": 0.5, "delta": 0.85, "implied_vol": 0.50
}, order_type="stop")

# George: short SOL put, profitable buyback  (+$36.10)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=21, hours=1, minutes=30),
    "trader_id": WALLETS["george"], "market_id": "SOL-PUT-90-JAN15",
    "product_type": "option", "option_type": "put", "strike": 90,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "sell", "price": 4.0, "size": 15, "fee_usd": 0.7, "delta": -0.25, "implied_vol": 0.40
}, order_type="stop")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=22, hours=12),
    "trader_id": WALLETS["george"], "market_id": "SOL-PUT-90-JAN15",
    "product_type": "option", "option_type": "put", "strike": 90,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "buy", "price": 1.5, "size": 15, "fee_usd": 0.7, "delta": -0.10, "implied_vol": 0.30
}, order_type="market")

# Hannah: long ETH put, losing close  (-$127)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=22, hours=2),
    "trader_id": WALLETS["hannah"], "market_id": "ETH-PUT-1900-JAN15",
    "product_type": "option", "option_type": "put", "strike": 1900,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "buy", "price": 45.0, "size": 5, "fee_usd": 1.0, "delta": -0.35, "implied_vol": 0.55
}, order_type="stop")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=24, hours=0),
    "trader_id": WALLETS["hannah"], "market_id": "ETH-PUT-1900-JAN15",
    "product_type": "option", "option_type": "put", "strike": 1900,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "sell", "price": 20.0, "size": 5, "fee_usd": 1.0, "delta": -0.15, "implied_vol": 0.40
}, order_type="limit")

# Ivan: long BTC call, exercised ITM  (+$2980)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=24, hours=3),
    "trader_id": WALLETS["ivan"], "market_id": "BTC-CALL-50000-JAN15",
    "product_type": "option", "option_type": "call", "strike": 50000,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "buy", "price": 2000.0, "size": 1, "fee_usd": 10.0
}, order_type="market")
emit({
    "event_type": "exercise", "timestamp": now - timedelta(days=1),
    "trader_id": WALLETS["ivan"], "market_id": "BTC-CALL-50000-JAN15",
    "product_type": "option", "option_type": "call", "strike": 50000,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "exercise", "size": 1, "fee_usd": 10.0, "underlying_price": 55000
})

# Julia: long SOL put, expired worthless  (-$60.20)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=25, hours=4),
    "trader_id": WALLETS["julia"], "market_id": "SOL-PUT-80-JAN15",
    "product_type": "option", "option_type": "put", "strike": 80,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "buy", "price": 3.0, "size": 20, "fee_usd": 0.2
}, order_type="market")
emit({
    "event_type": "expire", "timestamp": now - timedelta(hours=12),
    "trader_id": WALLETS["julia"], "market_id": "SOL-PUT-80-JAN15",
    "product_type": "option", "option_type": "put", "strike": 80,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "expire", "price": 0.0, "size": 20, "fee_usd": 0.0, "underlying_price": 95
})

# Alice: long SOL call, partial close (2 legs)  (+$108)
emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=26, hours=5),
    "trader_id": WALLETS["alice"], "market_id": "SOL-CALL-110-JAN15",
    "product_type": "option", "option_type": "call", "strike": 110,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "buy", "price": 8.0, "size": 20, "fee_usd": 1.0
}, order_type="market")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=27, hours=2),
    "trader_id": WALLETS["alice"], "market_id": "SOL-CALL-110-JAN15",
    "product_type": "option", "option_type": "call", "strike": 110,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "sell", "price": 12.0, "size": 10, "fee_usd": 0.5
}, order_type="market")
emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=28, hours=2),
    "trader_id": WALLETS["alice"], "market_id": "SOL-CALL-110-JAN15",
    "product_type": "option", "option_type": "call", "strike": 110,
    "expiry": (base_date + timedelta(days=38)).isoformat().replace("+00:00", "Z"),
    "side": "sell", "price": 15.0, "size": 10, "fee_usd": 0.5
}, order_type="market")


# ================================================================================
# OPEN POSITIONS
# ================================================================================

emit({
    "event_type": "open", "timestamp": now - timedelta(days=2, hours=7),
    "trader_id": WALLETS["alice"], "market_id": "BTC/USDC",
    "product_type": "spot", "side": "buy", "price": 51000, "size": 0.5, "fee_usd": 5.0
}, order_type="market")

emit({
    "event_type": "open", "timestamp": now - timedelta(days=1, hours=8),
    "trader_id": WALLETS["bob"], "market_id": "AVAX-PERP",
    "product_type": "perp", "side": "long", "price": 35.5, "size": 100, "fee_usd": 1.5
}, order_type="limit")

emit({
    "event_type": "open", "timestamp": now - timedelta(hours=12),
    "trader_id": WALLETS["charlie"], "market_id": "ETH-CALL-2200-FEB13",
    "product_type": "option", "option_type": "call", "strike": 2200,
    "expiry": (now + timedelta(days=15)).isoformat().replace("+00:00", "Z"),
    "side": "buy", "price": 85.0, "size": 3, "fee_usd": 0.5
}, order_type="limit")


# ================================================================================
# EDGE CASES
# ================================================================================

emit({
    "event_type": "open", "timestamp": base_date + timedelta(days=5, minutes=60),
    "trader_id": WALLETS["alice"], "market_id": "SOL/USDC",
    "product_type": "spot", "side": "buy", "price": 105, "size": 5, "fee_usd": 0.3
}, order_type="stop")

emit({
    "event_type": "close", "timestamp": base_date + timedelta(days=10, hours=10),
    "trader_id": "GhostWallet1111111111111111111111111111",
    "market_id": "GHOST-PERP", "product_type": "perp",
    "side": "long", "price": 999, "size": 1, "fee_usd": 0.1
}, order_type="stop")

emit({
    "event_type": "trade", "timestamp": base_date + timedelta(days=3, minutes=15),
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "SOL/USDC", "product_type": "spot",
    "side": "buy", "price": 101, "size": 100, "fee_usd": 1.0
})

emit({
    "event_type": "trade", "timestamp": base_date + timedelta(days=8, minutes=45),
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "ETH-PERP", "product_type": "perp",
    "side": "sell", "price": 2105, "size": 50, "fee_usd": 5.0
})


# ================================================================================
# WRITE OUTPUT
# ================================================================================

with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(events, f, indent=2)

spot_closes = [e for e in events if e.get("product_type") == "spot" and e.get("event_type") == "close"]
print(f"\nSpot closed trades: {len(spot_closes)}")
perp_closes = [e for e in events if e.get("product_type") == "perp" and e.get("event_type") == "close"]
print(f"\nPerp closed trades: {len(perp_closes)}")
option_closes = [e for e in events if e.get("product_type") == "option" and e.get("event_type") == "close"]
print(f"\nOption closed trades: {len(option_closes)}")
print(f"\nGenerated {len(events)} events -> {OUTPUT_PATH}")
print(f"\nAll Data Generated Completely")


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\scripts\run_analytics.py =====

# scripts/run_analytics.py - WITH OPEN POSITIONS SUPPORT
import pandas as pd
from pathlib import Path
import io
import logging
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.analytics.pnl_engine import compute_realized_pnl
from src.analytics.summary import compute_executive_summary
from src.analytics.analytics_builder import AnalyticsBuilder

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

NORMALIZED_EVENTS_PATH = Path("data/normalized/events.jsonl")
ANALYTICS_OUTPUT_DIR = Path("data/analytics_output")


def load_events(path: Path) -> pd.DataFrame:
    """Load events from JSONL file with flexible timestamp parsing."""
    events = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                events.append(pd.read_json(io.StringIO(line), typ="series"))
    
    if not events:
        logger.warning(f"No events found in {path}")
        return pd.DataFrame()
    
    df = pd.DataFrame(events)
    
    try:
        df["timestamp"] = pd.to_datetime(df["timestamp"], format='ISO8601', utc=True)
    except Exception as e:
        logger.warning(f"ISO8601 parsing failed, trying mixed format: {e}")
        df["timestamp"] = pd.to_datetime(df["timestamp"], format='mixed', utc=True)
    
    return df


def run_analytics(events_df, auto_summary=True):
    if events_df.empty:
        logger.error("No events to analyze")
        return None, None, None
    
    logger.info(f"Loaded {len(events_df)} events")

    logger.info("Computing realized PnL (truth engine)")
    positions_df, pnl_df, open_positions_df = compute_realized_pnl(events_df)  # âœ… NOW RETURNS 3 VALUES

    # Build all analytics outputs
    logger.info("Building comprehensive analytics tables...")
    builder = AnalyticsBuilder(positions_df, pnl_df, open_positions_df, ANALYTICS_OUTPUT_DIR)  # âœ… PASS OPEN POSITIONS
    builder.build_all()

    if not positions_df.empty and auto_summary:
        summary = compute_executive_summary(positions_df, pnl_df)
        
        print("\n" + "=" * 50)
        print("EXECUTIVE SUMMARY")
        print("=" * 50)
        print(f"Total Realized PnL:  ${summary['total_pnl']:,.2f}")
        print(f"Total Fees Paid:     ${summary['total_fees']:,.2f}")
        print(f"Total Trades:        {summary['trade_count']}")
        print(f"Win Rate:            {summary['win_rate']:.1%}")
        print(f"Avg Win:             ${summary['avg_win']:,.2f}")
        print(f"Avg Loss:            ${summary['avg_loss']:,.2f}")
        print(f"Best Trade:          ${summary['best_trade']:,.2f}")
        print(f"Worst Trade:         ${summary['worst_trade']:,.2f}")
        print(f"Avg Duration:        {summary['avg_duration']}")
        print(f"Long Ratio:          {summary['long_ratio']:.1%}")
        print(f"Short Ratio:         {summary['short_ratio']:.1%}")
        print(f"Max Drawdown:        ${summary['max_drawdown']:,.2f}")
        
        if 'sharpe_ratio' in summary:
            print(f"Sharpe Ratio:        {summary['sharpe_ratio']:.2f}")
        if 'sortino_ratio' in summary:
            print(f"Sortino Ratio:       {summary['sortino_ratio']:.2f}")
        
        print("=" * 50 + "\n")
    
    # âœ… Show open positions summary
    if not open_positions_df.empty:
        print(f"ðŸ“Š OPEN POSITIONS: {len(open_positions_df)} positions still active")
        for _, pos in open_positions_df.iterrows():
            trader_short = pos['trader_id'][:8] + "..." if len(pos['trader_id']) > 12 else pos['trader_id']
            print(f"  â€¢ {trader_short} | {pos['market_id']:20} | {pos['side']:5} | ${pos['entry_price']:>8,.2f} Ã— {pos['size']:.2f}")

    logger.info("Analytics run complete âœ…")
    return positions_df, pnl_df, open_positions_df


def main():
    logger.info("=" * 60)
    logger.info("Starting Deriverse Analytics Pipeline")
    logger.info("=" * 60)

    if not NORMALIZED_EVENTS_PATH.exists():
        logger.error(f"Normalized events not found at {NORMALIZED_EVENTS_PATH}")
        logger.error("Run 'python -m scripts.generate_mock_data' first")
        return

    events_df = load_events(NORMALIZED_EVENTS_PATH)
    run_analytics(events_df)


if __name__ == "__main__":
    main()


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\scripts\run_ingestion.py =====


# scripts/run_ingestion.py
import logging
from src.ingestion.pipelines import IngestionPipeline
from configs.loader import load_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    logger.info("Starting incremental ingestion")

    config = load_config("configs/ingestion.yaml")
    
    pipeline = IngestionPipeline(
        raw_path=config["raw_data_path"],
        output_path=config["normalized_output_path"],
        checkpoint_path=config["checkpoint_path"],
    )

    count = pipeline.run()
    logger.info(f"Ingested {count} new events")


if __name__ == "__main__":
    main()


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\scripts\validate_analytics.py =====

# scripts/validate_analytics.py
"""
Validation script to verify analytics output quality and correctness.
Run after python -m scripts.run_analytics
"""

import pandas as pd
from pathlib import Path
import sys

OUTPUT_DIR = Path("data/analytics_output")

class bcolors:
    OK = '\033[92m'
    FAIL = '\033[91m'
    WARN = '\033[93m'
    END = '\033[0m'

def validate_file_exists(filename):
    """Check if required file exists."""
    path = OUTPUT_DIR / filename
    if path.exists():
        print(f"{bcolors.OK}âœ“{bcolors.END} {filename} exists")
        return True
    else:
        print(f"{bcolors.FAIL}âœ—{bcolors.END} {filename} missing")
        return False

def validate_positions():
    """Validate positions.csv structure and data quality."""
    df = pd.read_csv(OUTPUT_DIR / "positions.csv")
    
    required_cols = [
        'position_id', 'trader_id', 'market_id', 'product_type', 'side',
        'open_time', 'close_time', 'duration_seconds',
        'entry_price', 'exit_price', 'size', 'gross_pnl', 'fees', 'realized_pnl'
    ]
    
    issues = []
    
    # Check columns
    missing_cols = set(required_cols) - set(df.columns)
    if missing_cols:
        issues.append(f"Missing columns: {missing_cols}")
    
    # Check data quality
    if not df.empty:
        if (df['duration_seconds'] < 0).any():
            issues.append("Negative duration_seconds found")
        
        if (df['fees'] < 0).any():
            issues.append("Negative fees found")
        
        # PnL consistency check
        expected_pnl = df['gross_pnl'] - df['fees']
        if not expected_pnl.equals(df['realized_pnl']):
            max_diff = abs(expected_pnl - df['realized_pnl']).max()
            if max_diff > 0.01:  # Allow for rounding
                issues.append(f"PnL inconsistency detected (max diff: {max_diff:.4f})")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} positions.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} positions.csv is valid ({len(df)} rows)")
        return True

def validate_summary_metrics():
    """Validate summary_metrics.csv calculations."""
    positions = pd.read_csv(OUTPUT_DIR / "positions.csv")
    summary = pd.read_csv(OUTPUT_DIR / "summary_metrics.csv")
    
    issues = []
    
    for _, row in summary.iterrows():
        trader = row['trader_id']
        trader_pos = positions[positions['trader_id'] == trader]
        
        # Validate win rate
        actual_wins = (trader_pos['realized_pnl'] > 0).sum()
        actual_total = len(trader_pos)
        expected_win_rate = actual_wins / actual_total if actual_total > 0 else 0
        
        if abs(row['win_rate'] - expected_win_rate) > 0.01:
            issues.append(f"{trader}: win_rate mismatch ({row['win_rate']:.2f} vs {expected_win_rate:.2f})")
        
        # Validate long/short ratio
        if abs(row['long_ratio'] + row['short_ratio'] - 1.0) > 0.01:
            issues.append(f"{trader}: long_ratio + short_ratio != 1.0")
        
        # Validate total_pnl
        expected_total = trader_pos['realized_pnl'].sum()
        if abs(row['total_pnl'] - expected_total) > 0.01:
            issues.append(f"{trader}: total_pnl mismatch")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} summary_metrics.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} summary_metrics.csv is valid")
        return True

def validate_equity_curve():
    """Validate equity_curve.csv drawdown calculations."""
    df = pd.read_csv(OUTPUT_DIR / "equity_curve.csv")
    
    issues = []
    
    for trader in df['trader_id'].unique():
        trader_data = df[df['trader_id'] == trader].sort_values('timestamp')
        
        # Validate drawdown is always <= 0
        if (trader_data['drawdown'] > 0.01).any():
            issues.append(f"{trader}: Positive drawdown found")
        
        # Validate cumulative PnL is monotonic sum
        if not trader_data['cumulative_pnl'].is_monotonic_increasing and not trader_data['cumulative_pnl'].is_monotonic_decreasing:
            # This is actually OK - cumulative can go up and down
            pass
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} equity_curve.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} equity_curve.csv is valid")
        return True

def validate_directional_bias():
    """Validate directional_bias.csv calculations."""
    df = pd.read_csv(OUTPUT_DIR / "directional_bias.csv")
    
    issues = []
    
    for _, row in df.iterrows():
        total = row['long_trades'] + row['short_trades']
        
        if total == 0:
            issues.append(f"{row['trader_id']}: No trades")
            continue
        
        expected_long_ratio = row['long_trades'] / total
        expected_short_ratio = row['short_trades'] / total
        
        if abs(row['long_ratio'] - expected_long_ratio) > 0.01:
            issues.append(f"{row['trader_id']}: long_ratio calculation error")
        
        if abs(row['short_ratio'] - expected_short_ratio) > 0.01:
            issues.append(f"{row['trader_id']}: short_ratio calculation error")
        
        if abs(row['long_ratio'] + row['short_ratio'] - 1.0) > 0.01:
            issues.append(f"{row['trader_id']}: ratios don't sum to 1.0")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} directional_bias.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} directional_bias.csv is valid")
        return True

def main():
    print("\n" + "=" * 60)
    print("DERIVERSE ANALYTICS VALIDATION")
    print("=" * 60 + "\n")
    
    if not OUTPUT_DIR.exists():
        print(f"{bcolors.FAIL}âœ—{bcolors.END} Output directory not found: {OUTPUT_DIR}")
        print("Run: python -m scripts.run_analytics")
        sys.exit(1)
    
    # Check file existence
    print("Checking required files...")
    required_files = [
        'positions.csv',
        'realized_pnl.csv',
        'equity_curve.csv',
        'summary_metrics.csv',
        'volume_by_market.csv',
        'fees_breakdown.csv',
        'pnl_by_day.csv',
        'pnl_by_hour.csv',
        'directional_bias.csv',
        'order_type_performance.csv'
    ]
    
    all_exist = all(validate_file_exists(f) for f in required_files)
    
    if not all_exist:
        print(f"\n{bcolors.FAIL}âœ— Some files are missing{bcolors.END}")
        sys.exit(1)
    
    print("\nValidating data quality...")
    
    validations = [
        validate_positions(),
        validate_summary_metrics(),
        validate_equity_curve(),
        validate_directional_bias()
    ]
    
    print("\n" + "=" * 60)
    if all(validations):
        print(f"{bcolors.OK}âœ“ ALL VALIDATIONS PASSED{bcolors.END}")
        print("=" * 60 + "\n")
        sys.exit(0)
    else:
        print(f"{bcolors.WARN}âš  SOME VALIDATIONS FAILED{bcolors.END}")
        print("=" * 60 + "\n")
        sys.exit(1)

if __name__ == "__main__":
    main()


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\analytics\trades\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\analytics\analytics_builder.py =====

# src/analytics/analytics_builder.py
"""
Analytics builder for generating comprehensive trading reports.
Builds all required analytics tables from canonical PnL engine outputs.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class AnalyticsBuilder:
    """Build all required analytics tables from canonical PnL engine outputs."""
    
    def __init__(self, positions_df: pd.DataFrame, pnl_df: pd.DataFrame, 
                 open_positions_df: pd.DataFrame, output_dir: Path):
        self.positions = positions_df.copy() if not positions_df.empty else pd.DataFrame()
        self.pnl = pnl_df.copy() if not pnl_df.empty else pd.DataFrame()
        self.open_positions = open_positions_df.copy() if not open_positions_df.empty else pd.DataFrame()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        if not self.positions.empty:
            self.positions['open_time'] = pd.to_datetime(self.positions['open_time'])
            self.positions['close_time'] = pd.to_datetime(self.positions['close_time'])
            self.positions['duration_seconds'] = (
                self.positions['close_time'] - self.positions['open_time']
            ).dt.total_seconds()
    
    def build_all(self):
        """Generate all required analytics outputs."""
        logger.info("Building core truth tables...")
        self._build_positions()
        self._build_realized_pnl()
        self._build_open_positions()
        
        if self.positions.empty:
            logger.warning("No closed positions - generating empty analytics")
            self._generate_empty_outputs()
            return
        
        logger.info("Building performance metrics...")
        self._build_equity_curve()
        self._build_summary_metrics()
        
        logger.info("Building volume & fees analytics...")
        self._build_volume_by_market()
        self._build_fees_breakdown()
        
        logger.info("Building time-based analytics...")
        self._build_pnl_by_day()
        self._build_pnl_by_hour()
        
        logger.info("Building behavioral analytics...")
        self._build_directional_bias()
        self._build_order_type_performance()
        
        logger.info("Building options Greeks...")
        self._build_greeks_exposure()
        
        logger.info(f"âœ… All analytics saved to {self.output_dir}")
    
    def _build_positions(self):
        """Core truth table: positions.csv with transaction tracking."""
        if self.positions.empty:
            pd.DataFrame().to_csv(self.output_dir / 'positions.csv', index=False)
            return
        
        output_cols = [
            'position_id', 'trader_id', 'market_id', 'product_type', 'side',
            'open_time', 'close_time', 'duration_seconds',
            'entry_price', 'exit_price', 'size', 'gross_pnl', 'fees', 'realized_pnl',
            'close_reason'
        ]
        
        if 'open_tx_hash' in self.positions.columns:
            output_cols.insert(output_cols.index('open_time'), 'open_tx_hash')
        
        if 'close_tx_hash' in self.positions.columns:
            output_cols.insert(output_cols.index('close_time'), 'close_tx_hash')
        
        available_cols = [col for col in output_cols if col in self.positions.columns]
        output = self.positions[available_cols].copy()
        output.to_csv(self.output_dir / 'positions.csv', index=False)
    
    def _build_realized_pnl(self):
        """Core truth table: realized_pnl.csv."""
        if self.positions.empty:
            pd.DataFrame().to_csv(self.output_dir / 'realized_pnl.csv', index=False)
            return
            
        output = self.positions[[
            'close_time', 'trader_id', 'market_id', 'realized_pnl', 'fees'
        ]].copy()
        output.rename(columns={'close_time': 'timestamp'}, inplace=True)
        output['net_pnl'] = output['realized_pnl'] - output['fees']
        output = output[['timestamp', 'trader_id', 'market_id', 'realized_pnl', 'fees', 'net_pnl']]
        output.to_csv(self.output_dir / 'realized_pnl.csv', index=False)
    
    def _build_open_positions(self):
        """Active positions table: open_positions.csv."""
        if self.open_positions.empty:
            pd.DataFrame().to_csv(self.output_dir / 'open_positions.csv', index=False)
            logger.info("No open positions")
            return
        
        output = self.open_positions[[
            'position_id', 'trader_id', 'market_id', 'product_type', 'side',
            'entry_price', 'size', 'fees_paid', 'open_time', 'time_held_seconds'
        ]].copy()
        
        output.to_csv(self.output_dir / 'open_positions.csv', index=False)
        logger.info(f"Saved {len(output)} open positions")
    
    def _build_equity_curve(self):
        """Performance table: equity_curve.csv."""
        equity = self.positions.copy()
        equity = equity.sort_values('close_time')
        
        result = []
        for trader in equity['trader_id'].unique():
            trader_data = equity[equity['trader_id'] == trader].copy()
            trader_data['cumulative_pnl'] = trader_data['realized_pnl'].cumsum()
            trader_data['rolling_max'] = trader_data['cumulative_pnl'].cummax()
            trader_data['drawdown'] = trader_data['cumulative_pnl'] - trader_data['rolling_max']
            
            for _, row in trader_data.iterrows():
                result.append({
                    'timestamp': row['close_time'],
                    'trader_id': row['trader_id'],
                    'net_realized_pnl': row['realized_pnl'],
                    'cumulative_pnl': row['cumulative_pnl'],
                    'drawdown': row['drawdown']
                })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'equity_curve.csv', index=False)
    
    def _build_summary_metrics(self):
        """Performance summary: summary_metrics.csv."""
        result = []
        
        for trader in self.positions['trader_id'].unique():
            trader_pos = self.positions[self.positions['trader_id'] == trader]
            
            winning = trader_pos[trader_pos['realized_pnl'] > 0]
            losing = trader_pos[trader_pos['realized_pnl'] < 0]
            
            total_pnl = trader_pos['realized_pnl'].sum()
            total_fees = trader_pos['fees'].sum()
            trade_count = len(trader_pos)
            win_rate = len(winning) / trade_count if trade_count > 0 else 0
            avg_win = winning['realized_pnl'].mean() if len(winning) > 0 else 0
            avg_loss = losing['realized_pnl'].mean() if len(losing) > 0 else 0
            best_trade = trader_pos['realized_pnl'].max()
            worst_trade = trader_pos['realized_pnl'].min()
            avg_duration = trader_pos['duration_seconds'].mean()
            
            long_trades = trader_pos[trader_pos['side'].isin(['long', 'buy'])]
            short_trades = trader_pos[trader_pos['side'].isin(['short', 'sell'])]
            long_ratio = len(long_trades) / trade_count if trade_count > 0 else 0
            short_ratio = len(short_trades) / trade_count if trade_count > 0 else 0
            
            trader_sorted = trader_pos.sort_values('close_time')
            cum_pnl = trader_sorted['realized_pnl'].cumsum()
            rolling_max = cum_pnl.cummax()
            drawdown = cum_pnl - rolling_max
            max_drawdown = drawdown.min()
            
            if len(trader_pos) > 1:
                trader_daily = trader_pos.copy()
                trader_daily['date'] = trader_daily['close_time'].dt.date
                daily_returns = trader_daily.groupby('date')['realized_pnl'].sum()
                
                mean_return = daily_returns.mean()
                std_return = daily_returns.std()
                sharpe_ratio = mean_return / std_return if std_return > 0 else 0
                
                downside_returns = daily_returns[daily_returns < 0]
                downside_std = downside_returns.std() if len(downside_returns) > 0 else std_return
                sortino_ratio = mean_return / downside_std if downside_std > 0 else 0
            else:
                sharpe_ratio = 0
                sortino_ratio = 0
            
            result.append({
                'trader_id': trader,
                'total_pnl': total_pnl,
                'total_fees': total_fees,
                'trade_count': trade_count,
                'win_rate': win_rate,
                'avg_win': avg_win,
                'avg_loss': avg_loss,
                'best_trade': best_trade,
                'worst_trade': worst_trade,
                'avg_duration_seconds': avg_duration,
                'long_ratio': long_ratio,
                'short_ratio': short_ratio,
                'max_drawdown': max_drawdown,
                'sharpe_ratio': sharpe_ratio,
                'sortino_ratio': sortino_ratio
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'summary_metrics.csv', index=False)
    
    def _build_volume_by_market(self):
        """Volume analytics: volume_by_market.csv."""
        result = []
        
        for (market, product), group in self.positions.groupby(['market_id', 'product_type']):
            total_volume = (group['exit_price'] * group['size']).sum()
            trade_count = len(group)
            
            result.append({
                'market_id': market,
                'product_type': product,
                'total_volume': total_volume,
                'trade_count': trade_count
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'volume_by_market.csv', index=False)
    
    def _build_fees_breakdown(self):
        """Fees analytics: fees_breakdown.csv."""
        result = []
        
        for (trader, product), group in self.positions.groupby(['trader_id', 'product_type']):
            total_fees = group['fees'].sum()
            
            result.append({
                'trader_id': trader,
                'product_type': product,
                'total_fees': total_fees
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'fees_breakdown.csv', index=False)
    
    def _build_pnl_by_day(self):
        """Time analytics: pnl_by_day.csv."""
        df = self.positions.copy()
        df['date'] = df['close_time'].dt.date
        
        result = []
        for (date, trader), group in df.groupby(['date', 'trader_id']):
            daily_pnl = group['realized_pnl'].sum()
            
            trader_data = df[df['trader_id'] == trader]
            trader_data = trader_data[trader_data['date'] <= date]
            cumulative_pnl = trader_data['realized_pnl'].sum()
            
            result.append({
                'date': date,
                'trader_id': trader,
                'daily_pnl': daily_pnl,
                'cumulative_pnl': cumulative_pnl
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'pnl_by_day.csv', index=False)
    
    def _build_pnl_by_hour(self):
        """Time analytics: pnl_by_hour.csv."""
        df = self.positions.copy()
        df['hour_of_day'] = df['close_time'].dt.hour
        
        result = []
        for (hour, trader), group in df.groupby(['hour_of_day', 'trader_id']):
            avg_pnl = group['realized_pnl'].mean()
            trade_count = len(group)
            
            result.append({
                'hour_of_day': hour,
                'trader_id': trader,
                'avg_pnl': avg_pnl,
                'trade_count': trade_count
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'pnl_by_hour.csv', index=False)
    
    def _build_directional_bias(self):
        """Behavioral analytics: directional_bias.csv."""
        result = []
        
        for trader, group in self.positions.groupby('trader_id'):
            long_trades = len(group[group['side'].isin(['long', 'buy'])])
            short_trades = len(group[group['side'].isin(['short', 'sell'])])
            total = long_trades + short_trades
            
            result.append({
                'trader_id': trader,
                'long_trades': long_trades,
                'short_trades': short_trades,
                'long_ratio': long_trades / total if total > 0 else 0,
                'short_ratio': short_trades / total if total > 0 else 0
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'directional_bias.csv', index=False)
    
    def _build_order_type_performance(self):
        """Behavioral analytics: order_type_performance.csv."""
        df = self.positions.copy()
        
        df['order_type'] = df['duration_seconds'].apply(lambda x:
            'market' if x < 300 else
            'limit' if x < 3600 else
            'stop'
        )
        
        result = []
        for order_type, group in df.groupby('order_type'):
            trade_count = len(group)
            avg_pnl = group['realized_pnl'].mean()
            win_rate = (group['realized_pnl'] > 0).mean()
            
            result.append({
                'order_type': order_type,
                'trade_count': trade_count,
                'avg_pnl': avg_pnl,
                'win_rate': win_rate
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'order_type_performance.csv', index=False)
    
    def _build_greeks_exposure(self):
        """Options analytics: greeks_exposure.csv."""
        options = self.positions[self.positions['product_type'] == 'option'].copy()
        
        if options.empty:
            pd.DataFrame().to_csv(self.output_dir / 'greeks_exposure.csv', index=False)
            return
        
        result = []
        for trader in options['trader_id'].unique():
            trader_opts = options[options['trader_id'] == trader]
            
            net_delta = 0
            net_gamma = 0
            net_theta = 0
            
            for _, opt in trader_opts.iterrows():
                if opt['side'] in ['buy', 'long']:
                    direction = 1
                else:
                    direction = -1
                
                if 'delta' in opt and pd.notna(opt['delta']):
                    option_delta = opt['delta']
                else:
                    if 'option_type' in opt and pd.notna(opt['option_type']):
                        if opt['option_type'] == 'call':
                            option_delta = 0.5
                        else:
                            option_delta = -0.5
                    else:
                        option_delta = 0.5
                
                position_delta = direction * option_delta * opt['size']
                net_delta += position_delta
                
                if 'gamma' in opt and pd.notna(opt['gamma']):
                    net_gamma += direction * opt['gamma'] * opt['size']
                
                if 'theta' in opt and pd.notna(opt['theta']):
                    net_theta += direction * opt['theta'] * opt['size']
            
            result.append({
                'trader_id': trader,
                'total_option_positions': len(trader_opts),
                'net_delta': round(net_delta, 4),
                'gamma_exposure': round(net_gamma, 4),
                'theta_decay': round(net_theta, 4)
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'greeks_exposure.csv', index=False)
    
    def _generate_empty_outputs(self):
        """Generate empty CSV files when no data available."""
        empty_files = [
            'equity_curve.csv', 'summary_metrics.csv', 'volume_by_market.csv',
            'fees_breakdown.csv', 'pnl_by_day.csv', 'pnl_by_hour.csv',
            'directional_bias.csv', 'order_type_performance.csv', 'greeks_exposure.csv'
        ]
        
        for filename in empty_files:
            pd.DataFrame().to_csv(self.output_dir / filename, index=False)


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\analytics\pnl_engine.py =====

# src/analytics/pnl_engine.py
"""
Canonical PnL engine with full lifecycle support.
Handles position tracking, PnL calculation, and transaction mapping.
"""

import pandas as pd
import hashlib
import logging
from datetime import datetime, timezone

logger = logging.getLogger(__name__)


def compute_realized_pnl(events: pd.DataFrame):
    """
    Canonical PnL engine with full options lifecycle support.
    
    Returns:
        positions_df: Closed positions with realized PnL
        pnl_df: Daily PnL aggregates
        open_positions_df: Currently open positions
    """

    required_cols = {
        "event_type", "timestamp", "trader_id",
        "market_id", "product_type", "side",
        "price", "size"
    }

    missing = required_cols - set(events.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    events = events.sort_values("timestamp")

    open_positions = {}
    closed_positions = []

    stats = {
        "duplicate_opens": 0,
        "close_without_open": 0,
        "oversized_closes": 0,
    }

    for _, event in events.iterrows():

        if event["product_type"] == "perp":
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"],
                event["side"]
            )
        elif event["product_type"] == "option":
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"]
            )
        else:
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"]
            )

        if event["event_type"] == "open":
            if key in open_positions:
                stats["duplicate_opens"] += 1
                continue

            position_id = event.get('position_id')
            if not position_id:
                position_data = (
                    f"{event['trader_id']}|{event['market_id']}|"
                    f"{event['timestamp']}|{event.get('side', '')}"
                )
                position_id = hashlib.sha256(position_data.encode()).hexdigest()[:16]

            fee_value = event.get("fee_usd", event.get("fee", 0))

            open_positions[key] = {
                "position_id": position_id,
                "open_time": event["timestamp"],
                "entry_price": event["price"],
                "size": event["size"],
                "fees": fee_value,
                "trader_id": event["trader_id"],
                "market_id": event["market_id"],
                "product_type": event["product_type"],
                "side": event["side"],
                "open_tx_hash": event.get("tx_hash"),
            }
            
            if event["product_type"] == "option":
                open_positions[key]["option_type"] = event.get("option_type")
                open_positions[key]["strike"] = event.get("strike")
                open_positions[key]["expiry"] = event.get("expiry")

        elif event["event_type"] in {"close", "liquidation", "exercise", "expire"}:
            if key not in open_positions:
                stats["close_without_open"] += 1
                continue

            pos = open_positions[key]
            close_size = event["size"]

            if close_size > pos["size"]:
                stats["oversized_closes"] += 1
                continue

            fee_ratio = close_size / pos["size"]
            allocated_open_fee = pos["fees"] * fee_ratio
            
            close_fee_value = event.get("fee_usd", event.get("fee", 0))
            total_fees = allocated_open_fee + close_fee_value

            if pos["product_type"] == "option":
                gross_pnl = calculate_option_pnl(
                    event_type=event["event_type"],
                    option_type=pos.get("option_type"),
                    side=pos["side"],
                    entry_price=pos["entry_price"],
                    exit_price=event.get("price", 0),
                    strike=pos.get("strike"),
                    underlying_price=event.get("underlying_price"),
                    size=close_size
                )
                net_pnl = gross_pnl - total_fees
                exit_price = event.get("price", 0)

            else:
                exit_price = event["price"]

                if pd.notna(event.get("pnl")):
                    net_pnl = event["pnl"]
                    gross_pnl = net_pnl + total_fees
                else:
                    if pos["side"] in {"long", "buy"}:
                        gross_pnl = (exit_price - pos["entry_price"]) * close_size
                    else:
                        gross_pnl = (pos["entry_price"] - exit_price) * close_size

                    net_pnl = gross_pnl - total_fees

            closed_positions.append({
                "position_id": pos["position_id"],
                "open_time": pos["open_time"],
                "close_time": event["timestamp"],
                "trader_id": pos["trader_id"],
                "market_id": pos["market_id"],
                "product_type": pos["product_type"],
                "side": pos["side"],
                "entry_price": pos["entry_price"],
                "exit_price": exit_price,
                "size": close_size,
                "gross_pnl": round(gross_pnl, 4),
                "net_pnl": round(net_pnl, 4),
                "realized_pnl": round(net_pnl, 4),
                "fees": round(total_fees, 4),
                "close_reason": event["event_type"],
                "open_tx_hash": pos.get("open_tx_hash"),
                "close_tx_hash": event.get("tx_hash"),
            })

            pos["size"] -= close_size
            pos["fees"] -= allocated_open_fee

            if pos["size"] <= 0:
                open_positions.pop(key)

    positions_df = pd.DataFrame(closed_positions)

    logger.info(
        "PnL validation summary | "
        f"duplicate_opens={stats['duplicate_opens']} | "
        f"close_without_open={stats['close_without_open']} | "
        f"oversized_closes={stats['oversized_closes']}"
    )

    if positions_df.empty:
        positions_df = pd.DataFrame()
        pnl_df = pd.DataFrame()
    else:
        pnl_df = (
            positions_df
            .assign(date=lambda df: pd.to_datetime(df["close_time"]).dt.date)
            .groupby(
                ["date", "trader_id", "market_id", "product_type"],
                as_index=False
            )
            .agg(
                net_pnl=("net_pnl", "sum"),
                realized_pnl=("realized_pnl", "sum"),
                fees=("fees", "sum"),
                trade_count=("position_id", "count")
            )
        )

    open_positions_list = []
    for key, pos in open_positions.items():
        now = datetime.now(timezone.utc)
        open_time = pd.to_datetime(pos["open_time"])
        if open_time.tzinfo is None:
            open_time = open_time.tz_localize(timezone.utc)
        
        time_held = (now - open_time).total_seconds()
        
        open_positions_list.append({
            "position_id": pos["position_id"],
            "trader_id": pos["trader_id"],
            "market_id": pos["market_id"],
            "product_type": pos["product_type"],
            "side": pos["side"],
            "entry_price": pos["entry_price"],
            "size": pos["size"],
            "fees_paid": pos["fees"],
            "open_time": pos["open_time"],
            "time_held_seconds": time_held,
            "open_tx_hash": pos.get("open_tx_hash")
        })
    
    open_positions_df = pd.DataFrame(open_positions_list)

    logger.info(
        f"PnL engine results: {len(positions_df)} closed positions, "
        f"{len(open_positions_df)} still open"
    )

    return positions_df, pnl_df, open_positions_df


def calculate_option_pnl(
    event_type: str,
    option_type: str,
    side: str,
    entry_price: float,
    exit_price: float,
    strike: float,
    underlying_price: float,
    size: float
) -> float:
    """Calculate options PnL based on event type."""
    
    if event_type == "close":
        if side == "buy":
            gross_pnl = (exit_price - entry_price) * size
        else:
            gross_pnl = (entry_price - exit_price) * size
        return gross_pnl
    
    elif event_type == "exercise":
        if side == "buy":
            if option_type == "call":
                intrinsic_value = max(0, underlying_price - strike)
            else:
                intrinsic_value = max(0, strike - underlying_price)
            gross_pnl = (intrinsic_value - entry_price) * size
        else:
            if option_type == "call":
                intrinsic_value = max(0, underlying_price - strike)
            else:
                intrinsic_value = max(0, strike - underlying_price)
            gross_pnl = (entry_price - intrinsic_value) * size
        
        return gross_pnl
    
    elif event_type == "expire":
        if side == "buy":
            gross_pnl = -entry_price * size
        else:
            gross_pnl = entry_price * size
        
        return gross_pnl
    
    return 0.0


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\analytics\summary.py =====

# src/analytics/summary.py
import pandas as pd

def compute_executive_summary(positions: pd.DataFrame, pnl: pd.DataFrame) -> dict:
    """
    Compute high-level KPIs from canonical PnL outputs.
    
    Args:
        positions: Output from compute_realized_pnl (positions_df)
        pnl: Output from compute_realized_pnl (pnl_df)
    
    Returns:
        Dictionary of KPI metrics
    """
    if positions.empty:
        return {"status": "no_data"}
    
    summary = {}
    
    # Core PnL
    summary["total_pnl"] = pnl["net_pnl"].sum()
    summary["total_fees"] = pnl["fees"].sum()
    summary["trade_count"] = len(positions)
    summary["win_rate"] = (positions["net_pnl"] > 0).mean()
    
    # Win/Loss Analysis
    winning_trades = positions[positions["net_pnl"] > 0]
    losing_trades = positions[positions["net_pnl"] < 0]
    
    summary["avg_win"] = winning_trades["net_pnl"].mean() if len(winning_trades) > 0 else 0
    summary["avg_loss"] = losing_trades["net_pnl"].mean() if len(losing_trades) > 0 else 0
    summary["best_trade"] = positions["net_pnl"].max()
    summary["worst_trade"] = positions["net_pnl"].min()
    
    # Duration Analysis
    positions = positions.copy()
    positions["duration"] = (
        pd.to_datetime(positions["close_time"]) - 
        pd.to_datetime(positions["open_time"])
    )
    summary["avg_duration"] = positions["duration"].mean()
    
    # Directional Bias
    summary["long_ratio"] = (positions["side"].isin(["long", "buy"])).mean()
    summary["short_ratio"] = (positions["side"].isin(["short", "sell"])).mean()
    
    # Drawdown
    pnl_sorted = pnl.sort_values("date")
    pnl_sorted["cum_pnl"] = pnl_sorted["net_pnl"].cumsum()
    pnl_sorted["drawdown"] = pnl_sorted["cum_pnl"] - pnl_sorted["cum_pnl"].cummax()
    summary["max_drawdown"] = pnl_sorted["drawdown"].min()
    
    # âœ… NEW: Risk-Adjusted Returns
    if not pnl.empty and len(pnl) > 1:
        daily_returns = pnl.groupby('date')['net_pnl'].sum()
        
        # Sharpe Ratio (assuming risk-free rate = 0)
        mean_return = daily_returns.mean()
        std_return = daily_returns.std()
        sharpe_ratio = mean_return / std_return if std_return > 0 else 0
        summary["sharpe_ratio"] = sharpe_ratio
        
        # Sortino Ratio (downside deviation only)
        downside_returns = daily_returns[daily_returns < 0]
        downside_std = downside_returns.std() if len(downside_returns) > 0 else std_return
        sortino_ratio = mean_return / downside_std if downside_std > 0 else 0
        summary["sortino_ratio"] = sortino_ratio
    else:
        summary["sharpe_ratio"] = 0
        summary["sortino_ratio"] = 0
    
    return summary


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\analytics\validate.py =====

# src/analytics/validate.py
"""
Event validation with support for enhanced mock data fields.
Validates position_id, tx_hash, entry_price, and fee_usd.
"""

from typing import Dict, Any, Set
from datetime import datetime

class EventValidationError(Exception):
    """Raised when event fails validation."""
    pass

BASE_REQUIRED_FIELDS = {
    "event_id",
    "event_type",
    "timestamp",
    "trader_id",
    "market_id",
    "product_type"
}

BASE_OPTIONAL_FIELDS = {
    "side",
    "price",
    "size",
    "fee_usd",
    "pnl",
    "order_type",
    "position_id",
    "tx_hash",
    "entry_price"
}

OPTION_REQUIRED_FIELDS = {
    "option_type",
    "strike",
    "expiry"
}

OPTION_OPTIONAL_FIELDS = {
    "delta",
    "gamma",
    "theta",
    "vega",
    "implied_vol",
    "underlying_price"
}

EVENT_TYPE_SCHEMAS = {
    "trade": {
        "required": {"side", "price", "size"},
        "optional": {"fee_usd", "pnl", "tx_hash"}
    },
    "open": {
        "required": {"side", "price", "size"},
        "optional": {"fee_usd", "pnl", "order_type", "position_id", "tx_hash"}
    },
    "close": {
        "required": {"side", "price", "size"},
        "optional": {"fee_usd", "pnl", "order_type", "position_id", "entry_price", "tx_hash"}
    },
    "liquidation": {
        "required": {"side", "price", "size"},
        "optional": {"fee_usd", "pnl", "order_type", "position_id", "entry_price", "tx_hash"}
    },
    "exercise": {
        "required": {"side", "size"},
        "optional": {"price", "fee_usd", "pnl", "underlying_price", "position_id", "entry_price", "tx_hash"}
    },
    "expire": {
        "required": {"side", "size"},
        "optional": {"price", "fee_usd", "pnl", "underlying_price", "position_id", "entry_price", "tx_hash"}
    }
}


def validate_event(event: dict) -> None:
    """
    Validate event schema and data quality.
    
    Raises:
        EventValidationError: If validation fails
    """
    event_type = event.get("event_type")
    product_type = event.get("product_type")

    if event_type == "trade":
        return

    valid_products = {"spot", "perp", "option"}
    if product_type not in valid_products:
        raise EventValidationError(
            f"Invalid product_type: {product_type}. Allowed: {valid_products}"
        )

    if product_type == "option":
        allowed_sides = {"buy", "sell", "long", "short", "exercise", "expire"}
    elif product_type == "perp":
        allowed_sides = {"long", "short"}
    elif product_type == "spot":
        allowed_sides = {"buy", "sell"}
    
    side = event.get("side")
    if side and side not in allowed_sides:
        raise EventValidationError(
            f"Invalid side '{side}' for product_type '{product_type}'. "
            f"Must be one of: {allowed_sides}"
        )

    allowed_fields = BASE_REQUIRED_FIELDS | BASE_OPTIONAL_FIELDS
    
    if product_type == "option":
        allowed_fields |= OPTION_REQUIRED_FIELDS | OPTION_OPTIONAL_FIELDS
    
    if event_type in EVENT_TYPE_SCHEMAS:
        schema = EVENT_TYPE_SCHEMAS[event_type]
        allowed_fields |= schema["required"] | schema["optional"]

    extra_fields = set(event.keys()) - allowed_fields
    if extra_fields:
        raise EventValidationError(
            f"Unexpected fields detected: {extra_fields}. "
            f"Allowed for {product_type}/{event_type}: {allowed_fields}"
        )

    if event_type in EVENT_TYPE_SCHEMAS:
        schema = EVENT_TYPE_SCHEMAS[event_type]
        missing_required = schema["required"] - set(event.keys())
        if missing_required:
            raise EventValidationError(
                f"Event type '{event_type}' missing required fields: {missing_required}"
            )

    if product_type == "option":
        missing_option_required = OPTION_REQUIRED_FIELDS - set(event.keys())
        if missing_option_required:
            raise EventValidationError(
                f"Option product missing required fields: {missing_option_required}"
            )

    try:
        timestamp_str = event["timestamp"]
        if timestamp_str.endswith("Z"):
            timestamp_str = timestamp_str.replace("Z", "+00:00")
        datetime.fromisoformat(timestamp_str)
    except (ValueError, AttributeError, TypeError) as e:
        raise EventValidationError(f"Invalid timestamp format: {event.get('timestamp')} - {e}")

    numeric_fields = {"price", "size", "fee_usd", "pnl", "strike", "delta", "gamma", 
                     "theta", "vega", "implied_vol", "underlying_price", "entry_price"}
    for field in numeric_fields & event.keys():
        value = event[field]
        if value is not None and not isinstance(value, (int, float)):
            raise EventValidationError(
                f"Field '{field}' must be numeric or null, got {type(value)}: {value}"
            )

    if product_type == "option":
        option_type = event.get("option_type")
        if option_type not in {"call", "put"}:
            raise EventValidationError(f"Invalid option_type: {option_type}. Must be 'call' or 'put'")
        
        expiry = event.get("expiry")
        if expiry:
            try:
                if expiry.endswith("Z"):
                    expiry = expiry.replace("Z", "+00:00")
                datetime.fromisoformat(expiry)
            except (ValueError, AttributeError):
                raise EventValidationError(f"Invalid expiry format: {expiry}")


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\analytics\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\ingestion\normalizer.py =====

# src/ingestion/normalizer.py
"""
Event normalization with support for enhanced mock data fields.
Handles position_id, tx_hash, entry_price, and fee_usd.
"""

from typing import Dict, Any
from datetime import datetime, timezone
import hashlib

def normalize_event(raw_event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalize raw event data into canonical schema.
    Preserves new fields: position_id, tx_hash, entry_price, fee_usd
    """
    event = raw_event.copy()

    ts = event.get("timestamp")
    if isinstance(ts, (int, float)):
        event["timestamp"] = datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()
    elif isinstance(ts, datetime):
        event["timestamp"] = ts.isoformat()
    elif isinstance(ts, str):
        try:
            ts_clean = ts.replace("Z", "+00:00")
            dt = datetime.fromisoformat(ts_clean)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            event["timestamp"] = dt.isoformat().replace("+00:00", "Z")
        except ValueError:
            pass

    key_mappings = {
        "trader": "trader_id",
        "market": "market_id", 
        "type": "event_type",
        "product": "product_type",
        "optionType": "option_type",
        "impliedVol": "implied_vol",
        "fee": "fee_usd"
    }
    
    for old_key, new_key in key_mappings.items():
        if old_key in event and new_key not in event:
            event[new_key] = event.pop(old_key)

    if "product_type" in event:
        product = event["product_type"].lower()
        if product in ["perpetual", "future", "futures", "perp"]:
            event["product_type"] = "perp"
        elif product in ["options", "option"]:
            event["product_type"] = "option"
        elif product in ["spot", "cash"]:
            event["product_type"] = "spot"

    if "side" in event and event.get("product_type") in ["spot", "option"]:
        side = event["side"].lower()
        
        if event.get("event_type") in ["open", "close", "trade"]:
            if side == "long":
                event["side"] = "buy"
            elif side == "short":
                event["side"] = "sell"

    if event.get("product_type") == "option":
        if "option_type" in event:
            event["option_type"] = event["option_type"].lower()
        
        if "expiry" in event and event["expiry"]:
            expiry = event["expiry"]
            if isinstance(expiry, str):
                try:
                    expiry_clean = expiry.replace("Z", "+00:00")
                    dt = datetime.fromisoformat(expiry_clean)
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    event["expiry"] = dt.isoformat().replace("+00:00", "Z")
                except ValueError:
                    pass

    if "event_id" not in event:
        raw_parts = [
            str(event.get('event_type', '')),
            str(event.get('timestamp', '')),
            str(event.get('trader_id', '')),
            str(event.get('market_id', '')),
            str(event.get('product_type', ''))
        ]
        raw = "|".join(raw_parts)
        event["event_id"] = hashlib.sha256(raw.encode()).hexdigest()

    numeric_fields = ["price", "size", "fee_usd", "pnl", "strike", "delta", 
                     "gamma", "theta", "vega", "implied_vol", "underlying_price", "entry_price"]
    
    for field in numeric_fields:
        if field in event and event[field] is not None:
            try:
                event[field] = float(event[field])
            except (ValueError, TypeError):
                pass

    return event


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\ingestion\pipelines.py =====

# src/ingestion/pipelines.py
import json
import hashlib
from pathlib import Path
from src.ingestion.watermark import WatermarkStore
from src.ingestion.normalizer import normalize_event
from src.analytics.validate import validate_event, EventValidationError


class IngestionPipeline:
    def __init__(self, raw_path: str, output_path: str, checkpoint_path: str):
        self.raw_path = Path(raw_path)
        self.output_path = Path(output_path)
        self.watermark = WatermarkStore(checkpoint_path)

    def run(self) -> int:
        """
        Event-driven ingestion: normalize once, append forever.
        Supports both JSON array and JSONL formats.
        """
        if not self.raw_path.exists():
            raise FileNotFoundError(f"Raw data source not found: {self.raw_path}")

        # Load events based on file format
        if self.raw_path.suffix == '.json':
            # JSON array format (e.g., configs/mock_data.json)
            with self.raw_path.open("r", encoding="utf-8") as f:
                raw_events = json.load(f)
        elif self.raw_path.suffix == '.jsonl':
            # JSONL format (one JSON object per line)
            raw_events = []
            with self.raw_path.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:  # Skip empty lines
                        raw_events.append(json.loads(line))
        else:
            raise ValueError(f"Unsupported file format: {self.raw_path.suffix}")

        new_events = []
        errors = []

        for idx, raw in enumerate(raw_events, 1):
            try:
                # Generate event_id if missing
                if "event_id" not in raw:
                    seed = (
                        f"{raw.get('event_type')}|"
                        f"{raw.get('timestamp')}|"
                        f"{raw.get('trader_id')}|"
                        f"{raw.get('market_id')}|{idx}"
                    )
                    raw["event_id"] = hashlib.sha256(seed.encode()).hexdigest()

                # Skip if already processed
                if not self.watermark.is_new(raw["event_id"]):
                    continue

                # Normalize and validate
                normalized = normalize_event(raw)
                validate_event(normalized)

                new_events.append(normalized)
                self.watermark.mark(raw["event_id"])

            except EventValidationError as e:
                errors.append(f"Event {idx}: Validation failed - {e}")
            except Exception as e:
                errors.append(f"Event {idx}: Unexpected error - {e}")

        # Report errors
        if errors:
            print(f"âš ï¸  {len(errors)} events had issues:")
            for e in errors[:5]:
                print(f"   - {e}")
            if len(errors) > 5:
                print(f"   ... and {len(errors) - 5} more")

        # Write normalized events to output (JSONL format)
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        with self.output_path.open("a", encoding="utf-8") as f:
            for e in new_events:
                f.write(json.dumps(e) + "\n")

        print(f"âœ… Ingested {len(new_events)} valid events")
        return len(new_events)


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\ingestion\watermark.py =====

# src/ingestion/watermark.py
import json
from pathlib import Path
from typing import Set

class WatermarkStore:
    """
    Persistent watermark store to prevent reprocessing events.
    """

    def __init__(self, path: str):
        self.path = Path(path)
        self.seen: Set[str] = set()
        self._load()

    def _load(self):
        if self.path.exists():
            with open(self.path, "r") as f:
                self.seen = set(json.load(f))

    def _save(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.path, "w") as f:
            json.dump(list(self.seen), f)

    def is_new(self, event_id: str) -> bool:
        return event_id not in self.seen

    def mark(self, event_id: str):
        self.seen.add(event_id)
        self._save()


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\ingestion\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\src\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\tests\analytics\test_ingestion.py =====

# Create a test script test_ingestion.py
import json

with open("data/normalized/events.jsonl", "r") as f:
    for i, line in enumerate(f, 1):
        line = line.strip()
        if line:
            try:
                data = json.loads(line)
                print(f"Line {i}: OK - {data.get('event_type', 'N/A')}")
            except json.JSONDecodeError as e:
                print(f"Line {i}: ERROR - {e}")
                print(f"  Content: {line[:50]}...")
        else:
            print(f"Line {i}: EMPTY LINE")


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\tests\analytics\test_pnl_engine.py =====

import pandas as pd
from datetime import datetime, timezone

from src.analytics.pnl_engine import compute_realized_pnl

def test_simple_open_close_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100.0,
            "size": 1,
            "fee": 0.5,
        },
        {
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110.0,
            "size": 1,
            "fee": 0.5,
            "pnl": 9.0,  # truth reference
        },
    ])

    positions, pnl, _ = compute_realized_pnl(events)

    assert len(positions) == 1
    assert positions.iloc[0]["realized_pnl"] == 9.0

def test_open_without_close_has_no_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T1",
            "market_id": "SOL-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 50,
            "size": 2,
            "fee": 0.2,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty

def test_pnl_only_on_close_events():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "trade",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T1",
            "market_id": "SOL/USDC",
            "product_type": "spot",
            "side": "buy",
            "price": 100,
            "size": 1,
            "fee": 0.1,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty
def test_pnl_engine_is_deterministic():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T2",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "short",
            "price": 200,
            "size": 1,
            "fee": 0.3,
        },
        {
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T2",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "short",
            "price": 180,
            "size": 1,
            "fee": 0.3,
            "pnl": 19.4,
        },
    ])

    p1, pnl1 = compute_realized_pnl(events)
    p2, pnl2 = compute_realized_pnl(events)

    pd.testing.assert_frame_equal(p1, p2)
    pd.testing.assert_frame_equal(pnl1, pnl2)

def test_close_without_open_is_rejected():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "close",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T3",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120,
            "size": 1,
            "fee": 0.4,
            "pnl": 0,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty
def test_partial_close_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100.0,
            "size": 10,
            "fee": 1.0,
        },
        {
            # partial close (50%)
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110.0,
            "size": 5,
            "fee": 0.5,
        },
        {
            # final close
            "event_id": "e3",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 3, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120.0,
            "size": 5,
            "fee": 0.5,
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 2

    realized = positions["realized_pnl"].sum()
    assert round(realized, 2) == round(
        ((110 - 100) * 5 + (120 - 100) * 5) - 2.0, 2
    )

def test_multiple_partial_closes_order_independent():
    base_events = [
        {
            "event_id": "o1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100,
            "size": 10,
            "fee": 1.0,
        },
        {
            "event_id": "c1",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110,
            "size": 4,
            "fee": 0.4,
        },
        {
            "event_id": "c2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 3, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120,
            "size": 6,
            "fee": 0.6,
        },
    ]

    df1 = pd.DataFrame(base_events)
    df2 = pd.DataFrame(reversed(base_events))

    p1, pnl1 = compute_realized_pnl(df1)
    p2, pnl2 = compute_realized_pnl(df2)

    pd.testing.assert_frame_equal(p1, p2)
    pd.testing.assert_frame_equal(pnl1, pnl2)
def test_liquidation_is_partial_close():
    events = pd.DataFrame([
        {
            "event_id": "o1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T9",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 200,
            "size": 10,
            "fee": 1.0,
        },
        {
            "event_id": "l1",
            "event_type": "liquidation",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T9",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 150,
            "size": 4,
            "fee": 0.5,
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 1
    assert positions.iloc[0]["close_reason"] == "liquidation"


===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\tests\analytics\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\tests\__init__.py =====



===== FILE: C:\Users\g\Deriverse\deriverse-data-puller\pyproject.toml =====

[project]
name = "deriverse-data-puller"
version = "0.1.0"
description = "Mock on-chain trading analytics system for Deriverse"
readme = "README.md"
requires-python = ">=3.10"

dependencies = [
    "base58>=2.1.1",
    "dotenv",
    "matplotlib>=3.10.8",
    "pandas>=2.3.3",
    "plotly>=6.5.2",
    "pyyaml",
    "requests>=2.32.5",
    "streamlit",
]

