

===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\.vscode\settings.json =====

{
    "python.defaultInterpreterPath": "${workspaceFolder}/.venv/Scripts/python.exe",
    "python.analysis.extraPaths": ["./"],
    "python.autoComplete.extraPaths": [
        "${workspaceFolder}/.venv/Lib/site-packages"
    ]
}


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\ingestion.yaml =====

# configs/ingestion.yaml

# Path to raw mock protocol events (JSON array)
raw_data_path: configs/mock_data.json

# Append-only normalized output (JSONL format)
normalized_output_path: data/normalized/events.jsonl

# Watermark / checkpoint store for incremental ingestion
checkpoint_path: data/checkpoints/watermark.json

# Allowed lateness for event-time processing (seconds)
allowed_lateness_seconds: 0

# Optional controls (future-safe)
markets: []
traders: []
max_events_per_run: null


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\loader.py =====

# configs/loader.py

import yaml
from pathlib import Path
import logging  # âœ… ADD THIS

logger = logging.getLogger(__name__) 

def load_config(path: str) -> dict:
    with open(Path(path), "r") as f:
        return yaml.safe_load(f)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\mock_data.json =====

[
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:07:02.792332Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 100,
    "size": 10,
    "fee": 0.5,
    "event_id": "d9f670f82d10b9d66c5ce2522c5b167063cccc5cfdd18e792deef03fd65421e1",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T18:07:02.792332Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 110,
    "size": 10,
    "fee": 0.5,
    "event_id": "3554f00720e1da9a37ac76720ed31ca89e51c74910615e324205e3ded264b101",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:17:02.792332Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 2000,
    "size": 5,
    "fee": 1.0,
    "event_id": "930afc0bc925f065be34cdb4ba5027251031042ce7635a1298b18d29b402b776",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T19:07:02.792332Z",
    "trader_id": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "sell",
    "price": 1950,
    "size": 5,
    "fee": 1.0,
    "event_id": "2b7732c004ba844afd2778f370cbc823722b9a97a2f3fa5f4c07fe17c34eeae9",
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:27:02.792332Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 100,
    "size": 10,
    "fee": 0.5,
    "event_id": "b9f17bb1c9b9061c0b0e0596b4f2938b2cf11da48bc5e13dfbeff70c58064cf3",
    "order_type": "limit"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T20:07:02.792332Z",
    "trader_id": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 120,
    "size": 10,
    "fee": 0.5,
    "event_id": "0530f0c0af81e1b6a949064ce8da97509a1555f79a2d417b7fc6efc35fdd5190",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:37:02.792332Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 50000,
    "size": 1,
    "fee": 5.0,
    "event_id": "13b415247354e84a2c1dacb7558f937700eb966fd1c861fc2f33266a9eb67a7e",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T21:07:02.792332Z",
    "trader_id": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",
    "price": 48000,
    "size": 1,
    "fee": 5.0,
    "event_id": "4aac0dc10fa24d5ee63de61fc50353749caaaf8307e7c83f620ce9a5d9944584",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T16:47:02.792332Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2100,
    "size": 5,
    "fee": 2.0,
    "event_id": "3fa66177a9f28180f0aa5d30499bab407cfa042996b74456ce6d22d37f64d934",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-11T22:07:02.792332Z",
    "trader_id": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 2050,
    "size": 5,
    "fee": 2.0,
    "event_id": "b708a815cdaca1ad43fee20629867a340ece34b7773f306210b48db0e29dc9c5",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T17:07:02.792332Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "buy",
    "price": 5.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.65,
    "implied_vol": 0.45,
    "event_id": "d6e84536be55306698226b8ac9df2fa442a6f2fc3bb3a7ce510bc330f62eca0f",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-12T16:07:02.792332Z",
    "trader_id": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "sell",
    "price": 8.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.85,
    "implied_vol": 0.5,
    "event_id": "cac1e88c0846b1878ac3dbdf667a5796c954a16da23271931f46934aee4b7807",
    "order_type": "stop"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T17:37:02.792332Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "sell",
    "price": 4.0,
    "size": 15,
    "fee": 0.7,
    "delta": -0.25,
    "implied_vol": 0.4,
    "event_id": "d54682ea1bf854c0226eb8683ee5181404b751b39ddc6ac328c096a4bdb1a327",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-13T04:07:02.792332Z",
    "trader_id": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "buy",
    "price": 1.5,
    "size": 15,
    "fee": 0.7,
    "delta": -0.1,
    "implied_vol": 0.3,
    "event_id": "970e3b26d34837ed879b4ebf48f33940bf5fd62df78d8350737eaa6d6cdab30d",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T18:07:02.792332Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "buy",
    "price": 45.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.35,
    "implied_vol": 0.55,
    "event_id": "cc202945b5f92a14d9e67b1078d32190374e9ac1b68d92250a614a5b3c1d1960",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-13T16:07:02.792332Z",
    "trader_id": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "sell",
    "price": 20.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.15,
    "implied_vol": 0.4,
    "event_id": "bdcf82b7553a8e644e5f658cb9e210673111fb180fb5b8585247ebdcd12f48bd",
    "order_type": "limit"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T19:07:02.792332Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "buy",
    "price": 2000.0,
    "size": 1,
    "fee": 10.0,
    "event_id": "35445b32cfd72046c10e93674471ea3e6d335bee9d02d5b280682e6f018fdfd1",
    "order_type": "market"
  },
  {
    "event_type": "exercise",
    "timestamp": "2026-02-28T16:07:02.792332Z",
    "trader_id": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "exercise",
    "size": 1,
    "fee": 10.0,
    "underlying_price": 55000,
    "event_id": "d9923d600f28e29dea3f7d33023c2910540e2c21a93c622ad0cb91a7d6c676c7"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T20:07:02.792332Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "buy",
    "price": 3.0,
    "size": 20,
    "fee": 0.2,
    "event_id": "5c043c280a3d9d9bddca0d4d31b2c859c906ec8474c4527096ad09076931116e",
    "order_type": "market"
  },
  {
    "event_type": "expire",
    "timestamp": "2026-03-01T16:07:02.792332Z",
    "trader_id": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "expire",
    "price": 0.0,
    "size": 20,
    "fee": 0.0,
    "underlying_price": 95,
    "event_id": "82c8061612e569de8be023c268965db2a39b177000d099c5e6f0818007742c98"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T21:07:02.792332Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "buy",
    "price": 8.0,
    "size": 20,
    "fee": 1.0,
    "event_id": "0d4b88fbb7f76ae082eccd60e6963c7f0fbc0b3964ecf72f1f75e853ec5473e4",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-12T18:07:02.792332Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "sell",
    "price": 12.0,
    "size": 10,
    "fee": 0.5,
    "event_id": "e37cd84d9e5fae49e2f07ea5401065c740edca7c19711144917850b148499fdb",
    "order_type": "market"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-13T18:07:02.792332Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": "2026-03-01T16:07:02.792332Z",
    "side": "sell",
    "price": 15.0,
    "size": 10,
    "fee": 0.5,
    "event_id": "10c395044de0c2a19baaacb6072521d056756d1ed30e10246a880cddb99a23c2",
    "order_type": "market"
  },
  {
    "event_type": "open",
    "timestamp": "2026-02-11T17:07:02.792332Z",
    "trader_id": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 105,
    "size": 5,
    "fee": 0.3,
    "event_id": "c7320d276afae24c50fe9fbbdfbfbf593f2eec88acc9ef13a946401ef53438fb",
    "order_type": "stop"
  },
  {
    "event_type": "close",
    "timestamp": "2026-02-12T02:07:02.792332Z",
    "trader_id": "GhostWallet1111111111111111111111111111",
    "market_id": "GHOST-PERP",
    "product_type": "perp",
    "side": "long",
    "price": 999,
    "size": 1,
    "fee": 0.1,
    "event_id": "ab834bd961f71bf1f9b282d1843dc86ea5554f2264ae4ba6ce632e7ac11911f7",
    "order_type": "stop"
  },
  {
    "event_type": "trade",
    "timestamp": "2026-02-11T16:22:02.792332Z",
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",
    "price": 101,
    "size": 100,
    "fee": 1.0,
    "event_id": "55e7dbb452424b938ee0086e38daa41daf7a2eed6fff9d3dcad8722117ac7d5f"
  },
  {
    "event_type": "trade",
    "timestamp": "2026-02-11T16:52:02.792332Z",
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "sell",
    "price": 2105,
    "size": 50,
    "fee": 5.0,
    "event_id": "ed106e29d898c318ebb2cc04c2e43e35800302d97c2a67fac91e688dd33d495d"
  }
]


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\configs\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\dashboards\app.py =====

# dashboards/app.py - COMPLETE FINAL VERSION
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
from datetime import datetime

DATA_DIR = Path("data/analytics_output")

st.set_page_config(page_title="Deriverse Trading Analytics", layout="wide")

st.markdown("""
    <style>
    .metric-card { padding: 20px; border-radius: 10px; background: #f0f2f6; }
    .stMetric { background: white; padding: 15px; border-radius: 8px; }
    </style>
""", unsafe_allow_html=True)

st.title("ðŸ“Š Deriverse Trading Analytics Dashboard")

@st.cache_data
def load_data():
    try:
        equity = pd.read_csv(DATA_DIR / "equity_curve.csv", parse_dates=["timestamp"])
        positions = pd.read_csv(DATA_DIR / "positions.csv", parse_dates=["open_time", "close_time"])
        summary = pd.read_csv(DATA_DIR / "summary_metrics.csv")
        fees = pd.read_csv(DATA_DIR / "fees_breakdown.csv")
        volume = pd.read_csv(DATA_DIR / "volume_by_market.csv")
        pnl_day = pd.read_csv(DATA_DIR / "pnl_by_day.csv", parse_dates=["date"])
        pnl_hour = pd.read_csv(DATA_DIR / "pnl_by_hour.csv")
        directional = pd.read_csv(DATA_DIR / "directional_bias.csv")
        order_perf = pd.read_csv(DATA_DIR / "order_type_performance.csv")
        greeks = pd.read_csv(DATA_DIR / "greeks_exposure.csv")
        
        return {
            'equity': equity, 'positions': positions, 'summary': summary,
            'fees': fees, 'volume': volume, 'pnl_day': pnl_day,
            'pnl_hour': pnl_hour, 'directional': directional, 
            'order_perf': order_perf, 'greeks': greeks
        }
    except FileNotFoundError:
        st.error("Analytics files not found. Run: python -m scripts.run_analytics")
        return None

data = load_data()

if data is None or data['positions'].empty:
    st.warning("No trading data available.")
    st.stop()

# âœ… ADD: Calculate volume in USD for positions
if not data['positions'].empty:
    data['positions']['volume_usd'] = data['positions']['exit_price'] * data['positions']['size']

# Sidebar Filters
st.sidebar.header("ðŸŽ›ï¸ Filters")
traders = sorted(data['positions']['trader_id'].unique())
selected_trader = st.sidebar.selectbox("Trader", ["All"] + traders)

min_date = data['positions']['close_time'].min().date()
max_date = data['positions']['close_time'].max().date()
date_range = st.sidebar.date_input("Date Range", value=(min_date, max_date))

markets = sorted(data['positions']['market_id'].unique())
selected_market = st.sidebar.selectbox("Market", ["All"] + markets)

# Filter Data
filtered_positions = data['positions'].copy()
if selected_trader != "All":
    filtered_positions = filtered_positions[filtered_positions['trader_id'] == selected_trader]
if len(date_range) == 2:
    start, end = date_range
    filtered_positions = filtered_positions[
        (filtered_positions['close_time'].dt.date >= start) &
        (filtered_positions['close_time'].dt.date <= end)
    ]
if selected_market != "All":
    filtered_positions = filtered_positions[filtered_positions['market_id'] == selected_market]

# KPI Tiles
st.header("ðŸ“ˆ Key Performance Indicators")
col1, col2, col3, col4 = st.columns(4)

with col1:
    total_pnl = filtered_positions['realized_pnl'].sum()
    st.metric("Total PnL", f"${total_pnl:,.2f}", delta=f"{total_pnl:,.2f}" if total_pnl > 0 else None)

with col2:
    win_rate = (filtered_positions['realized_pnl'] > 0).mean() * 100
    st.metric("Win Rate", f"{win_rate:.1f}%")

with col3:
    if selected_trader != "All":
        trader_data = data['summary'][data['summary']['trader_id'] == selected_trader]
        max_dd = trader_data['max_drawdown'].iloc[0] if not trader_data.empty else 0
    else:
        max_dd = data['summary']['max_drawdown'].min()
    st.metric("Max Drawdown", f"${max_dd:,.2f}")

with col4:
    total_fees = filtered_positions['fees'].sum()
    st.metric("Total Fees", f"${total_fees:,.2f}")

# Win/Loss Analysis
st.header("ðŸ’¹ Win/Loss Analysis")
col1, col2, col3 = st.columns(3)

with col1:
    winning = filtered_positions[filtered_positions['realized_pnl'] > 0]
    avg_win = winning['realized_pnl'].mean() if len(winning) > 0 else 0
    st.metric("Average Win", f"${avg_win:,.2f}")

with col2:
    losing = filtered_positions[filtered_positions['realized_pnl'] < 0]
    avg_loss = losing['realized_pnl'].mean() if len(losing) > 0 else 0
    st.metric("Average Loss", f"${avg_loss:,.2f}")

with col3:
    profit_factor = abs(avg_win / avg_loss) if avg_loss != 0 else 0
    st.metric("Profit Factor", f"{profit_factor:.2f}x")

# Risk-Adjusted Returns
if 'sharpe_ratio' in data['summary'].columns:
    st.header("ðŸ“Š Risk-Adjusted Returns")
    col1, col2 = st.columns(2)
    with col1:
        if selected_trader != "All":
            trader_sharpe = data['summary'][data['summary']['trader_id'] == selected_trader]['sharpe_ratio'].iloc[0] if not data['summary'][data['summary']['trader_id'] == selected_trader].empty else 0
        else:
            trader_sharpe = data['summary']['sharpe_ratio'].mean()
        st.metric("Sharpe Ratio", f"{trader_sharpe:.2f}")
    with col2:
        if selected_trader != "All":
            trader_sortino = data['summary'][data['summary']['trader_id'] == selected_trader]['sortino_ratio'].iloc[0] if not data['summary'][data['summary']['trader_id'] == selected_trader].empty else 0
        else:
            trader_sortino = data['summary']['sortino_ratio'].mean()
        st.metric("Sortino Ratio", f"{trader_sortino:.2f}")

# Equity Curve
st.header("ðŸ’° Equity Curve")
filtered_equity = data['equity'].copy()
if selected_trader != "All":
    filtered_equity = filtered_equity[filtered_equity['trader_id'] == selected_trader]

if not filtered_equity.empty:
    fig = go.Figure()
    for trader in filtered_equity['trader_id'].unique():
        trader_eq = filtered_equity[filtered_equity['trader_id'] == trader].sort_values('timestamp')
        trader_short = trader[:8] + "..." if len(trader) > 12 else trader
        fig.add_trace(go.Scatter(x=trader_eq['timestamp'], y=trader_eq['cumulative_pnl'], name=f"{trader_short} PnL"))
        fig.add_trace(go.Scatter(x=trader_eq['timestamp'], y=trader_eq['drawdown'], fill='tozeroy', fillcolor='rgba(255,0,0,0.2)', line=dict(color='red', width=0.5), name=f"{trader_short} DD"))
    fig.update_layout(xaxis_title="Time", yaxis_title="PnL ($)", hovermode='x unified', height=400)
    st.plotly_chart(fig, width='stretch')

# Charts Row 1
col1, col2 = st.columns(2)

with col1:
    st.subheader("ðŸ“… Daily PnL")
    if not filtered_positions.empty:
        daily = filtered_positions.groupby(filtered_positions['close_time'].dt.date)['realized_pnl'].sum().reset_index()
        daily.columns = ['date', 'pnl']
        fig = px.bar(daily, x='date', y='pnl', color='pnl', color_continuous_scale=['red', 'gray', 'green'], color_continuous_midpoint=0)
        fig.update_layout(height=300, showlegend=False)
        st.plotly_chart(fig, width='stretch')

with col2:
    st.subheader("ðŸ’¸ Fees by Product")
    if not filtered_positions.empty:
        fees_prod = filtered_positions.groupby('product_type')['fees'].sum().reset_index()
        fig = px.bar(fees_prod, x='product_type', y='fees', color='product_type')
        fig.update_layout(height=300, showlegend=False)
        st.plotly_chart(fig, width='stretch')

# âœ… NEW: Complete Fee Analysis Section
st.header("ðŸ’¸ Complete Fee Analysis")
col1, col2 = st.columns(2)

with col1:
    st.subheader("Fee Composition Breakdown")
    if not data['fees'].empty:
        # Get filtered trader fees
        if selected_trader != "All":
            trader_fees = data['fees'][data['fees']['trader_id'] == selected_trader]
        else:
            trader_fees = data['fees']
        
        fig = px.pie(trader_fees, values='total_fees', names='product_type', 
                     title='Fee Distribution by Product Type')
        fig.update_layout(height=300)
        st.plotly_chart(fig, width='stretch')
    
    # Show fee breakdown table
    if not filtered_positions.empty:
        fee_summary = filtered_positions.groupby('product_type').agg({
            'fees': ['sum', 'mean', 'count']
        }).round(2)
        fee_summary.columns = ['Total Fees', 'Avg Fee', 'Trade Count']
        st.dataframe(fee_summary, width='stretch')

with col2:
    st.subheader("Cumulative Fee Tracking")
    if not filtered_positions.empty:
        # Calculate cumulative fees over time
        fee_timeline = filtered_positions.sort_values('close_time')[['close_time', 'fees']].copy()
        fee_timeline['cumulative_fees'] = fee_timeline['fees'].cumsum()
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=fee_timeline['close_time'],
            y=fee_timeline['cumulative_fees'],
            mode='lines+markers',
            name='Cumulative Fees',
            line=dict(color='red', width=2)
        ))
        fig.update_layout(
            xaxis_title="Time",
            yaxis_title="Cumulative Fees ($)",
            height=300
        )
        st.plotly_chart(fig, width='stretch')

# âœ… NEW: Complete Volume Analysis
st.header("ðŸ“Š Complete Trading Volume Analysis")
col1, col2 = st.columns(2)

with col1:
    st.subheader("Volume by Market (USD)")
    if not filtered_positions.empty:
        # âœ… FIXED: Calculate volume in USD properly
        market_vol = filtered_positions.groupby('market_id').agg({
            'volume_usd': 'sum',
            'realized_pnl': 'sum'
        }).sort_values('volume_usd', ascending=False)
        
        fig = px.bar(market_vol.reset_index(), x='market_id', y='volume_usd',
                     title='Trading Volume by Market (USD)',
                     labels={'volume_usd': 'Volume (USD)', 'market_id': 'Market'})
        fig.update_layout(height=350, xaxis_tickangle=-45)
        st.plotly_chart(fig, width='stretch')

with col2:
    st.subheader("Volume vs PnL by Market")
    if not filtered_positions.empty:
        market_stats = filtered_positions.groupby('market_id').agg({
            'volume_usd': 'sum',
            'realized_pnl': 'sum',
            'size': 'sum'
        }).sort_values('volume_usd', ascending=False).head(10)
        
        # Display as table with proper formatting
        display_stats = market_stats.copy()
        display_stats.columns = ['Volume (USD)', 'PnL ($)', 'Size (Units)']
        st.dataframe(display_stats.style.format({
            'Volume (USD)': '${:,.2f}',
            'PnL ($)': '${:,.2f}',
            'Size (Units)': '{:,.2f}'
        }), width='stretch')

# Charts Row 2
col1, col2 = st.columns(2)

with col1:
    st.subheader("âš–ï¸ Long vs Short")
    if not filtered_positions.empty:
        long_cnt = len(filtered_positions[filtered_positions['side'].isin(['long', 'buy'])])
        short_cnt = len(filtered_positions[filtered_positions['side'].isin(['short', 'sell'])])
        fig = go.Figure(data=[go.Pie(labels=['Long', 'Short'], values=[long_cnt, short_cnt], hole=0.4, marker_colors=['#00cc96', '#ef553b'])])
        fig.update_layout(height=300)
        st.plotly_chart(fig, width='stretch')

with col2:
    st.subheader("ðŸ• Hourly Performance")
    if not filtered_positions.empty:
        hourly = filtered_positions.copy()
        hourly['hour'] = hourly['close_time'].dt.hour
        hour_pnl = hourly.groupby('hour')['realized_pnl'].mean().reset_index()
        fig = px.line(hour_pnl, x='hour', y='realized_pnl', markers=True)
        fig.update_layout(height=300, xaxis_title="Hour", yaxis_title="Avg PnL")
        st.plotly_chart(fig, width='stretch')

# Order Type Performance
st.header("ðŸ“Š Order Type Performance")
if not data['order_perf'].empty:
    fig = go.Figure()
    fig.add_trace(go.Bar(name='Win Rate', x=data['order_perf']['order_type'], y=data['order_perf']['win_rate']*100, yaxis='y', marker_color='lightblue'))
    fig.add_trace(go.Scatter(name='Avg PnL', x=data['order_perf']['order_type'], y=data['order_perf']['avg_pnl'], yaxis='y2', marker_color='green', line=dict(width=3)))
    fig.update_layout(xaxis_title="Order Type", yaxis_title="Win Rate (%)", yaxis2=dict(title="Avg PnL ($)", overlaying='y', side='right'), height=400)
    st.plotly_chart(fig, width='stretch')

# Greeks Exposure
if not data['greeks'].empty:
    st.header("ðŸ”¬ Options Greeks Exposure")
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Net Delta by Trader")
        fig = px.bar(data['greeks'], x='trader_id', y='net_delta', title="Delta Exposure")
        st.plotly_chart(fig, width='stretch')
    
    with col2:
        st.subheader("Options Position Count")
        fig = px.bar(data['greeks'], x='trader_id', y='total_option_positions', title="Number of Options Positions")
        st.plotly_chart(fig, width='stretch')

# âœ… NEW: Open Positions with Unrealized PnL
st.header("ðŸ“‹ Open Positions (Unrealized PnL)")
st.info("âš ï¸ Note: This is a demo system with mock data. In production, open positions would be calculated from current market prices.")

# Create sample open positions (since all are closed in demo)
open_positions_sample = pd.DataFrame([
    {
        'position_id': 'OPEN-001',
        'trader_id': '7KNXqv...Bp9uV',
        'market_id': 'BTC-PERP',
        'product_type': 'perp',
        'side': 'long',
        'entry_price': 51000.00,
        'current_price': 52500.00,  # Mock current price
        'size': 0.5,
        'unrealized_pnl': (52500 - 51000) * 0.5,
        'open_time': datetime.now()
    },
    {
        'position_id': 'OPEN-002',
        'trader_id': '5FxM2n...fXyZ',
        'market_id': 'ETH/USDC',
        'product_type': 'spot',
        'side': 'buy',
        'entry_price': 2100.00,
        'current_price': 2150.00,
        'size': 10,
        'unrealized_pnl': (2150 - 2100) * 10,
        'open_time': datetime.now()
    }
])

st.dataframe(open_positions_sample.style.format({
    'entry_price': '${:,.2f}',
    'current_price': '${:,.2f}',
    'unrealized_pnl': '${:,.2f}',
    'size': '{:,.4f}'
}), width='stretch', hide_index=True)

st.caption("ðŸ’¡ In production: Unrealized PnL would be calculated using real-time market prices from the protocol")

# Trade History Table
st.header("ðŸ“‹ Closed Trade History")

if not filtered_positions.empty:
    # âœ… ADDED: Volume in USD column
    display_df = filtered_positions[[
        'position_id', 'trader_id', 'market_id', 'product_type', 'side',
        'entry_price', 'exit_price', 'size', 'volume_usd', 'realized_pnl', 'fees',
        'open_time', 'close_time'
    ]].copy()
    
    # Format columns
    display_df['realized_pnl'] = display_df['realized_pnl'].apply(lambda x: f"${x:,.2f}")
    display_df['fees'] = display_df['fees'].apply(lambda x: f"${x:,.2f}")
    display_df['entry_price'] = display_df['entry_price'].apply(lambda x: f"${x:,.2f}")
    display_df['exit_price'] = display_df['exit_price'].apply(lambda x: f"${x:,.2f}")
    display_df['volume_usd'] = display_df['volume_usd'].apply(lambda x: f"${x:,.2f}")  # âœ… ADDED
    display_df['size'] = display_df['size'].apply(lambda x: f"{x:,.4f}")
    
    # Rename for display
    display_df = display_df.rename(columns={
        'position_id': 'Position ID',
        'trader_id': 'Trader',
        'market_id': 'Market',
        'product_type': 'Type',
        'side': 'Side',
        'entry_price': 'Entry',
        'exit_price': 'Exit',
        'size': 'Size',
        'volume_usd': 'Volume (USD)',  # âœ… ADDED
        'realized_pnl': 'PnL',
        'fees': 'Fees',
        'open_time': 'Opened',
        'close_time': 'Closed'
    })
    
    # Sort by close time descending
    display_df = display_df.sort_values('Closed', ascending=False)
    
    st.dataframe(display_df, width='stretch', height=400, hide_index=True)
    
    # Download button
    csv = filtered_positions.to_csv(index=False)
    st.download_button("ðŸ“¥ Download CSV", data=csv, file_name=f"trades_{datetime.now().strftime('%Y%m%d')}.csv", mime="text/csv")
else:
    st.info("No trades match the selected filters")

# Additional Analytics
st.header("ðŸ“Š Additional Analytics")
col1, col2, col3 = st.columns(3)

with col1:
    st.subheader("ðŸŽ¯ Best/Worst Trades")
    if not filtered_positions.empty:
        best = filtered_positions.loc[filtered_positions['realized_pnl'].idxmax()]
        worst = filtered_positions.loc[filtered_positions['realized_pnl'].idxmin()]
        
        st.write(f"**Best Trade:** ${best['realized_pnl']:,.2f}")
        st.write(f"Market: {best['market_id']}")
        st.write(f"Volume: ${best['volume_usd']:,.2f}")
        st.write("")
        st.write(f"**Worst Trade:** ${worst['realized_pnl']:,.2f}")
        st.write(f"Market: {worst['market_id']}")
        st.write(f"Volume: ${worst['volume_usd']:,.2f}")

with col2:
    st.subheader("â±ï¸ Average Duration")
    if not filtered_positions.empty:
        avg_duration = filtered_positions['duration_seconds'].mean()
        hours = int(avg_duration // 3600)
        minutes = int((avg_duration % 3600) // 60)
        st.metric("Avg Position Duration", f"{hours}h {minutes}m")
        
        st.write(f"**Shortest:** {filtered_positions['duration_seconds'].min() / 60:.0f} min")
        st.write(f"**Longest:** {filtered_positions['duration_seconds'].max() / 3600:.1f} hrs")

with col3:
    st.subheader("ðŸ“ˆ Top Markets by Volume")
    if not filtered_positions.empty:
        market_vol = filtered_positions.groupby('market_id').agg({
            'volume_usd': 'sum',
            'realized_pnl': 'sum'
        }).sort_values('volume_usd', ascending=False)
        
        for market, row in market_vol.head(5).iterrows():
            market_short = market[:15] + "..." if len(market) > 18 else market
            st.write(f"**{market_short}**")
            st.write(f"Vol: ${row['volume_usd']:,.0f} | PnL: ${row['realized_pnl']:,.2f}")
            st.write("")

# Footer
st.markdown("---")
st.caption(f"Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Deriverse Analytics v2.0")


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\dashboards\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\diagnose_data.py =====

# scripts/diagnose_data.py
"""
Data quality diagnostic tool.
Run after analytics to verify all data is properly processed.
"""

import pandas as pd
from pathlib import Path
import json

print("=" * 60)
print("DATA QUALITY DIAGNOSTIC")
print("=" * 60)

# Check positions file
positions_path = Path("data/analytics_output/positions.csv")
if positions_path.exists():
    positions = pd.read_csv(positions_path)
    
    print(f"\nðŸ“Š POSITIONS SUMMARY ({len(positions)} total)")
    print("\nâœ… By Product Type:")
    product_counts = positions['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} positions")
    
    print("\nâœ… By Market:")
    market_counts = positions['market_id'].value_counts()
    for market, count in market_counts.items():
        print(f"  {market:25} {count:>3} positions")
    
    print("\nâœ… By Trader:")
    trader_counts = positions['trader_id'].value_counts()
    for trader, count in trader_counts.items():
        print(f"  {trader:10} {count:>3} positions")
    
    print("\nðŸ’° PnL BY PRODUCT TYPE:")
    pnl_by_product = positions.groupby('product_type')['realized_pnl'].sum()
    for product, pnl in pnl_by_product.items():
        print(f"  {product:10} ${pnl:>12,.2f}")
    
    print("\nðŸ” OPTION POSITIONS DETAIL:")
    option_positions = positions[positions['product_type'] == 'option']
    if not option_positions.empty:
        print(f"  Found {len(option_positions)} option positions")
        for _, row in option_positions.iterrows():
            print(f"    â€¢ {row['market_id']:30} {row['trader_id']:10} ${row['realized_pnl']:>10,.2f}")
    else:
        print("  âŒ NO OPTION POSITIONS FOUND")
    
    print("\nðŸ“‹ ALL POSITIONS SUMMARY:")
    # Check which columns exist
    available_cols = ['position_id', 'trader_id', 'market_id', 'product_type', 'side', 'realized_pnl']
    if 'close_reason' in positions.columns:
        available_cols.append('close_reason')
    
    print(positions[available_cols].to_string(index=False))
    
else:
    print("âŒ positions.csv not found")

# Check normalized events
events_path = Path("data/normalized/events.jsonl")
if events_path.exists():
    events = []
    with open(events_path) as f:
        for line in f:
            line = line.strip()
            if line:
                events.append(json.loads(line))
    
    df = pd.DataFrame(events)
    
    print(f"\nðŸ“¥ NORMALIZED EVENTS ({len(df)} total)")
    print("\nâœ… By Event Type:")
    event_counts = df['event_type'].value_counts()
    for event_type, count in event_counts.items():
        print(f"  {event_type:10} {count:>3} events")
    
    print("\nâœ… By Product Type:")
    product_counts = df['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} events")
    
    print("\nðŸŽ¯ OPTION EVENTS BREAKDOWN:")
    option_events = df[df['product_type'] == 'option']
    print(f"  Total option events: {len(option_events)}")
    if not option_events.empty:
        print("\n  By event type:")
        option_event_counts = option_events['event_type'].value_counts()
        for event_type, count in option_event_counts.items():
            print(f"    {event_type:10} {count:>3}")
        
        print("\n  By market:")
        option_market_counts = option_events['market_id'].value_counts()
        for market, count in option_market_counts.items():
            print(f"    {market:30} {count:>3}")
    else:
        print("  âŒ NO OPTION EVENTS")
else:
    print("âŒ events.jsonl not found")

# Check raw mock data
mock_path = Path("configs/mock_data.json")
if mock_path.exists():
    with open(mock_path) as f:
        mock_data = json.load(f)
    
    print(f"\nðŸ“¦ RAW MOCK DATA ({len(mock_data)} events)")
    mock_df = pd.DataFrame(mock_data)
    
    print("\nâœ… By Event Type:")
    event_counts = mock_df['event_type'].value_counts()
    for event_type, count in event_counts.items():
        print(f"  {event_type:10} {count:>3} events")
    
    print("\nâœ… By Product Type:")
    product_counts = mock_df['product_type'].value_counts()
    for product, count in product_counts.items():
        print(f"  {product:10} {count:>3} events")
else:
    print("\nâŒ configs/mock_data.json not found")

print("\n" + "=" * 60)

# Check for duplicates
if events_path.exists():
    print("\nðŸ” CHECKING FOR DUPLICATES...")
    event_ids = [e['event_id'] for e in events]
    unique_ids = set(event_ids)
    
    if len(event_ids) != len(unique_ids):
        print(f"  âš ï¸  WARNING: Found {len(event_ids) - len(unique_ids)} duplicate events!")
        print(f"  Total events: {len(event_ids)}, Unique: {len(unique_ids)}")
    else:
        print(f"  âœ… No duplicates found ({len(event_ids)} unique events)")

print("=" * 60)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\generate_mock_data.py =====

# scripts/generate_mock_data.py
import json
import random
import os
import hashlib
from datetime import datetime, timezone, timedelta
from pathlib import Path

# âœ… Output to configs/mock_data.json (JSON array format)
OUTPUT_PATH = Path("configs/mock_data.json")
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

SEED = int(os.getenv("MOCK_SEED", "42"))
random.seed(SEED)

now = datetime.now(timezone.utc)
events = []

# âœ… REALISTIC SOLANA WALLET ADDRESSES (base58-like)
WALLETS = {
    "alice": "7KNXqvHu2QWvDq8cGPGvKZhFvYnz3kQ5mL8xRt2Bp9uV",
    "bob": "5FxM2nQwP4vYkL9mT3xRd8eJbWp7sN6gH2cKt9uVfXyZ",
    "charlie": "9DpT3vHx5kN2qL8mR7wYfJ6bP4sE1cG9nZ5tK3uVwXyA",
    "diana": "4MqL8vYx2kP9nT7wR5fH3bJ6sE1cG4nZ8tK2uVwXyBpQ",
    "evan": "6NrK9wZx3mQ8pU7vS4gI2dL5tF1eH7oA9yM3xVbCwRtE",
    "fiona": "8QtN2xWy5lR7mV9uT6hK3eM4pG1fJ8nB7zL4wVcDxSeF",
    "george": "3HsJ7yVz4nQ6oW8tS5gL2fN9rH1eK6mC8xM5vBdEwRuG",
    "hannah": "2PrM8xUz6oT5nY7vR4jL3gP1sH9eN4mD6zK8wCfGxQuH",
    "ivan": "5TpQ9yXz7mS6oV8uR3kM2hN4rJ1fL5nE7xP6wDgHySvI",
    "julia": "4WqP8zYx5nT7mU9tS2lN6jM3rK1gH4oC8yL5vEfJxRwK",
}

def generate_event_id(event_data, index):
    """Generate deterministic event ID."""
    seed_parts = [
        str(event_data.get('event_type', '')),
        str(event_data.get('timestamp', '') if isinstance(event_data.get('timestamp'), str) else ''),
        str(event_data.get('trader_id', '')),
        str(event_data.get('market_id', '')),
        str(index)
    ]
    seed = "|".join(seed_parts)
    return hashlib.sha256(seed.encode()).hexdigest()

def emit(event):
    """Helper to add event with proper formatting and event_id."""
    if 'timestamp' in event and isinstance(event['timestamp'], datetime):
        event['timestamp'] = event['timestamp'].isoformat().replace("+00:00", "Z")
    
    if 'event_id' not in event:
        event['event_id'] = generate_event_id(event, len(events) + 1)
    
    # âœ… ADD THIS: Assign order type
    if event['event_type'] in ['open', 'close']:
        # Randomly assign order types for diversity
        event['order_type'] = random.choice(['market', 'limit', 'stop'])
    
    events.append(event)
    
# --------------------------------------------------
# 1ï¸âƒ£ SPOT TRADES (buy â†’ sell) - 1 WIN, 1 LOSS
# --------------------------------------------------

# WINNING SPOT TRADE
emit({
    "event_type": "open",
    "timestamp": now,
    "trader_id": WALLETS["alice"],
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 100,
    "size": 10,
    "fee": 0.5,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=2),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 110,
    "size": 10,
    "fee": 0.5,
})

# LOSING SPOT TRADE
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=10),
    "trader_id": WALLETS["bob"],
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 2000,
    "size": 5,
    "fee": 1.0,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=3),
    "trader_id": WALLETS["bob"],
    "market_id": "ETH/USDC",
    "product_type": "spot",
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 1950,
    "size": 5,
    "fee": 1.0,
})

# --------------------------------------------------
# 2ï¸âƒ£ PERPETUAL TRADES (long/short) - 2 WINS, 1 LOSS
# --------------------------------------------------

# WINNING LONG PERP
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=20),
    "trader_id": WALLETS["charlie"],
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Perps use long/short
    "price": 100,
    "size": 10,
    "fee": 0.5,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=4),
    "trader_id": WALLETS["charlie"],
    "market_id": "SOL-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Same side for perp close
    "price": 120,
    "size": 10,
    "fee": 0.5,
})

# WINNING SHORT PERP
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=30),
    "trader_id": WALLETS["diana"],
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",  # âœ… CORRECT: Perps use long/short
    "price": 50000,
    "size": 1,
    "fee": 5.0,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=5),
    "trader_id": WALLETS["diana"],
    "market_id": "BTC-PERP",
    "product_type": "perp",
    "side": "short",  # âœ… CORRECT: Same side for perp close
    "price": 48000,
    "size": 1,
    "fee": 5.0,
})

# LOSING LONG PERP
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=40),
    "trader_id": WALLETS["evan"],
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Perps use long/short
    "price": 2100,
    "size": 5,
    "fee": 2.0,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=6),
    "trader_id": WALLETS["evan"],
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Same side for perp close
    "price": 2050,
    "size": 5,
    "fee": 2.0,
})

# --------------------------------------------------
# 3ï¸âƒ£ OPTION TRADES - COMPLETE LIFECYCLE
# --------------------------------------------------

# === LONG CALL OPTION: Buy call, sell it back (Winning) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=1),
    "trader_id": WALLETS["fiona"],
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 5.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.65,
    "implied_vol": 0.45,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=24),
    "trader_id": WALLETS["fiona"],
    "market_id": "SOL-CALL-120-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 120,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 8.0,
    "size": 10,
    "fee": 0.5,
    "delta": 0.85,
    "implied_vol": 0.50,
})

# === SHORT PUT OPTION: Sell put, buy it back (Winning) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=1, minutes=30),
    "trader_id": WALLETS["george"],
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… CORRECT: Selling to open short position
    "price": 4.0,
    "size": 15,
    "fee": 0.7,
    "delta": -0.25,
    "implied_vol": 0.40,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=36),
    "trader_id": WALLETS["george"],
    "market_id": "SOL-PUT-90-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 90,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… CORRECT: Buying to close short position
    "price": 1.5,
    "size": 15,
    "fee": 0.7,
    "delta": -0.10,
    "implied_vol": 0.30,
})

# === LONG PUT OPTION: Buy put, sell it back (Losing) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=2),
    "trader_id": WALLETS["hannah"],
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 45.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.35,
    "implied_vol": 0.55,
})

emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=48),
    "trader_id": WALLETS["hannah"],
    "market_id": "ETH-PUT-1900-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 1900,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 20.0,
    "size": 5,
    "fee": 1.0,
    "delta": -0.15,
    "implied_vol": 0.40,
})

# === LONG CALL: Buy and EXERCISE (ITM) - Winning ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=3),
    "trader_id": WALLETS["ivan"],
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 2000.0,
    "size": 1,
    "fee": 10.0,
})

emit({
    "event_type": "exercise",
    "timestamp": now + timedelta(days=17),
    "trader_id": WALLETS["ivan"],
    "market_id": "BTC-CALL-50000-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 50000,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "exercise",  # âœ… CORRECT: Exercise is its own side
    "size": 1,
    "fee": 10.0,
    "underlying_price": 55000,
})

# === LONG PUT: Buy and let EXPIRE worthless (Losing) ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=4),
    "trader_id": WALLETS["julia"],
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 3.0,
    "size": 20,
    "fee": 0.2,
})

emit({
    "event_type": "expire",
    "timestamp": now + timedelta(days=18),
    "trader_id": WALLETS["julia"],
    "market_id": "SOL-PUT-80-FEB28",
    "product_type": "option",
    "option_type": "put",
    "strike": 80,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "expire",  # âœ… CORRECT: Expire is its own side
    "price": 0.0,
    "size": 20,
    "fee": 0.0,
    "underlying_price": 95,
})

# === PARTIAL CLOSE: Long call, close in 2 tranches ===
emit({
    "event_type": "open",
    "timestamp": now + timedelta(hours=5),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 8.0,
    "size": 20,
    "fee": 1.0,
})

# First partial close
emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=26),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 12.0,
    "size": 10,
    "fee": 0.5,
})

# Second partial close
emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=50),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL-CALL-110-FEB28",
    "product_type": "option",
    "option_type": "call",
    "strike": 110,
    "expiry": (now + timedelta(days=18)).isoformat().replace("+00:00", "Z"),
    "side": "sell",  # âœ… FIXED: Changed from "long" to "sell"
    "price": 15.0,
    "size": 10,
    "fee": 0.5,
})

# --------------------------------------------------
# 4ï¸âƒ£ EDGE CASES (for robustness testing)
# --------------------------------------------------

# Duplicate open (should be ignored)
emit({
    "event_type": "open",
    "timestamp": now + timedelta(minutes=60),
    "trader_id": WALLETS["alice"],
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… FIXED: Changed from "long" to "buy"
    "price": 105,
    "size": 5,
    "fee": 0.3,
})

# Close without open (should be rejected)
emit({
    "event_type": "close",
    "timestamp": now + timedelta(hours=10),
    "trader_id": "GhostWallet1111111111111111111111111111",
    "market_id": "GHOST-PERP",
    "product_type": "perp",
    "side": "long",  # âœ… CORRECT: Perps use long/short
    "price": 999,
    "size": 1,
    "fee": 0.1,
})

# Trade events (informational only)
emit({
    "event_type": "trade",
    "timestamp": now + timedelta(minutes=15),
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "SOL/USDC",
    "product_type": "spot",
    "side": "buy",  # âœ… CORRECT: Trades can use buy/sell
    "price": 101,
    "size": 100,
    "fee": 1.0,
})

emit({
    "event_type": "trade",
    "timestamp": now + timedelta(minutes=45),
    "trader_id": "MarketMaker1111111111111111111111111",
    "market_id": "ETH-PERP",
    "product_type": "perp",
    "side": "sell",  # âœ… CORRECT: Trades can use buy/sell
    "price": 2105,
    "size": 50,
    "fee": 5.0,
})

# --------------------------------------------------
# âœ… WRITE OUTPUT AS JSON ARRAY
# --------------------------------------------------
with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(events, f, indent=2)

print(f"âœ… Generated {len(events)} mock events â†’ {OUTPUT_PATH} (seed={SEED})")
print(f"   Wallets: {len(WALLETS)} realistic Solana addresses")
print(f"   Includes: Spot (buy/sell), Perps (long/short), Options (buy/sell/exercise/expire)")
print(f"   Format: JSON array - ready for ingestion")


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\launch_dashboard.py =====

# scripts/launch_dashboard.py

import streamlit as st
from dashboards.app import run_app
from src.common.logging import get_logger
from configs.loader import load_config

logger = get_logger(__name__)


def main():
    config = load_config("configs/dashboard.yaml")

    logger.info("Launching analytics dashboard")

    st.set_page_config(
        page_title="Deriverse Trading Analytics",
        layout="wide"
    )

    run_app(config)


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\run_analytics.py =====

# scripts/run_analytics.py
import pandas as pd
from pathlib import Path
import io
import logging
from datetime import datetime, UTC
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.analytics.pnl_engine import compute_realized_pnl
from src.analytics.summary import compute_executive_summary
from src.analytics.analytics_builder import AnalyticsBuilder

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

NORMALIZED_EVENTS_PATH = Path("data/normalized/events.jsonl")
ANALYTICS_OUTPUT_DIR = Path("data/analytics_output")


def load_events(path: Path) -> pd.DataFrame:
    """Load events from JSONL file with flexible timestamp parsing."""
    events = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                events.append(pd.read_json(io.StringIO(line), typ="series"))
    
    if not events:
        logger.warning(f"No events found in {path}")
        return pd.DataFrame()
    
    df = pd.DataFrame(events)
    
    try:
        df["timestamp"] = pd.to_datetime(df["timestamp"], format='ISO8601', utc=True)
    except Exception as e:
        logger.warning(f"ISO8601 parsing failed, trying mixed format: {e}")
        df["timestamp"] = pd.to_datetime(df["timestamp"], format='mixed', utc=True)
    
    return df


def run_analytics(events_df, auto_summary=True):
    if events_df.empty:
        logger.error("No events to analyze")
        return None, None
    
    logger.info(f"Loaded {len(events_df)} events")

    logger.info("Computing realized PnL (truth engine)")
    positions_df, pnl_df = compute_realized_pnl(events_df)

    if positions_df.empty:
        logger.warning("No closed positions found â€“ creating empty analytics")
    
    # Build all analytics outputs
    logger.info("Building comprehensive analytics tables...")
    builder = AnalyticsBuilder(positions_df, pnl_df, ANALYTICS_OUTPUT_DIR)
    builder.build_all()

    if not positions_df.empty and auto_summary:
        summary = compute_executive_summary(positions_df, pnl_df)
        
        print("\n" + "=" * 50)
        print("EXECUTIVE SUMMARY")
        print("=" * 50)
        print(f"Total Realized PnL:  ${summary['total_pnl']:,.2f}")
        print(f"Total Fees Paid:     ${summary['total_fees']:,.2f}")
        print(f"Total Trades:        {summary['trade_count']}")
        print(f"Win Rate:            {summary['win_rate']:.1%}")
        print(f"Avg Win:             ${summary['avg_win']:,.2f}")
        print(f"Avg Loss:            ${summary['avg_loss']:,.2f}")
        print(f"Best Trade:          ${summary['best_trade']:,.2f}")
        print(f"Worst Trade:         ${summary['worst_trade']:,.2f}")
        print(f"Avg Duration:        {summary['avg_duration']}")
        print(f"Long Ratio:          {summary['long_ratio']:.1%}")
        print(f"Short Ratio:         {summary['short_ratio']:.1%}")
        print(f"Max Drawdown:        ${summary['max_drawdown']:,.2f}")
        
        # âœ… FIXED: Added risk-adjusted metrics to match summary.py
        if 'sharpe_ratio' in summary:
            print(f"Sharpe Ratio:        {summary['sharpe_ratio']:.2f}")
        if 'sortino_ratio' in summary:
            print(f"Sortino Ratio:       {summary['sortino_ratio']:.2f}")
        
        print("=" * 50 + "\n")

    logger.info("Analytics run complete âœ…")
    return positions_df, pnl_df


def main():
    logger.info("=" * 60)
    logger.info("Starting Deriverse Analytics Pipeline")
    logger.info("=" * 60)

    if not NORMALIZED_EVENTS_PATH.exists():
        logger.error(f"Normalized events not found at {NORMALIZED_EVENTS_PATH}")
        logger.error("Run 'python -m scripts.generate_mock_data' first")
        return

    events_df = load_events(NORMALIZED_EVENTS_PATH)
    run_analytics(events_df)


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\run_ingestion.py =====


# scripts/run_ingestion.py
import logging
from src.ingestion.pipelines import IngestionPipeline
from configs.loader import load_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    logger.info("Starting incremental ingestion")

    config = load_config("configs/ingestion.yaml")
    
    pipeline = IngestionPipeline(
        raw_path=config["raw_data_path"],
        output_path=config["normalized_output_path"],
        checkpoint_path=config["checkpoint_path"],
    )

    count = pipeline.run()
    logger.info(f"Ingested {count} new events")


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\scripts\validate_analytics.py =====

# scripts/validate_analytics.py
"""
Validation script to verify analytics output quality and correctness.
Run after python -m scripts.run_analytics
"""

import pandas as pd
from pathlib import Path
import sys

OUTPUT_DIR = Path("data/analytics_output")

class bcolors:
    OK = '\033[92m'
    FAIL = '\033[91m'
    WARN = '\033[93m'
    END = '\033[0m'

def validate_file_exists(filename):
    """Check if required file exists."""
    path = OUTPUT_DIR / filename
    if path.exists():
        print(f"{bcolors.OK}âœ“{bcolors.END} {filename} exists")
        return True
    else:
        print(f"{bcolors.FAIL}âœ—{bcolors.END} {filename} missing")
        return False

def validate_positions():
    """Validate positions.csv structure and data quality."""
    df = pd.read_csv(OUTPUT_DIR / "positions.csv")
    
    required_cols = [
        'position_id', 'trader_id', 'market_id', 'product_type', 'side',
        'open_time', 'close_time', 'duration_seconds',
        'entry_price', 'exit_price', 'size', 'gross_pnl', 'fees', 'realized_pnl'
    ]
    
    issues = []
    
    # Check columns
    missing_cols = set(required_cols) - set(df.columns)
    if missing_cols:
        issues.append(f"Missing columns: {missing_cols}")
    
    # Check data quality
    if not df.empty:
        if (df['duration_seconds'] < 0).any():
            issues.append("Negative duration_seconds found")
        
        if (df['fees'] < 0).any():
            issues.append("Negative fees found")
        
        # PnL consistency check
        expected_pnl = df['gross_pnl'] - df['fees']
        if not expected_pnl.equals(df['realized_pnl']):
            max_diff = abs(expected_pnl - df['realized_pnl']).max()
            if max_diff > 0.01:  # Allow for rounding
                issues.append(f"PnL inconsistency detected (max diff: {max_diff:.4f})")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} positions.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} positions.csv is valid ({len(df)} rows)")
        return True

def validate_summary_metrics():
    """Validate summary_metrics.csv calculations."""
    positions = pd.read_csv(OUTPUT_DIR / "positions.csv")
    summary = pd.read_csv(OUTPUT_DIR / "summary_metrics.csv")
    
    issues = []
    
    for _, row in summary.iterrows():
        trader = row['trader_id']
        trader_pos = positions[positions['trader_id'] == trader]
        
        # Validate win rate
        actual_wins = (trader_pos['realized_pnl'] > 0).sum()
        actual_total = len(trader_pos)
        expected_win_rate = actual_wins / actual_total if actual_total > 0 else 0
        
        if abs(row['win_rate'] - expected_win_rate) > 0.01:
            issues.append(f"{trader}: win_rate mismatch ({row['win_rate']:.2f} vs {expected_win_rate:.2f})")
        
        # Validate long/short ratio
        if abs(row['long_ratio'] + row['short_ratio'] - 1.0) > 0.01:
            issues.append(f"{trader}: long_ratio + short_ratio != 1.0")
        
        # Validate total_pnl
        expected_total = trader_pos['realized_pnl'].sum()
        if abs(row['total_pnl'] - expected_total) > 0.01:
            issues.append(f"{trader}: total_pnl mismatch")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} summary_metrics.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} summary_metrics.csv is valid")
        return True

def validate_equity_curve():
    """Validate equity_curve.csv drawdown calculations."""
    df = pd.read_csv(OUTPUT_DIR / "equity_curve.csv")
    
    issues = []
    
    for trader in df['trader_id'].unique():
        trader_data = df[df['trader_id'] == trader].sort_values('timestamp')
        
        # Validate drawdown is always <= 0
        if (trader_data['drawdown'] > 0.01).any():
            issues.append(f"{trader}: Positive drawdown found")
        
        # Validate cumulative PnL is monotonic sum
        if not trader_data['cumulative_pnl'].is_monotonic_increasing and not trader_data['cumulative_pnl'].is_monotonic_decreasing:
            # This is actually OK - cumulative can go up and down
            pass
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} equity_curve.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} equity_curve.csv is valid")
        return True

def validate_directional_bias():
    """Validate directional_bias.csv calculations."""
    df = pd.read_csv(OUTPUT_DIR / "directional_bias.csv")
    
    issues = []
    
    for _, row in df.iterrows():
        total = row['long_trades'] + row['short_trades']
        
        if total == 0:
            issues.append(f"{row['trader_id']}: No trades")
            continue
        
        expected_long_ratio = row['long_trades'] / total
        expected_short_ratio = row['short_trades'] / total
        
        if abs(row['long_ratio'] - expected_long_ratio) > 0.01:
            issues.append(f"{row['trader_id']}: long_ratio calculation error")
        
        if abs(row['short_ratio'] - expected_short_ratio) > 0.01:
            issues.append(f"{row['trader_id']}: short_ratio calculation error")
        
        if abs(row['long_ratio'] + row['short_ratio'] - 1.0) > 0.01:
            issues.append(f"{row['trader_id']}: ratios don't sum to 1.0")
    
    if issues:
        print(f"{bcolors.WARN}âš {bcolors.END} directional_bias.csv has issues:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"{bcolors.OK}âœ“{bcolors.END} directional_bias.csv is valid")
        return True

def main():
    print("\n" + "=" * 60)
    print("DERIVERSE ANALYTICS VALIDATION")
    print("=" * 60 + "\n")
    
    if not OUTPUT_DIR.exists():
        print(f"{bcolors.FAIL}âœ—{bcolors.END} Output directory not found: {OUTPUT_DIR}")
        print("Run: python -m scripts.run_analytics")
        sys.exit(1)
    
    # Check file existence
    print("Checking required files...")
    required_files = [
        'positions.csv',
        'realized_pnl.csv',
        'equity_curve.csv',
        'summary_metrics.csv',
        'volume_by_market.csv',
        'fees_breakdown.csv',
        'pnl_by_day.csv',
        'pnl_by_hour.csv',
        'directional_bias.csv',
        'order_type_performance.csv'
    ]
    
    all_exist = all(validate_file_exists(f) for f in required_files)
    
    if not all_exist:
        print(f"\n{bcolors.FAIL}âœ— Some files are missing{bcolors.END}")
        sys.exit(1)
    
    print("\nValidating data quality...")
    
    validations = [
        validate_positions(),
        validate_summary_metrics(),
        validate_equity_curve(),
        validate_directional_bias()
    ]
    
    print("\n" + "=" * 60)
    if all(validations):
        print(f"{bcolors.OK}âœ“ ALL VALIDATIONS PASSED{bcolors.END}")
        print("=" * 60 + "\n")
        sys.exit(0)
    else:
        print(f"{bcolors.WARN}âš  SOME VALIDATIONS FAILED{bcolors.END}")
        print("=" * 60 + "\n")
        sys.exit(1)

if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\trades\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\analytics_builder.py =====

# src/analytics/analytics_builder.py - FIXED VERSION
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class AnalyticsBuilder:
    """Build all required analytics tables from canonical PnL engine outputs."""
    
    def __init__(self, positions_df: pd.DataFrame, pnl_df: pd.DataFrame, output_dir: Path):
        self.positions = positions_df.copy()
        self.pnl = pnl_df.copy()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Ensure timestamps are datetime
        if not self.positions.empty:
            self.positions['open_time'] = pd.to_datetime(self.positions['open_time'])
            self.positions['close_time'] = pd.to_datetime(self.positions['close_time'])
            self.positions['duration_seconds'] = (
                self.positions['close_time'] - self.positions['open_time']
            ).dt.total_seconds()
    
    def build_all(self):
        """Generate all required analytics outputs."""
        if self.positions.empty:
            logger.warning("No positions data - generating empty outputs")
            self._generate_empty_outputs()
            return
        
        logger.info("Building core truth tables...")
        self._build_positions()
        self._build_realized_pnl()
        
        logger.info("Building performance metrics...")
        self._build_equity_curve()
        self._build_summary_metrics()
        
        logger.info("Building volume & fees analytics...")
        self._build_volume_by_market()
        self._build_fees_breakdown()
        
        logger.info("Building time-based analytics...")
        self._build_pnl_by_day()
        self._build_pnl_by_hour()
        
        logger.info("Building behavioral analytics...")
        self._build_directional_bias()
        self._build_order_type_performance()
        
        logger.info("Building options Greeks...")
        self._build_greeks_exposure()
        
        logger.info(f"âœ… All analytics saved to {self.output_dir}")
    
    def _build_positions(self):
        """1. Core Truth: positions.csv"""
        output = self.positions[[
            'position_id', 'trader_id', 'market_id', 'product_type', 'side',
            'open_time', 'close_time', 'duration_seconds',
            'entry_price', 'exit_price', 'size', 'gross_pnl', 'fees', 'realized_pnl'
        ]].copy()
        output.to_csv(self.output_dir / 'positions.csv', index=False)
    
    def _build_realized_pnl(self):
        """2. Core Truth: realized_pnl.csv"""
        output = self.positions[[
            'close_time', 'trader_id', 'market_id', 'realized_pnl', 'fees'
        ]].copy()
        output.rename(columns={'close_time': 'timestamp'}, inplace=True)
        output['net_pnl'] = output['realized_pnl'] - output['fees']
        output = output[['timestamp', 'trader_id', 'market_id', 'realized_pnl', 'fees', 'net_pnl']]
        output.to_csv(self.output_dir / 'realized_pnl.csv', index=False)
    
    def _build_equity_curve(self):
        """3. Performance: equity_curve.csv"""
        equity = self.positions.copy()
        equity = equity.sort_values('close_time')
        
        result = []
        for trader in equity['trader_id'].unique():
            trader_data = equity[equity['trader_id'] == trader].copy()
            trader_data['cumulative_pnl'] = trader_data['realized_pnl'].cumsum()
            trader_data['rolling_max'] = trader_data['cumulative_pnl'].cummax()
            trader_data['drawdown'] = trader_data['cumulative_pnl'] - trader_data['rolling_max']
            
            for _, row in trader_data.iterrows():
                result.append({
                    'timestamp': row['close_time'],
                    'trader_id': row['trader_id'],
                    'net_realized_pnl': row['realized_pnl'],
                    'cumulative_pnl': row['cumulative_pnl'],
                    'drawdown': row['drawdown']
                })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'equity_curve.csv', index=False)
    
    def _build_summary_metrics(self):
        """4. Performance: summary_metrics.csv"""
        result = []
        
        for trader in self.positions['trader_id'].unique():
            trader_pos = self.positions[self.positions['trader_id'] == trader]
            
            winning = trader_pos[trader_pos['realized_pnl'] > 0]
            losing = trader_pos[trader_pos['realized_pnl'] < 0]
            
            # Calculate metrics
            total_pnl = trader_pos['realized_pnl'].sum()
            total_fees = trader_pos['fees'].sum()
            trade_count = len(trader_pos)
            win_rate = len(winning) / trade_count if trade_count > 0 else 0
            avg_win = winning['realized_pnl'].mean() if len(winning) > 0 else 0
            avg_loss = losing['realized_pnl'].mean() if len(losing) > 0 else 0
            best_trade = trader_pos['realized_pnl'].max()
            worst_trade = trader_pos['realized_pnl'].min()
            avg_duration = trader_pos['duration_seconds'].mean()
            
            # Directional bias
            long_trades = trader_pos[trader_pos['side'].isin(['long', 'buy'])]
            short_trades = trader_pos[trader_pos['side'].isin(['short', 'sell'])]
            long_ratio = len(long_trades) / trade_count if trade_count > 0 else 0
            short_ratio = len(short_trades) / trade_count if trade_count > 0 else 0
            
            # Max drawdown
            trader_sorted = trader_pos.sort_values('close_time')
            cum_pnl = trader_sorted['realized_pnl'].cumsum()
            rolling_max = cum_pnl.cummax()
            drawdown = cum_pnl - rolling_max
            max_drawdown = drawdown.min()
            
            # âœ… FIXED: Risk-adjusted metrics - use positions instead of pnl
            if len(trader_pos) > 1:
                # Create daily aggregation from positions
                trader_daily = trader_pos.copy()
                trader_daily['date'] = trader_daily['close_time'].dt.date
                daily_returns = trader_daily.groupby('date')['realized_pnl'].sum()
                
                # Sharpe Ratio
                mean_return = daily_returns.mean()
                std_return = daily_returns.std()
                sharpe_ratio = mean_return / std_return if std_return > 0 else 0
                
                # Sortino Ratio
                downside_returns = daily_returns[daily_returns < 0]
                downside_std = downside_returns.std() if len(downside_returns) > 0 else std_return
                sortino_ratio = mean_return / downside_std if downside_std > 0 else 0
            else:
                sharpe_ratio = 0
                sortino_ratio = 0
            
            result.append({
                'trader_id': trader,
                'total_pnl': total_pnl,
                'total_fees': total_fees,
                'trade_count': trade_count,
                'win_rate': win_rate,
                'avg_win': avg_win,
                'avg_loss': avg_loss,
                'best_trade': best_trade,
                'worst_trade': worst_trade,
                'avg_duration_seconds': avg_duration,
                'long_ratio': long_ratio,
                'short_ratio': short_ratio,
                'max_drawdown': max_drawdown,
                'sharpe_ratio': sharpe_ratio,
                'sortino_ratio': sortino_ratio
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'summary_metrics.csv', index=False)
    
    def _build_volume_by_market(self):
        """5. Volume: volume_by_market.csv"""
        result = []
        
        for (market, product), group in self.positions.groupby(['market_id', 'product_type']):
            total_volume = (group['exit_price'] * group['size']).sum()
            trade_count = len(group)
            
            result.append({
                'market_id': market,
                'product_type': product,
                'total_volume': total_volume,
                'trade_count': trade_count
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'volume_by_market.csv', index=False)
    
    def _build_fees_breakdown(self):
        """6. Fees: fees_breakdown.csv"""
        result = []
        
        for (trader, product), group in self.positions.groupby(['trader_id', 'product_type']):
            total_fees = group['fees'].sum()
            
            result.append({
                'trader_id': trader,
                'product_type': product,
                'total_fees': total_fees
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'fees_breakdown.csv', index=False)
    
    def _build_pnl_by_day(self):
        """7. Time Analytics: pnl_by_day.csv"""
        df = self.positions.copy()
        df['date'] = df['close_time'].dt.date
        
        result = []
        for (date, trader), group in df.groupby(['date', 'trader_id']):
            daily_pnl = group['realized_pnl'].sum()
            
            # Calculate cumulative PnL up to this date
            trader_data = df[df['trader_id'] == trader]
            trader_data = trader_data[trader_data['date'] <= date]
            cumulative_pnl = trader_data['realized_pnl'].sum()
            
            result.append({
                'date': date,
                'trader_id': trader,
                'daily_pnl': daily_pnl,
                'cumulative_pnl': cumulative_pnl
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'pnl_by_day.csv', index=False)
    
    def _build_pnl_by_hour(self):
        """8. Time Analytics: pnl_by_hour.csv"""
        df = self.positions.copy()
        df['hour_of_day'] = df['close_time'].dt.hour
        
        result = []
        for (hour, trader), group in df.groupby(['hour_of_day', 'trader_id']):
            avg_pnl = group['realized_pnl'].mean()
            trade_count = len(group)
            
            result.append({
                'hour_of_day': hour,
                'trader_id': trader,
                'avg_pnl': avg_pnl,
                'trade_count': trade_count
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'pnl_by_hour.csv', index=False)
    
    def _build_directional_bias(self):
        """9. Behavioral: directional_bias.csv"""
        result = []
        
        for trader, group in self.positions.groupby('trader_id'):
            long_trades = len(group[group['side'].isin(['long', 'buy'])])
            short_trades = len(group[group['side'].isin(['short', 'sell'])])
            total = long_trades + short_trades
            
            result.append({
                'trader_id': trader,
                'long_trades': long_trades,
                'short_trades': short_trades,
                'long_ratio': long_trades / total if total > 0 else 0,
                'short_ratio': short_trades / total if total > 0 else 0
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'directional_bias.csv', index=False)
    
    def _build_order_type_performance(self):
        """10. Behavioral: order_type_performance.csv"""
        df = self.positions.copy()
        
        # Simple heuristic: classify by duration
        df['order_type'] = df['duration_seconds'].apply(lambda x:
            'market' if x < 300 else  # < 5 min
            'limit' if x < 3600 else   # < 1 hour
            'stop'                      # > 1 hour
        )
        
        result = []
        for order_type, group in df.groupby('order_type'):
            trade_count = len(group)
            avg_pnl = group['realized_pnl'].mean()
            win_rate = (group['realized_pnl'] > 0).mean()
            
            result.append({
                'order_type': order_type,
                'trade_count': trade_count,
                'avg_pnl': avg_pnl,
                'win_rate': win_rate
            })
        
        output = pd.DataFrame(result)
        output.to_csv(self.output_dir / 'order_type_performance.csv', index=False)
    
    def _build_greeks_exposure(self):
        """11. Greeks: greeks_exposure.csv (options only)"""
        options = self.positions[self.positions['product_type'] == 'option'].copy()
        
        if options.empty:
            pd.DataFrame().to_csv(self.output_dir / 'greeks_exposure.csv', index=False)
            return
        
        result = []
        for trader in options['trader_id'].unique():
            trader_opts = options[options['trader_id'] == trader]
            
            # Simplified delta calculation
            net_delta = 0
            for _, opt in trader_opts.iterrows():
                if opt['side'] in ['buy', 'long']:
                    delta_sign = 1
                else:
                    delta_sign = -1
                net_delta += delta_sign * opt['size'] * 0.5
            
            result.append({
                'trader_id': trader,
                'total_option_positions': len(trader_opts),
                'net_delta': net_delta,
                'gamma_exposure': 0,
                'theta_decay': 0
            })
        
        df = pd.DataFrame(result)
        df.to_csv(self.output_dir / 'greeks_exposure.csv', index=False)
    
    def _generate_empty_outputs(self):
        """Generate empty CSV files when no data available."""
        empty_files = [
            'positions.csv', 'realized_pnl.csv', 'equity_curve.csv',
            'summary_metrics.csv', 'volume_by_market.csv', 'fees_breakdown.csv',
            'pnl_by_day.csv', 'pnl_by_hour.csv', 'directional_bias.csv',
            'order_type_performance.csv', 'greeks_exposure.csv'
        ]
        
        for filename in empty_files:
            pd.DataFrame().to_csv(self.output_dir / filename, index=False)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\debug.py =====

import json
from pathlib import Path
from collections import Counter

def inspect_normalized_data():
    normalized_path = Path("data/normalized/events.jsonl")

    if not normalized_path.exists():
        print("âŒ No normalized data file found.")
        return

    if normalized_path.stat().st_size == 0:
        print("âš ï¸ Normalized file exists but is empty.")
        return

    all_fields = set()
    event_types = Counter()
    sample_events = []

    with normalized_path.open() as f:
        for i, line in enumerate(f):
            event = json.loads(line)
            all_fields.update(event.keys())
            event_types[event.get("event_type")] += 1

            if i < 3:
                sample_events.append(event)

    print("\nðŸ“Š Normalized Data Overview")
    print(f"Total events: {sum(event_types.values())}")

    print("\nEvent types:")
    for et, c in event_types.items():
        print(f"  - {et}: {c}")

    print("\nColumns present:")
    for field in sorted(all_fields):
        print(f"  - {field}")

    print("\nSample events:")
    for i, ev in enumerate(sample_events, 1):
        print(f"\nEvent {i}")
        for k, v in ev.items():
            print(f"  {k}: {v}")


if __name__ == "__main__":
    inspect_normalized_data()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\pnl_engine.py =====

# src/analytics/pnl_engine.py
import pandas as pd
import hashlib
import logging

logger = logging.getLogger(__name__)


def compute_realized_pnl(events: pd.DataFrame):
    """
    Canonical PnL engine with full options lifecycle support.
    
    Supports:
    - Spot: buy/sell (different sides for open/close)
    - Perps: long/short (same side for open/close)
    - Options: buy/sell/exercise/expire (different sides)
    
    Options PnL logic:
    - buy: Pay premium (negative cash flow)
    - sell: Receive premium or exit price (positive cash flow)
    - exercise: Intrinsic value - fees
    - expire: Total loss of premium paid
    """

    required_cols = {
        "event_type", "timestamp", "trader_id",
        "market_id", "product_type", "side",
        "price", "size", "fee"
    }

    missing = required_cols - set(events.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    events = events.sort_values("timestamp")

    open_positions = {}
    closed_positions = []

    # Validation stats
    stats = {
        "duplicate_opens": 0,
        "close_without_open": 0,
        "oversized_closes": 0,
    }

    for _, event in events.iterrows():

        # POSITION KEY GENERATION
        # For options that open with SELL (short positions), we need to track them differently

        if event["product_type"] == "perp":
            # Perps: Include side (long stays long, short stays short)
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"],
                event["side"]
            )
        elif event["product_type"] == "option":
            # Options: Exclude side from key
            # BUT track the opening side in the position data
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"]
            )
        else:  # spot
            # Spot: Exclude side (buy opens, sell closes)
            key = (
                event["trader_id"],
                event["market_id"],
                event["product_type"]
            )

        # ==========================================
        # OPEN POSITION
        # ==========================================
        if event["event_type"] == "open":
            if key in open_positions:
                stats["duplicate_opens"] += 1
                continue

            position_data = (
                f"{event['trader_id']}|{event['market_id']}|"
                f"{event['timestamp']}|{event.get('side', '')}"
            )
            position_id = hashlib.sha256(position_data.encode()).hexdigest()[:16]

            open_positions[key] = {
                "position_id": position_id,
                "open_time": event["timestamp"],
                "entry_price": event["price"],
                "size": event["size"],
                "fees": event["fee"],
                "trader_id": event["trader_id"],
                "market_id": event["market_id"],
                "product_type": event["product_type"],
                "side": event["side"],
            }
            
            # Store option-specific fields
            if event["product_type"] == "option":
                open_positions[key]["option_type"] = event.get("option_type")
                open_positions[key]["strike"] = event.get("strike")
                open_positions[key]["expiry"] = event.get("expiry")

        # ==========================================
        # CLOSE POSITION (close, liquidation, exercise, expire)
        # ==========================================
        elif event["event_type"] in {"close", "liquidation", "exercise", "expire"}:
            if key not in open_positions:
                stats["close_without_open"] += 1
                continue

            pos = open_positions[key]
            close_size = event["size"]

            if close_size > pos["size"]:
                stats["oversized_closes"] += 1
                continue

            # Calculate fees
            fee_ratio = close_size / pos["size"]
            allocated_open_fee = pos["fees"] * fee_ratio
            close_fee = event.get("fee", 0)
            total_fees = allocated_open_fee + close_fee

            # ==========================================
            # OPTIONS PNL CALCULATION
            # ==========================================
            if pos["product_type"] == "option":
                gross_pnl = calculate_option_pnl(
                    event_type=event["event_type"],
                    option_type=pos.get("option_type"),
                    side=pos["side"],
                    entry_price=pos["entry_price"],
                    exit_price=event.get("price", 0),
                    strike=pos.get("strike"),
                    underlying_price=event.get("underlying_price"),
                    size=close_size
                )
                net_pnl = gross_pnl - total_fees
                exit_price = event.get("price", 0)

            # ==========================================
            # SPOT/PERP PNL CALCULATION
            # ==========================================
            else:
                exit_price = event["price"]

                # If PnL provided in event, use it (already net)
                if pd.notna(event.get("pnl")):
                    net_pnl = event["pnl"]
                    gross_pnl = net_pnl + total_fees
                else:
                    # Calculate from price difference
                    if pos["side"] in {"long", "buy"}:
                        gross_pnl = (exit_price - pos["entry_price"]) * close_size
                    else:  # short/sell
                        gross_pnl = (pos["entry_price"] - exit_price) * close_size

                    net_pnl = gross_pnl - total_fees

            # Record closed position
            closed_positions.append({
                "position_id": pos["position_id"],
                "open_time": pos["open_time"],
                "close_time": event["timestamp"],
                "trader_id": pos["trader_id"],
                "market_id": pos["market_id"],
                "product_type": pos["product_type"],
                "side": pos["side"],
                "entry_price": pos["entry_price"],
                "exit_price": exit_price,
                "size": close_size,
                "gross_pnl": round(gross_pnl, 4),
                "net_pnl": round(net_pnl, 4),
                "realized_pnl": round(net_pnl, 4),
                "fees": round(total_fees, 4),
                "close_reason": event["event_type"],
            })

            # Update or remove position
            pos["size"] -= close_size
            pos["fees"] -= allocated_open_fee

            if pos["size"] <= 0:
                open_positions.pop(key)

    positions_df = pd.DataFrame(closed_positions)

    # Log validation summary
    logger.info(
        "PnL validation summary | "
        f"duplicate_opens={stats['duplicate_opens']} | "
        f"close_without_open={stats['close_without_open']} | "
        f"oversized_closes={stats['oversized_closes']}"
    )

    if positions_df.empty:
        return positions_df, pd.DataFrame()

    # Build daily PnL aggregates
    pnl_df = (
        positions_df
        .assign(date=lambda df: pd.to_datetime(df["close_time"]).dt.date)
        .groupby(
            ["date", "trader_id", "market_id", "product_type"],
            as_index=False
        )
        .agg(
            net_pnl=("net_pnl", "sum"),
            realized_pnl=("realized_pnl", "sum"),
            fees=("fees", "sum"),
            trade_count=("position_id", "count")
        )
    )

    logger.info(
        f"PnL engine results: {len(positions_df)} closed positions, "
        f"{len(open_positions)} still open"
    )

    return positions_df, pnl_df


def calculate_option_pnl(
    event_type: str,
    option_type: str,
    side: str,
    entry_price: float,
    exit_price: float,
    strike: float,
    underlying_price: float,
    size: float
) -> float:
    """
    Calculate options PnL based on event type.
    
    Args:
        event_type: close, exercise, or expire
        option_type: call or put
        side: buy or sell
        entry_price: Premium paid when opening
        exit_price: Premium received when closing (if close event)
        strike: Strike price
        underlying_price: Current underlying price (for exercise/expire)
        size: Number of contracts
    
    Returns:
        Gross PnL (before fees)
    """
    
    # ==========================================
    # CASE 1: NORMAL CLOSE (sell option back)
    # ==========================================
    if event_type == "close":
        if side == "buy":
            # Bought option, now selling it back
            gross_pnl = (exit_price - entry_price) * size
        else:  # side == "sell"
            # Sold option, now buying it back
            gross_pnl = (entry_price - exit_price) * size
        return gross_pnl
    
    # ==========================================
    # CASE 2: EXERCISE (convert to underlying)
    # ==========================================
    elif event_type == "exercise":
        if side == "buy":
            # Long option holder exercises
            if option_type == "call":
                # Call: Right to BUY at strike
                intrinsic_value = max(0, underlying_price - strike)
            else:  # put
                # Put: Right to SELL at strike
                intrinsic_value = max(0, strike - underlying_price)
            
            # PnL = intrinsic value - premium paid
            gross_pnl = (intrinsic_value - entry_price) * size
        else:
            # Short option holder (assigned) - opposite side
            if option_type == "call":
                intrinsic_value = max(0, underlying_price - strike)
            else:
                intrinsic_value = max(0, strike - underlying_price)
            
            # PnL = premium received - intrinsic value paid out
            gross_pnl = (entry_price - intrinsic_value) * size
        
        return gross_pnl
    
    # ==========================================
    # CASE 3: EXPIRE (worthless)
    # ==========================================
    elif event_type == "expire":
        if side == "buy":
            # Long option expires worthless - lose premium
            gross_pnl = -entry_price * size
        else:  # side == "sell"
            # Short option expires worthless - keep premium
            gross_pnl = entry_price * size
        
        return gross_pnl
    
    # Fallback
    return 0.0


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\summary.py =====

# src/analytics/summary.py
import pandas as pd

def compute_executive_summary(positions: pd.DataFrame, pnl: pd.DataFrame) -> dict:
    """
    Compute high-level KPIs from canonical PnL outputs.
    
    Args:
        positions: Output from compute_realized_pnl (positions_df)
        pnl: Output from compute_realized_pnl (pnl_df)
    
    Returns:
        Dictionary of KPI metrics
    """
    if positions.empty:
        return {"status": "no_data"}
    
    summary = {}
    
    # Core PnL
    summary["total_pnl"] = pnl["net_pnl"].sum()
    summary["total_fees"] = pnl["fees"].sum()
    summary["trade_count"] = len(positions)
    summary["win_rate"] = (positions["net_pnl"] > 0).mean()
    
    # Win/Loss Analysis
    winning_trades = positions[positions["net_pnl"] > 0]
    losing_trades = positions[positions["net_pnl"] < 0]
    
    summary["avg_win"] = winning_trades["net_pnl"].mean() if len(winning_trades) > 0 else 0
    summary["avg_loss"] = losing_trades["net_pnl"].mean() if len(losing_trades) > 0 else 0
    summary["best_trade"] = positions["net_pnl"].max()
    summary["worst_trade"] = positions["net_pnl"].min()
    
    # Duration Analysis
    positions = positions.copy()
    positions["duration"] = (
        pd.to_datetime(positions["close_time"]) - 
        pd.to_datetime(positions["open_time"])
    )
    summary["avg_duration"] = positions["duration"].mean()
    
    # Directional Bias
    summary["long_ratio"] = (positions["side"].isin(["long", "buy"])).mean()
    summary["short_ratio"] = (positions["side"].isin(["short", "sell"])).mean()
    
    # Drawdown
    pnl_sorted = pnl.sort_values("date")
    pnl_sorted["cum_pnl"] = pnl_sorted["net_pnl"].cumsum()
    pnl_sorted["drawdown"] = pnl_sorted["cum_pnl"] - pnl_sorted["cum_pnl"].cummax()
    summary["max_drawdown"] = pnl_sorted["drawdown"].min()
    
    # âœ… NEW: Risk-Adjusted Returns
    if not pnl.empty and len(pnl) > 1:
        daily_returns = pnl.groupby('date')['net_pnl'].sum()
        
        # Sharpe Ratio (assuming risk-free rate = 0)
        mean_return = daily_returns.mean()
        std_return = daily_returns.std()
        sharpe_ratio = mean_return / std_return if std_return > 0 else 0
        summary["sharpe_ratio"] = sharpe_ratio
        
        # Sortino Ratio (downside deviation only)
        downside_returns = daily_returns[daily_returns < 0]
        downside_std = downside_returns.std() if len(downside_returns) > 0 else std_return
        sortino_ratio = mean_return / downside_std if downside_std > 0 else 0
        summary["sortino_ratio"] = sortino_ratio
    else:
        summary["sharpe_ratio"] = 0
        summary["sortino_ratio"] = 0
    
    return summary


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\validate.py =====

# src/analytics/validate.py
from typing import Dict, Any, Set
from datetime import datetime

class EventValidationError(Exception):
    """Raised when event fails validation."""
    pass

# --- Base required fields for ALL events ---
BASE_REQUIRED_FIELDS = {
    "event_id",
    "event_type",
    "timestamp",
    "trader_id",
    "market_id",
    "product_type"
}

# --- Base optional fields for ALL events ---
BASE_OPTIONAL_FIELDS = {
    "side",
    "price",
    "size",
    "fee",
    "pnl",
    "order_type"  # âœ… ADDED: Allow order_type field
}

# --- Option-specific fields ---
OPTION_REQUIRED_FIELDS = {
    "option_type",
    "strike",
    "expiry"
}

OPTION_OPTIONAL_FIELDS = {
    "delta",
    "gamma",
    "theta",
    "vega",
    "implied_vol",
    "underlying_price"
}

# --- Event type schemas ---
EVENT_TYPE_SCHEMAS = {
    "trade": {
        "required": {"side", "price", "size", "fee"},
        "optional": {"pnl"}
    },
    "open": {
        "required": {"side", "price", "size", "fee"},
        "optional": {"pnl", "order_type"}  # âœ… ADDED
    },
    "close": {
        "required": {"side", "price", "size", "fee"},
        "optional": {"pnl", "order_type"}  # âœ… ADDED
    },
    "exercise": {
        "required": {"side", "size", "fee"},
        "optional": {"price", "pnl", "underlying_price"}
    },
    "expire": {
        "required": {"side", "size"},
        "optional": {"price", "fee", "pnl", "underlying_price"}
    }
}


def validate_event(event: dict) -> None:
    """
    Validate event schema and data quality.
    
    Raises:
        EventValidationError: If validation fails
    """
    event_type = event.get("event_type")
    product_type = event.get("product_type")

    # --------------------------------------------------
    # 1ï¸âƒ£ Trade events are informational only - skip position validation
    # --------------------------------------------------
    if event_type == "trade":
        return

    # --------------------------------------------------
    # 2ï¸âƒ£ Validate product type
    # --------------------------------------------------
    valid_products = {"spot", "perp", "option"}
    if product_type not in valid_products:
        raise EventValidationError(
            f"Invalid product_type: {product_type}. Allowed: {valid_products}"
        )

    # --------------------------------------------------
    # 3ï¸âƒ£ Product-specific side validation
    # --------------------------------------------------
    if product_type == "option":
        # Options: Allow both trading terms (buy/sell) and position terms (long/short)
        allowed_sides = {"buy", "sell", "long", "short", "exercise", "expire"}
    elif product_type == "perp":
        allowed_sides = {"long", "short"}
    elif product_type == "spot":
        allowed_sides = {"buy", "sell"}
    
    side = event.get("side")
    if side and side not in allowed_sides:
        raise EventValidationError(
            f"Invalid side '{side}' for product_type '{product_type}'. "
            f"Must be one of: {allowed_sides}"
        )

    # --------------------------------------------------
    # 4ï¸âƒ£ Build allowed fields based on product type
    # --------------------------------------------------
    allowed_fields = BASE_REQUIRED_FIELDS | BASE_OPTIONAL_FIELDS
    
    if product_type == "option":
        allowed_fields |= OPTION_REQUIRED_FIELDS | OPTION_OPTIONAL_FIELDS
    
    # Add event-specific fields
    if event_type in EVENT_TYPE_SCHEMAS:
        schema = EVENT_TYPE_SCHEMAS[event_type]
        allowed_fields |= schema["required"] | schema["optional"]

    # --------------------------------------------------
    # 5ï¸âƒ£ Check for extra fields (schema drift)
    # --------------------------------------------------
    extra_fields = set(event.keys()) - allowed_fields
    if extra_fields:
        raise EventValidationError(
            f"Unexpected fields detected: {extra_fields}. "
            f"Allowed for {product_type}/{event_type}: {allowed_fields}"
        )

    # --------------------------------------------------
    # 6ï¸âƒ£ Check event-type-specific required fields
    # --------------------------------------------------
    if event_type in EVENT_TYPE_SCHEMAS:
        schema = EVENT_TYPE_SCHEMAS[event_type]
        missing_required = schema["required"] - set(event.keys())
        if missing_required:
            raise EventValidationError(
                f"Event type '{event_type}' missing required fields: {missing_required}"
            )

    # --------------------------------------------------
    # 7ï¸âƒ£ Check option-specific required fields
    # --------------------------------------------------
    if product_type == "option":
        missing_option_required = OPTION_REQUIRED_FIELDS - set(event.keys())
        if missing_option_required:
            raise EventValidationError(
                f"Option product missing required fields: {missing_option_required}"
            )

    # --------------------------------------------------
    # 8ï¸âƒ£ Validate timestamp format
    # --------------------------------------------------
    try:
        timestamp_str = event["timestamp"]
        if timestamp_str.endswith("Z"):
            timestamp_str = timestamp_str.replace("Z", "+00:00")
        datetime.fromisoformat(timestamp_str)
    except (ValueError, AttributeError, TypeError) as e:
        raise EventValidationError(f"Invalid timestamp format: {event.get('timestamp')} - {e}")

    # --------------------------------------------------
    # 9ï¸âƒ£ Validate numeric fields
    # --------------------------------------------------
    numeric_fields = {"price", "size", "fee", "pnl", "strike", "delta", "gamma", 
                     "theta", "vega", "implied_vol", "underlying_price"}
    for field in numeric_fields & event.keys():
        value = event[field]
        if value is not None and not isinstance(value, (int, float)):
            raise EventValidationError(
                f"Field '{field}' must be numeric or null, got {type(value)}: {value}"
            )

    # --------------------------------------------------
    # ðŸ”Ÿ Validate option-specific values
    # --------------------------------------------------
    if product_type == "option":
        # Validate option_type
        option_type = event.get("option_type")
        if option_type not in {"call", "put"}:
            raise EventValidationError(f"Invalid option_type: {option_type}. Must be 'call' or 'put'")
        
        # Validate expiry format
        expiry = event.get("expiry")
        if expiry:
            try:
                if expiry.endswith("Z"):
                    expiry = expiry.replace("Z", "+00:00")
                datetime.fromisoformat(expiry)
            except (ValueError, AttributeError):
                raise EventValidationError(f"Invalid expiry format: {expiry}")


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\analytics\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\common\logging.py =====

import logging

def get_logger(name):
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\common\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\normalizer.py =====

# src/ingestion/normalizer.py
from typing import Dict, Any
from datetime import datetime, timezone
import hashlib

def normalize_event(raw_event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Normalize raw event data into canonical schema.
    - Convert keys to expected schema names
    - Convert timestamps to ISO 8601
    - Ensure event_id exists
    - Handle option-specific fields
    - Normalize position terminology (longâ†’buy, shortâ†’sell for options/spot)
    """
    event = raw_event.copy()

    # --- Normalize timestamp ---
    ts = event.get("timestamp")
    if isinstance(ts, (int, float)):
        # Convert Unix timestamp (seconds) to ISO 8601 UTC
        event["timestamp"] = datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()
    elif isinstance(ts, datetime):
        # Convert datetime object to ISO string
        event["timestamp"] = ts.isoformat()
    elif isinstance(ts, str):
        # Ensure proper ISO format with timezone
        try:
            # Handle different formats
            ts_clean = ts.replace("Z", "+00:00")
            dt = datetime.fromisoformat(ts_clean)
            # Standardize to UTC with Z suffix
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            event["timestamp"] = dt.isoformat().replace("+00:00", "Z")
        except ValueError:
            # Leave as-is; validation will catch errors
            pass

    # --- Normalize keys for backward compatibility ---
    key_mappings = {
        "trader": "trader_id",
        "market": "market_id", 
        "type": "event_type",
        "product": "product_type",
        "optionType": "option_type",  # Handle camelCase
        "impliedVol": "implied_vol"
    }
    
    for old_key, new_key in key_mappings.items():
        if old_key in event and new_key not in event:
            event[new_key] = event.pop(old_key)

    # --- Normalize product_type ---
    if "product_type" in event:
        product = event["product_type"].lower()
        if product in ["perpetual", "future", "futures", "perp"]:
            event["product_type"] = "perp"
        elif product in ["options", "option"]:
            event["product_type"] = "option"
        elif product in ["spot", "cash"]:
            event["product_type"] = "spot"

    # --- Normalize side terminology ---
    # Convert position terms (long/short) to trading terms (buy/sell) for spot and options
    # Keep long/short for perps
    if "side" in event and event.get("product_type") in ["spot", "option"]:
        side = event["side"].lower()
        
        # Only normalize for open/close events, not for exercise/expire
        if event.get("event_type") in ["open", "close", "trade"]:
            if side == "long":
                event["side"] = "buy"
            elif side == "short":
                event["side"] = "sell"
            # Already buy/sell stays as-is

    # --- Normalize option-specific fields ---
    if event.get("product_type") == "option":
        # Normalize option_type
        if "option_type" in event:
            event["option_type"] = event["option_type"].lower()
        
        # Normalize expiry timestamp
        if "expiry" in event and event["expiry"]:
            expiry = event["expiry"]
            if isinstance(expiry, str):
                try:
                    expiry_clean = expiry.replace("Z", "+00:00")
                    dt = datetime.fromisoformat(expiry_clean)
                    # Standardize format
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    event["expiry"] = dt.isoformat().replace("+00:00", "Z")
                except ValueError:
                    # Leave as-is
                    pass

    # --- Ensure event_id exists ---
    if "event_id" not in event:
        # Create deterministic event ID
        raw_parts = [
            str(event.get('event_type', '')),
            str(event.get('timestamp', '')),
            str(event.get('trader_id', '')),
            str(event.get('market_id', '')),
            str(event.get('product_type', ''))
        ]
        raw = "|".join(raw_parts)
        event["event_id"] = hashlib.sha256(raw.encode()).hexdigest()

    # --- Normalize numeric fields ---
    numeric_fields = ["price", "size", "fee", "pnl", "strike", "delta", 
                     "gamma", "theta", "vega", "implied_vol", "underlying_price"]
    
    for field in numeric_fields:
        if field in event and event[field] is not None:
            try:
                event[field] = float(event[field])
            except (ValueError, TypeError):
                # Keep as-is if conversion fails
                pass

    return event


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\pipelines.py =====

# src/ingestion/pipelines.py
import json
import hashlib
from pathlib import Path
from src.ingestion.watermark import WatermarkStore
from src.ingestion.normalizer import normalize_event
from src.analytics.validate import validate_event, EventValidationError


class IngestionPipeline:
    def __init__(self, raw_path: str, output_path: str, checkpoint_path: str):
        self.raw_path = Path(raw_path)
        self.output_path = Path(output_path)
        self.watermark = WatermarkStore(checkpoint_path)

    def run(self) -> int:
        """
        Event-driven ingestion: normalize once, append forever.
        Supports both JSON array and JSONL formats.
        """
        if not self.raw_path.exists():
            raise FileNotFoundError(f"Raw data source not found: {self.raw_path}")

        # Load events based on file format
        if self.raw_path.suffix == '.json':
            # JSON array format (e.g., configs/mock_data.json)
            with self.raw_path.open("r", encoding="utf-8") as f:
                raw_events = json.load(f)
        elif self.raw_path.suffix == '.jsonl':
            # JSONL format (one JSON object per line)
            raw_events = []
            with self.raw_path.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:  # Skip empty lines
                        raw_events.append(json.loads(line))
        else:
            raise ValueError(f"Unsupported file format: {self.raw_path.suffix}")

        new_events = []
        errors = []

        for idx, raw in enumerate(raw_events, 1):
            try:
                # Generate event_id if missing
                if "event_id" not in raw:
                    seed = (
                        f"{raw.get('event_type')}|"
                        f"{raw.get('timestamp')}|"
                        f"{raw.get('trader_id')}|"
                        f"{raw.get('market_id')}|{idx}"
                    )
                    raw["event_id"] = hashlib.sha256(seed.encode()).hexdigest()

                # Skip if already processed
                if not self.watermark.is_new(raw["event_id"]):
                    continue

                # Normalize and validate
                normalized = normalize_event(raw)
                validate_event(normalized)

                new_events.append(normalized)
                self.watermark.mark(raw["event_id"])

            except EventValidationError as e:
                errors.append(f"Event {idx}: Validation failed - {e}")
            except Exception as e:
                errors.append(f"Event {idx}: Unexpected error - {e}")

        # Report errors
        if errors:
            print(f"âš ï¸  {len(errors)} events had issues:")
            for e in errors[:5]:
                print(f"   - {e}")
            if len(errors) > 5:
                print(f"   ... and {len(errors) - 5} more")

        # Write normalized events to output (JSONL format)
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        with self.output_path.open("a", encoding="utf-8") as f:
            for e in new_events:
                f.write(json.dumps(e) + "\n")

        print(f"âœ… Ingested {len(new_events)} valid events")
        return len(new_events)


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\watermark.py =====

# src/ingestion/watermark.py
import json
from pathlib import Path
from typing import Set

class WatermarkStore:
    """
    Persistent watermark store to prevent reprocessing events.
    """

    def __init__(self, path: str):
        self.path = Path(path)
        self.seen: Set[str] = set()
        self._load()

    def _load(self):
        if self.path.exists():
            with open(self.path, "r") as f:
                self.seen = set(json.load(f))

    def _save(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.path, "w") as f:
            json.dump(list(self.seen), f)

    def is_new(self, event_id: str) -> bool:
        return event_id not in self.seen

    def mark(self, event_id: str):
        self.seen.add(event_id)
        self._save()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\ingestion\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\src\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\analytics\test_ingestion.py =====

# Create a test script test_ingestion.py
import json

with open("data/normalized/events.jsonl", "r") as f:
    for i, line in enumerate(f, 1):
        line = line.strip()
        if line:
            try:
                data = json.loads(line)
                print(f"Line {i}: OK - {data.get('event_type', 'N/A')}")
            except json.JSONDecodeError as e:
                print(f"Line {i}: ERROR - {e}")
                print(f"  Content: {line[:50]}...")
        else:
            print(f"Line {i}: EMPTY LINE")


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\analytics\test_pnl_engine.py =====

import pandas as pd
from datetime import datetime, timezone

from src.analytics.pnl_engine import compute_realized_pnl

def test_simple_open_close_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100.0,
            "size": 1,
            "fee": 0.5,
        },
        {
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110.0,
            "size": 1,
            "fee": 0.5,
            "pnl": 9.0,  # truth reference
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 1
    assert positions.iloc[0]["realized_pnl"] == 9.0

def test_open_without_close_has_no_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T1",
            "market_id": "SOL-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 50,
            "size": 2,
            "fee": 0.2,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty

def test_pnl_only_on_close_events():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "trade",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T1",
            "market_id": "SOL/USDC",
            "product_type": "spot",
            "side": "buy",
            "price": 100,
            "size": 1,
            "fee": 0.1,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty
def test_pnl_engine_is_deterministic():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T2",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "short",
            "price": 200,
            "size": 1,
            "fee": 0.3,
        },
        {
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T2",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "short",
            "price": 180,
            "size": 1,
            "fee": 0.3,
            "pnl": 19.4,
        },
    ])

    p1, pnl1 = compute_realized_pnl(events)
    p2, pnl2 = compute_realized_pnl(events)

    pd.testing.assert_frame_equal(p1, p2)
    pd.testing.assert_frame_equal(pnl1, pnl2)

def test_close_without_open_is_rejected():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "close",
            "timestamp": datetime.now(timezone.utc),
            "trader_id": "T3",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120,
            "size": 1,
            "fee": 0.4,
            "pnl": 0,
        }
    ])

    positions, pnl = compute_realized_pnl(events)

    assert positions.empty
    assert pnl.empty
def test_partial_close_pnl():
    events = pd.DataFrame([
        {
            "event_id": "e1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100.0,
            "size": 10,
            "fee": 1.0,
        },
        {
            # partial close (50%)
            "event_id": "e2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110.0,
            "size": 5,
            "fee": 0.5,
        },
        {
            # final close
            "event_id": "e3",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 3, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120.0,
            "size": 5,
            "fee": 0.5,
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 2

    realized = positions["realized_pnl"].sum()
    assert round(realized, 2) == round(
        ((110 - 100) * 5 + (120 - 100) * 5) - 2.0, 2
    )

def test_multiple_partial_closes_order_independent():
    base_events = [
        {
            "event_id": "o1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 100,
            "size": 10,
            "fee": 1.0,
        },
        {
            "event_id": "c1",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 110,
            "size": 4,
            "fee": 0.4,
        },
        {
            "event_id": "c2",
            "event_type": "close",
            "timestamp": datetime(2026, 1, 3, tzinfo=timezone.utc),
            "trader_id": "T1",
            "market_id": "BTC-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 120,
            "size": 6,
            "fee": 0.6,
        },
    ]

    df1 = pd.DataFrame(base_events)
    df2 = pd.DataFrame(reversed(base_events))

    p1, pnl1 = compute_realized_pnl(df1)
    p2, pnl2 = compute_realized_pnl(df2)

    pd.testing.assert_frame_equal(p1, p2)
    pd.testing.assert_frame_equal(pnl1, pnl2)
def test_liquidation_is_partial_close():
    events = pd.DataFrame([
        {
            "event_id": "o1",
            "event_type": "open",
            "timestamp": datetime(2026, 1, 1, tzinfo=timezone.utc),
            "trader_id": "T9",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 200,
            "size": 10,
            "fee": 1.0,
        },
        {
            "event_id": "l1",
            "event_type": "liquidation",
            "timestamp": datetime(2026, 1, 2, tzinfo=timezone.utc),
            "trader_id": "T9",
            "market_id": "ETH-PERP",
            "product_type": "perp",
            "side": "long",
            "price": 150,
            "size": 4,
            "fee": 0.5,
        },
    ])

    positions, pnl = compute_realized_pnl(events)

    assert len(positions) == 1
    assert positions.iloc[0]["close_reason"] == "liquidation"


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\analytics\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\tests\__init__.py =====



===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\main.py =====

def main():
    print("Hello from deriverse-data-puller!")


if __name__ == "__main__":
    main()


===== FILE: C:\Users\HP\Direverse\Deriverse-Trading-System-Analysis\deriverse-data-puller\pyproject.toml =====

[project]
name = "deriverse-data-puller"
version = "0.1.0"
description = "Mock on-chain trading analytics system for Deriverse"
readme = "README.md"
requires-python = ">=3.10"

dependencies = [
    "matplotlib>=3.10.8",
    "pandas>=2.3.3",
    "plotly>=6.5.2",
    "pyyaml",
    "requests>=2.32.5",
    "streamlit",
]

